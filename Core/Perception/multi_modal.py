"""
Multi-Modal Perception Engine - Gap 3: ë‹¤ì¤‘ ëª¨ë‹¬ ì¸ì‹

ì—˜ë¦¬ì‹œì•„ê°€ í…ìŠ¤íŠ¸ ì™¸ì—ë„ ì‹œê°, ì²­ê°, í–‰ë™ ë“±
ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤.

Gap 0 ì¤€ìˆ˜: ê° ëª¨ë‹¬ë¦¬í‹°ëŠ” ê³ ìœ í•œ ì¸ì‹ë¡ ì  ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Callable
from enum import Enum
from abc import ABC, abstractmethod
import logging
import time

logger = logging.getLogger("MultiModalPerception")

# ì„ë² ë”© ì°¨ì› ìƒìˆ˜
EMBEDDING_DIM = 64


class ModalityType(Enum):
    """ëª¨ë‹¬ë¦¬í‹° ìœ í˜•"""
    TEXT = "text"           # í…ìŠ¤íŠ¸ (í˜„ì¬ ì£¼ë ¥)
    VISION = "vision"       # ì‹œê°
    AUDIO = "audio"         # ì²­ê°
    ACTION = "action"       # í–‰ë™/ìš´ë™
    EMOTION = "emotion"     # ê°ì •
    MEMORY = "memory"       # ê¸°ì–µ
    INTENTION = "intention" # ì˜ë„


@dataclass
class PerceptualInput:
    """ì¸ì‹ ì…ë ¥"""
    modality: ModalityType
    data: Any
    timestamp: float = field(default_factory=time.time)
    confidence: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Gap 0: ì´ ì…ë ¥ì˜ ì¸ì‹ë¡ ì  ì˜ë¯¸
    epistemology: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {
        "point": {"score": 0.30, "meaning": "ìˆœê°„ì  ê°ê° ìê·¹"},
        "line": {"score": 0.25, "meaning": "ì´ì „ ì…ë ¥ê³¼ì˜ ì—°ê²°"},
        "space": {"score": 0.25, "meaning": "ì „ì²´ ë§¥ë½ì—ì„œì˜ ì˜ë¯¸"},
        "god": {"score": 0.20, "meaning": "ê¶ê·¹ì  í•´ì„"}
    })


@dataclass
class PerceptualRepresentation:
    """í†µí•©ëœ ì¸ì‹ í‘œí˜„"""
    modalities: List[ModalityType]
    unified_embedding: List[float]
    interpretations: Dict[str, Any]
    salience: float  # í˜„ì €ì„± (ì–¼ë§ˆë‚˜ ì£¼ëª©í•  ê°€ì¹˜ê°€ ìˆëŠ”ê°€)
    
    # Gap 0: í†µí•©ëœ ì¸ì‹ì˜ ì˜ë¯¸
    epistemology: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {
        "point": {"score": 0.20, "meaning": "ê°œë³„ ëª¨ë‹¬ë¦¬í‹°ì˜ í•©"},
        "line": {"score": 0.30, "meaning": "ëª¨ë‹¬ë¦¬í‹° ê°„ êµì°¨ ì—°ê²°"},
        "space": {"score": 0.30, "meaning": "ì „ì²´ ê²½í—˜ì˜ ê²ŒìŠˆíƒˆíŠ¸"},
        "god": {"score": 0.20, "meaning": "ì´ˆì›”ì  ì˜ë¯¸ ë¶€ì—¬"}
    })


class ModalityProcessor(ABC):
    """ëª¨ë‹¬ë¦¬í‹° í”„ë¡œì„¸ì„œ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, modality: ModalityType):
        self.modality = modality
        self.is_enabled = True
    
    @abstractmethod
    def process(self, data: Any) -> List[float]:
        """ë°ì´í„°ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        pass
    
    @abstractmethod
    def interpret(self, data: Any) -> Dict[str, Any]:
        """ë°ì´í„°ë¥¼ í•´ì„"""
        pass


class TextProcessor(ModalityProcessor):
    """í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹° í”„ë¡œì„¸ì„œ"""
    
    def __init__(self):
        super().__init__(ModalityType.TEXT)
    
    def process(self, data: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ê°„ë‹¨í•œ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ëª¨ë¸ ì‚¬ìš©)"""
        if not data:
            return [0.0] * EMBEDDING_DIM
        
        # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”©
        embedding = []
        for i in range(EMBEDDING_DIM):
            val = sum(ord(c) * (i + 1) for c in data) % 1000 / 1000
            embedding.append(val)
        
        return embedding
    
    def interpret(self, data: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ í•´ì„"""
        return {
            "length": len(data),
            "word_count": len(data.split()) if data else 0,
            "has_question": "?" in data,
            "sentiment": "neutral",  # ì‹¤ì œë¡œëŠ” ê°ì • ë¶„ì„ í•„ìš”
            "language": "ko" if any('\uac00' <= c <= '\ud7a3' for c in data) else "en"
        }


class VisionProcessor(ModalityProcessor):
    """ì‹œê° ëª¨ë‹¬ë¦¬í‹° í”„ë¡œì„¸ì„œ (í”Œë ˆì´ìŠ¤í™€ë”)"""
    
    def __init__(self):
        super().__init__(ModalityType.VISION)
    
    def process(self, data: Any) -> List[float]:
        """ì´ë¯¸ì§€ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (í”Œë ˆì´ìŠ¤í™€ë”)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” CNN ë˜ëŠ” ViT ì‚¬ìš©
        return [0.0] * EMBEDDING_DIM
    
    def interpret(self, data: Any) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ í•´ì„ (í”Œë ˆì´ìŠ¤í™€ë”)"""
        return {
            "objects": [],
            "scene": "unknown",
            "colors": [],
            "faces": 0
        }


class AudioProcessor(ModalityProcessor):
    """ì²­ê° ëª¨ë‹¬ë¦¬í‹° í”„ë¡œì„¸ì„œ (í”Œë ˆì´ìŠ¤í™€ë”)"""
    
    def __init__(self):
        super().__init__(ModalityType.AUDIO)
    
    def process(self, data: Any) -> List[float]:
        """ì˜¤ë””ì˜¤ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (í”Œë ˆì´ìŠ¤í™€ë”)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Whisper ë˜ëŠ” wav2vec ì‚¬ìš©
        return [0.0] * EMBEDDING_DIM
    
    def interpret(self, data: Any) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ í•´ì„ (í”Œë ˆì´ìŠ¤í™€ë”)"""
        return {
            "transcript": "",
            "speaker": "unknown",
            "emotion": "neutral",
            "volume": 0.5
        }


class ActionProcessor(ModalityProcessor):
    """í–‰ë™ ëª¨ë‹¬ë¦¬í‹° í”„ë¡œì„¸ì„œ"""
    
    def __init__(self):
        super().__init__(ModalityType.ACTION)
    
    def process(self, data: Dict[str, Any]) -> List[float]:
        """í–‰ë™ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        # í–‰ë™ ìœ í˜•ì— ë”°ë¥¸ ê°„ë‹¨í•œ ì„ë² ë”©
        embedding = [0.0] * EMBEDDING_DIM
        
        action_type = data.get("type", "")
        if action_type == "speak":
            embedding[0] = 1.0
        elif action_type == "move":
            embedding[1] = 1.0
        elif action_type == "think":
            embedding[2] = 1.0
        
        return embedding
    
    def interpret(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í–‰ë™ í•´ì„"""
        return {
            "type": data.get("type", "unknown"),
            "target": data.get("target", None),
            "intensity": data.get("intensity", 0.5),
            "duration": data.get("duration", 0.0)
        }


class MultiModalPerceptionEngine:
    """
    Gap 3: ë‹¤ì¤‘ ëª¨ë‹¬ ì¸ì‹ ì—”ì§„
    
    ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•˜ì—¬ í†µì¼ëœ ì¸ì‹ í‘œí˜„ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    í˜„ì¬ êµ¬í˜„:
    - TextProcessor: í…ìŠ¤íŠ¸ ì²˜ë¦¬
    - VisionProcessor: ì‹œê° ì²˜ë¦¬ (í”Œë ˆì´ìŠ¤í™€ë”)
    - AudioProcessor: ì²­ê° ì²˜ë¦¬ (í”Œë ˆì´ìŠ¤í™€ë”)
    - ActionProcessor: í–‰ë™ ì²˜ë¦¬
    
    Gap 0 ì¤€ìˆ˜: ëª¨ë“  ì¸ì‹ì— ì² í•™ì  ì˜ë¯¸ ë¶€ì—¬
    """
    
    # Gap 0: ë‹¤ì¤‘ ëª¨ë‹¬ ì¸ì‹ì˜ ì¸ì‹ë¡ 
    EPISTEMOLOGY = {
        "point": {"score": 0.20, "meaning": "ê°œë³„ ê°ê°ì˜ ìˆœê°„"},
        "line": {"score": 0.25, "meaning": "ê°ê° ê°„ ì‹œê°„ì  ì—°ê²°"},
        "space": {"score": 0.35, "meaning": "ëª¨ë‹¬ë¦¬í‹° í†µí•©"},
        "god": {"score": 0.20, "meaning": "ê²½í—˜ì˜ ì´ˆì›”ì  í•´ì„"}
    }
    
    def __init__(self):
        self.epistemology = self.EPISTEMOLOGY
        self.processors: Dict[ModalityType, ModalityProcessor] = {}
        self.perception_buffer: List[PerceptualInput] = []
        self.max_buffer_size = 100
        
        # ê¸°ë³¸ í”„ë¡œì„¸ì„œ ë“±ë¡
        self.register_processor(TextProcessor())
        self.register_processor(VisionProcessor())
        self.register_processor(AudioProcessor())
        self.register_processor(ActionProcessor())
        
        logger.info("ğŸ‘ï¸ MultiModalPerceptionEngine initialized")
    
    def explain_meaning(self) -> str:
        """Gap 0 ì¤€ìˆ˜: ì¸ì‹ë¡ ì  ì˜ë¯¸ ì„¤ëª…"""
        lines = ["=== ë‹¤ì¤‘ ëª¨ë‹¬ ì¸ì‹ ì¸ì‹ë¡  ==="]
        for basis, data in self.epistemology.items():
            lines.append(f"  {basis}: {data['score']:.0%} - {data['meaning']}")
        return "\n".join(lines)
    
    def register_processor(self, processor: ModalityProcessor) -> None:
        """ëª¨ë‹¬ë¦¬í‹° í”„ë¡œì„¸ì„œ ë“±ë¡"""
        self.processors[processor.modality] = processor
        logger.info(f"ğŸ“¦ Registered {processor.modality.value} processor")
    
    def perceive(
        self,
        modality: ModalityType,
        data: Any,
        confidence: float = 1.0,
        metadata: Optional[Dict[str, Any]] = None
    ) -> PerceptualInput:
        """
        ë‹¨ì¼ ëª¨ë‹¬ë¦¬í‹° ì¸ì‹
        
        Args:
            modality: ëª¨ë‹¬ë¦¬í‹° ìœ í˜•
            data: ì…ë ¥ ë°ì´í„°
            confidence: í™•ì‹ ë„
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        
        Returns:
            PerceptualInput
        """
        perception = PerceptualInput(
            modality=modality,
            data=data,
            confidence=confidence,
            metadata=metadata or {}
        )
        
        # ë²„í¼ì— ì¶”ê°€
        self.perception_buffer.append(perception)
        if len(self.perception_buffer) > self.max_buffer_size:
            self.perception_buffer = self.perception_buffer[-self.max_buffer_size:]
        
        return perception
    
    def integrate(
        self,
        inputs: List[PerceptualInput]
    ) -> PerceptualRepresentation:
        """
        ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹° ì…ë ¥ì„ í†µí•©
        
        Args:
            inputs: ì¸ì‹ ì…ë ¥ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            í†µí•©ëœ ì¸ì‹ í‘œí˜„
        """
        modalities = list(set(inp.modality for inp in inputs))
        embeddings = []
        interpretations = {}
        
        for inp in inputs:
            processor = self.processors.get(inp.modality)
            if processor and processor.is_enabled:
                # ì„ë² ë”© ìƒì„±
                embedding = processor.process(inp.data)
                embeddings.append(embedding)
                
                # í•´ì„ ì¶”ê°€
                interp = processor.interpret(inp.data)
                interpretations[inp.modality.value] = interp
        
        # ì„ë² ë”© í†µí•© (í‰ê· )
        if embeddings:
            unified = [
                sum(emb[i] for emb in embeddings) / len(embeddings)
                for i in range(len(embeddings[0]))
            ]
        else:
            unified = [0.0] * EMBEDDING_DIM
        
        # í˜„ì €ì„± ê³„ì‚° (ì…ë ¥ ë‹¤ì–‘ì„± + í™•ì‹ ë„ í‰ê· )
        modality_diversity = len(modalities) / len(ModalityType)
        avg_confidence = sum(inp.confidence for inp in inputs) / len(inputs) if inputs else 0
        salience = (modality_diversity + avg_confidence) / 2
        
        return PerceptualRepresentation(
            modalities=modalities,
            unified_embedding=unified,
            interpretations=interpretations,
            salience=salience
        )
    
    def cross_modal_attention(
        self,
        query_modality: ModalityType,
        query_data: Any,
        context_inputs: List[PerceptualInput]
    ) -> Dict[ModalityType, float]:
        """
        êµì°¨ ëª¨ë‹¬ ì£¼ì˜ (Cross-Modal Attention)
        
        query_modalityì˜ ê´€ì ì—ì„œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ë“¤ì— ì–¼ë§ˆë‚˜ ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ì•¼ í•˜ëŠ”ê°€?
        
        Returns:
            ê° ëª¨ë‹¬ë¦¬í‹°ì— ëŒ€í•œ ì£¼ì˜ ê°€ì¤‘ì¹˜
        """
        attention_weights = {}
        
        query_processor = self.processors.get(query_modality)
        if not query_processor:
            return attention_weights
        
        query_embedding = query_processor.process(query_data)
        
        for inp in context_inputs:
            if inp.modality == query_modality:
                continue
            
            processor = self.processors.get(inp.modality)
            if processor:
                context_embedding = processor.process(inp.data)
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                dot_product = sum(q * c for q, c in zip(query_embedding, context_embedding))
                norm_q = sum(q ** 2 for q in query_embedding) ** 0.5
                norm_c = sum(c ** 2 for c in context_embedding) ** 0.5
                
                if norm_q > 0 and norm_c > 0:
                    similarity = dot_product / (norm_q * norm_c)
                else:
                    similarity = 0.0
                
                # ê¸°ì¡´ ê°€ì¤‘ì¹˜ì™€ ê²°í•©
                if inp.modality in attention_weights:
                    attention_weights[inp.modality] = max(
                        attention_weights[inp.modality],
                        similarity
                    )
                else:
                    attention_weights[inp.modality] = similarity
        
        return attention_weights
    
    def get_recent_perceptions(
        self,
        modality: Optional[ModalityType] = None,
        limit: int = 10
    ) -> List[PerceptualInput]:
        """ìµœê·¼ ì¸ì‹ ë°˜í™˜"""
        if modality:
            filtered = [p for p in self.perception_buffer if p.modality == modality]
        else:
            filtered = self.perception_buffer
        
        return filtered[-limit:]
    
    def enable_modality(self, modality: ModalityType) -> None:
        """ëª¨ë‹¬ë¦¬í‹° í™œì„±í™”"""
        if modality in self.processors:
            self.processors[modality].is_enabled = True
    
    def disable_modality(self, modality: ModalityType) -> None:
        """ëª¨ë‹¬ë¦¬í‹° ë¹„í™œì„±í™”"""
        if modality in self.processors:
            self.processors[modality].is_enabled = False
    
    def get_enabled_modalities(self) -> List[ModalityType]:
        """í™œì„±í™”ëœ ëª¨ë‹¬ë¦¬í‹° ëª©ë¡"""
        return [
            modality for modality, processor in self.processors.items()
            if processor.is_enabled
        ]


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ‘ï¸ MultiModalPerceptionEngine Unit Test")
    print("="*60)
    
    engine = MultiModalPerceptionEngine()
    
    # ì¸ì‹ë¡  ì¶œë ¥
    print("\n" + engine.explain_meaning())
    
    # í…ìŠ¤íŠ¸ ì¸ì‹
    print("\n[í…ìŠ¤íŠ¸ ì¸ì‹]")
    text_input = engine.perceive(
        ModalityType.TEXT,
        "ì•ˆë…•í•˜ì„¸ìš”, ì—˜ë¦¬ì‹œì•„ì…ë‹ˆë‹¤!",
        confidence=0.95
    )
    print(f"ëª¨ë‹¬ë¦¬í‹°: {text_input.modality.value}")
    print(f"í™•ì‹ ë„: {text_input.confidence}")
    
    # í–‰ë™ ì¸ì‹
    print("\n[í–‰ë™ ì¸ì‹]")
    action_input = engine.perceive(
        ModalityType.ACTION,
        {"type": "speak", "target": "user", "intensity": 0.8},
        confidence=0.9
    )
    
    # í†µí•©
    print("\n[ë‹¤ì¤‘ ëª¨ë‹¬ í†µí•©]")
    representation = engine.integrate([text_input, action_input])
    print(f"ëª¨ë‹¬ë¦¬í‹°ë“¤: {[m.value for m in representation.modalities]}")
    print(f"í˜„ì €ì„±: {representation.salience:.3f}")
    print(f"í•´ì„: {representation.interpretations}")
    
    # êµì°¨ ëª¨ë‹¬ ì£¼ì˜
    print("\n[êµì°¨ ëª¨ë‹¬ ì£¼ì˜]")
    attention = engine.cross_modal_attention(
        ModalityType.TEXT,
        "ë¬´ì—‡ì„ í•˜ê³  ìˆë‚˜ìš”?",
        [action_input]
    )
    print(f"ì£¼ì˜ ê°€ì¤‘ì¹˜: {attention}")
    
    print("\nâœ… MultiModalPerceptionEngine test complete!")
    print("="*60)
