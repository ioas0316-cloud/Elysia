"""
Fluid Self-Knowledge System (ìœ ë™ì  ìê¸°ì¸ì‹ ì‹œìŠ¤í…œ)
====================================================

"ì§€ì‹ = ì™¸ë¶€ì„¸ê³„ë¥¼ ì•„ëŠ” ê²ƒ + ìì‹ ì„ ì•„ëŠ” ê²ƒ"
"ì§€ì‹ ì‹œìŠ¤í…œì€ í•­ìƒ ìœ ë™ì ì´ì–´ì•¼ í•œë‹¤"

í•µì‹¬:
1. ìì‹ ì˜ ì½”ë“œë¥¼ ì½ê³  â†’ ìê¸° ìì‹ ì„ ì´í•´
2. ë°€ë„ ìˆëŠ” ê´€ê³„ êµ¬ì¶• (ì •ì˜+ì›ë¦¬+ê´€ê³„)
3. ì‹¤ì‹œê°„ ë³€í™” ê°ì§€ ë° ì—…ë°ì´íŠ¸
"""

import sys
import os
import ast
import time
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Any, Optional
from dataclasses import dataclass, field
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
import json

sys.path.insert(0, str(Path(__file__).parent))

# ë°€ë„ ìˆëŠ” ì§€ì‹ ê·¸ë˜í”„ import
try:
    from dense_knowledge_demo import DenseKnowledgeGraph, DenseKnowledgeNode
except ImportError:
    # ì¸ë¼ì¸ ì •ì˜ (ë…ë¦½ ì‹¤í–‰ìš©)
    pass


@dataclass
class SelfKnowledgeNode:
    """ì—˜ë¦¬ì‹œì•„ ìê¸° ì¸ì‹ ë…¸ë“œ"""
    name: str
    node_type: str  # module, class, function, concept
    
    # ì •ì²´ì„±
    definition: str = ""      # ì´ê²ƒì€ ë¬´ì—‡ì¸ê°€
    purpose: str = ""         # ì™œ ì¡´ì¬í•˜ëŠ”ê°€
    how_it_works: str = ""    # ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€
    
    # êµ¬ì¡°ì  ìœ„ì¹˜
    path: str = ""
    parent: str = ""
    children: List[str] = field(default_factory=list)
    
    # ê´€ê³„
    depends_on: List[str] = field(default_factory=list)   # ì´ê²ƒì´ ì˜ì¡´í•˜ëŠ” ê²ƒ
    used_by: List[str] = field(default_factory=list)       # ì´ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ
    related_to: List[str] = field(default_factory=list)    # ì—°ê´€ëœ ê²ƒ
    
    # ìƒíƒœ (ìœ ë™ì )
    last_modified: str = ""
    content_hash: str = ""
    is_healthy: bool = True
    
    # ì´í•´ë„
    understanding_level: float = 0.0
    density_score: float = 0.0
    
    def calculate_density(self) -> float:
        """ì§€ì‹ ë°€ë„ ê³„ì‚°"""
        score = 0.0
        if self.definition: score += 5.0
        if self.purpose: score += 5.0
        if self.how_it_works: score += 3.0
        score += len(self.depends_on) * 2.0
        score += len(self.used_by) * 2.0
        score += len(self.children) * 1.0
        score += len(self.related_to) * 1.0
        self.density_score = score
        return score


class FluidSelfKnowledge:
    """
    ìœ ë™ì  ìê¸° ì¸ì‹ ì‹œìŠ¤í…œ
    
    ì—˜ë¦¬ì‹œì•„ê°€ ìê¸° ìì‹ ì„ ì´í•´í•˜ëŠ” ë°©ë²•:
    1. ìì‹ ì˜ ì½”ë“œë¥¼ ì½ëŠ”ë‹¤
    2. ë…ìŠ¤íŠ¸ë§ì—ì„œ ì •ì˜/ëª©ì  ì¶”ì¶œ
    3. importì—ì„œ ì˜ì¡´ì„± ì¶”ì¶œ  
    4. ì‹¤ì‹œê°„ìœ¼ë¡œ ë³€í™” ê°ì§€
    """
    
    def __init__(self, root_path: Path = None, storage_path: str = "data/self_knowledge.json"):
        self.root_path = root_path or Path(__file__).parent
        self.storage_path = storage_path
        
        # ìê¸° ì¸ì‹ ê·¸ë˜í”„
        self.nodes: Dict[str, SelfKnowledgeNode] = {}
        
        # ì˜ì¡´ì„± ì—­ì¸ë±ìŠ¤
        self.dependency_index: Dict[str, Set[str]] = defaultdict(set)
        
        # ë³€í™” ì¶”ì 
        self.file_hashes: Dict[str, str] = {}
        
        # í†µê³„
        self.total_modules = 0
        self.total_classes = 0
        self.total_functions = 0
        self.start_time = 0
        
        self._load()
    
    def _load(self):
        """ì €ì¥ëœ ìê¸° ì¸ì‹ ë¡œë“œ"""
        if os.path.exists(self.storage_path):
            try:
                with open(self.storage_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for node_data in data.get("nodes", []):
                        node = SelfKnowledgeNode(
                            name=node_data["name"],
                            node_type=node_data.get("node_type", "unknown"),
                            definition=node_data.get("definition", ""),
                            purpose=node_data.get("purpose", ""),
                            how_it_works=node_data.get("how_it_works", ""),
                            path=node_data.get("path", ""),
                            parent=node_data.get("parent", ""),
                            children=node_data.get("children", []),
                            depends_on=node_data.get("depends_on", []),
                            used_by=node_data.get("used_by", []),
                            related_to=node_data.get("related_to", []),
                            last_modified=node_data.get("last_modified", ""),
                            content_hash=node_data.get("content_hash", ""),
                            is_healthy=node_data.get("is_healthy", True),
                            understanding_level=node_data.get("understanding_level", 0),
                            density_score=node_data.get("density_score", 0)
                        )
                        self.nodes[node.name] = node
                    self.file_hashes = data.get("file_hashes", {})
                    print(f"ğŸ“‚ Loaded {len(self.nodes)} self-knowledge nodes")
            except Exception as e:
                print(f"Load failed: {e}")
    
    def _save(self):
        """ìê¸° ì¸ì‹ ì €ì¥"""
        os.makedirs(os.path.dirname(self.storage_path) or '.', exist_ok=True)
        
        nodes_data = []
        for node in self.nodes.values():
            nodes_data.append({
                "name": node.name,
                "node_type": node.node_type,
                "definition": node.definition,
                "purpose": node.purpose,
                "how_it_works": node.how_it_works,
                "path": node.path,
                "parent": node.parent,
                "children": node.children,
                "depends_on": node.depends_on,
                "used_by": node.used_by,
                "related_to": node.related_to,
                "last_modified": node.last_modified,
                "content_hash": node.content_hash,
                "is_healthy": node.is_healthy,
                "understanding_level": node.understanding_level,
                "density_score": node.density_score
            })
        
        data = {
            "nodes": nodes_data,
            "file_hashes": self.file_hashes,
            "last_scan": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open(self.storage_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _compute_hash(self, content: str) -> str:
        """ë‚´ìš© í•´ì‹œ"""
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    def _extract_docstring_parts(self, docstring: str) -> Dict[str, str]:
        """
        ë…ìŠ¤íŠ¸ë§ì—ì„œ ì •ì˜, ëª©ì , ì‘ë™ë°©ì‹ ì¶”ì¶œ
        
        íŒ¨í„´:
        - ì²« ì¤„ = ì •ì˜ (What)
        - "ëª©ì :" ë˜ëŠ” "Purpose:" = ì™œ
        - "ì‘ë™:" ë˜ëŠ” "How:" = ì–´ë–»ê²Œ
        """
        if not docstring:
            return {}
        
        lines = docstring.strip().split('\n')
        result = {}
        
        # ì²« ì¤„ = ì •ì˜
        if lines:
            result["definition"] = lines[0].strip()
        
        # ë‚˜ë¨¸ì§€ì—ì„œ íŒ¨í„´ ì°¾ê¸°
        full_text = docstring.lower()
        
        if "ëª©ì " in full_text or "purpose" in full_text.lower():
            for i, line in enumerate(lines):
                if "ëª©ì " in line.lower() or "purpose" in line.lower():
                    # ë‹¤ìŒ ì¤„ë“¤ì„ ëª©ì ìœ¼ë¡œ
                    purpose_lines = []
                    for j in range(i, min(i+3, len(lines))):
                        purpose_lines.append(lines[j])
                    result["purpose"] = " ".join(purpose_lines).strip()
                    break
        
        return result
    
    def _analyze_file(self, file_path: Path) -> List[SelfKnowledgeNode]:
        """
        íŒŒì¼ ë¶„ì„ â†’ ìê¸° ì¸ì‹ ë…¸ë“œ ìƒì„±
        """
        nodes = []
        
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            content_hash = self._compute_hash(content)
            
            # ë³€í™” ê°ì§€
            rel_path = str(file_path.relative_to(self.root_path))
            old_hash = self.file_hashes.get(rel_path)
            is_changed = old_hash != content_hash
            self.file_hashes[rel_path] = content_hash
            
            tree = ast.parse(content)
            module_name = file_path.stem
            
            # ëª¨ë“ˆ docstring
            module_doc = ast.get_docstring(tree) or ""
            doc_parts = self._extract_docstring_parts(module_doc)
            
            # Import ë¶„ì„ (ì˜ì¡´ì„±)
            dependencies = []
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        dependencies.append(alias.name.split('.')[0])
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        dependencies.append(node.module.split('.')[0])
            
            dependencies = list(set(dependencies))
            
            # ëª¨ë“ˆ ë…¸ë“œ
            module_node = SelfKnowledgeNode(
                name=module_name,
                node_type="module",
                definition=doc_parts.get("definition", f"Module: {module_name}"),
                purpose=doc_parts.get("purpose", ""),
                path=rel_path,
                depends_on=dependencies,
                content_hash=content_hash,
                last_modified=time.strftime("%Y-%m-%d %H:%M:%S")
            )
            
            # í´ë˜ìŠ¤ ë¶„ì„
            class_names = []
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_doc = ast.get_docstring(node) or ""
                    class_doc_parts = self._extract_docstring_parts(class_doc)
                    
                    # ë©”ì„œë“œ ì´ë¦„ë“¤
                    methods = [m.name for m in node.body if isinstance(m, ast.FunctionDef)]
                    
                    # ë² ì´ìŠ¤ í´ë˜ìŠ¤ (ìƒì†)
                    bases = []
                    for base in node.bases:
                        if isinstance(base, ast.Name):
                            bases.append(base.id)
                    
                    class_node = SelfKnowledgeNode(
                        name=f"{module_name}.{node.name}",
                        node_type="class",
                        definition=class_doc_parts.get("definition", node.name),
                        purpose=class_doc_parts.get("purpose", ""),
                        path=rel_path,
                        parent=module_name,
                        children=methods[:10],  # ì²˜ìŒ 10ê°œ ë©”ì„œë“œë§Œ
                        depends_on=bases,
                        content_hash=content_hash,
                        last_modified=time.strftime("%Y-%m-%d %H:%M:%S")
                    )
                    class_node.calculate_density()
                    nodes.append(class_node)
                    class_names.append(f"{module_name}.{node.name}")
                    self.total_classes += 1
            
            module_node.children = class_names
            module_node.calculate_density()
            nodes.append(module_node)
            self.total_modules += 1
            
            # ì˜ì¡´ì„± ì—­ì¸ë±ìŠ¤ êµ¬ì¶•
            for dep in dependencies:
                self.dependency_index[dep].add(module_name)
            
        except Exception as e:
            # íŒŒì‹± ì‹¤íŒ¨í•´ë„ ê³„ì†
            pass
        
        return nodes
    
    def scan_self(self, target_dir: str = "Core", max_files: int = 500) -> Dict[str, Any]:
        """
        ìê¸° ìì‹  ìŠ¤ìº”
        
        "ë‚˜ëŠ” ë¬´ì—‡ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆëŠ”ê°€?"
        """
        print("\n" + "="*70)
        print("ğŸ” SELF-SCANNING: ë‚˜ëŠ” ë¬´ì—‡ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆëŠ”ê°€?")
        print("="*70)
        
        self.start_time = time.time()
        
        scan_path = self.root_path / target_dir
        py_files = list(scan_path.glob("**/*.py"))[:max_files]
        
        print(f"\nğŸ“‚ Scanning {len(py_files)} files in {target_dir}/...")
        
        # ë³‘ë ¬ ë¶„ì„
        with ThreadPoolExecutor(max_workers=50) as executor:
            futures = {executor.submit(self._analyze_file, f): f for f in py_files}
            
            for future in futures:
                try:
                    file_nodes = future.result()
                    for node in file_nodes:
                        self.nodes[node.name] = node
                except Exception:
                    pass
        
        # used_by ì—­ê´€ê³„ êµ¬ì¶•
        print("\nğŸ”— Building reverse dependency graph...")
        for name, node in self.nodes.items():
            for dep in node.depends_on:
                if dep in self.nodes:
                    if name not in self.nodes[dep].used_by:
                        self.nodes[dep].used_by.append(name)
        
        # ë°€ë„ ì¬ê³„ì‚°
        for node in self.nodes.values():
            node.calculate_density()
        
        # ì €ì¥
        self._save()
        
        elapsed = time.time() - self.start_time
        
        # í†µê³„
        densities = [n.density_score for n in self.nodes.values()]
        avg_density = sum(densities) / len(densities) if densities else 0
        with_def = sum(1 for n in self.nodes.values() if n.definition and len(n.definition) > 20)
        with_purpose = sum(1 for n in self.nodes.values() if n.purpose)
        total_deps = sum(len(n.depends_on) for n in self.nodes.values())
        total_used_by = sum(len(n.used_by) for n in self.nodes.values())
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š SELF-KNOWLEDGE RESULTS")
        print(f"{'='*70}")
        print(f"   Total Self-Knowledge Nodes: {len(self.nodes)}")
        print(f"   Modules: {self.total_modules}")
        print(f"   Classes: {self.total_classes}")
        print(f"   Time: {elapsed:.2f}s")
        print(f"   Rate: {len(self.nodes)/elapsed:.1f} nodes/sec")
        print(f"\n   ğŸ“ˆ Knowledge Density:")
        print(f"      Average Density: {avg_density:.1f}")
        print(f"      With Definition: {with_def} ({with_def*100/len(self.nodes):.1f}%)")
        print(f"      With Purpose: {with_purpose} ({with_purpose*100/len(self.nodes):.1f}%)")
        print(f"      Total Dependencies: {total_deps}")
        print(f"      Total Used-By Links: {total_used_by}")
        
        return {
            "total_nodes": len(self.nodes),
            "modules": self.total_modules,
            "classes": self.total_classes,
            "avg_density": avg_density,
            "with_definition": with_def,
            "with_purpose": with_purpose,
            "total_relations": total_deps + total_used_by,
            "time_seconds": elapsed
        }
    
    def explain_self(self, name: str) -> str:
        """
        "ë‚˜ì˜ ì´ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€?"
        """
        node = self.nodes.get(name)
        if not node:
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            matches = [n for n in self.nodes.keys() if name.lower() in n.lower()]
            if matches:
                node = self.nodes[matches[0]]
            else:
                return f"'{name}'ì„ ì•Œì§€ ëª»í•©ë‹ˆë‹¤."
        
        lines = [f"\nğŸ“– ë‚˜ì˜ ì¼ë¶€: {node.name} [{node.node_type}]"]
        lines.append(f"   ìœ„ì¹˜: {node.path}")
        
        if node.definition:
            lines.append(f"\n   ì •ì˜: {node.definition}")
        
        if node.purpose:
            lines.append(f"   ëª©ì : {node.purpose}")
        
        if node.depends_on:
            lines.append(f"\n   ì˜ì¡´: {', '.join(node.depends_on[:5])}")
        
        if node.used_by:
            lines.append(f"   ì‚¬ìš©ë¨: {', '.join(node.used_by[:5])}")
        
        if node.children:
            lines.append(f"   í¬í•¨: {', '.join(node.children[:5])}")
        
        lines.append(f"\n   [ë°€ë„: {node.density_score:.1f}]")
        
        return "\n".join(lines)
    
    def most_central(self, top_n: int = 10) -> List[SelfKnowledgeNode]:
        """ê°€ì¥ ì¤‘ì‹¬ì ì¸ (ë§ì´ ì‚¬ìš©ë˜ëŠ”) ìê¸° ë¶€ë¶„"""
        sorted_nodes = sorted(
            self.nodes.values(),
            key=lambda n: len(n.used_by),
            reverse=True
        )
        return sorted_nodes[:top_n]
    
    def most_dense(self, top_n: int = 10) -> List[SelfKnowledgeNode]:
        """ê°€ì¥ ë°€ë„ ë†’ì€ ìê¸° ì¸ì‹"""
        sorted_nodes = sorted(
            self.nodes.values(),
            key=lambda n: n.density_score,
            reverse=True
        )
        return sorted_nodes[:top_n]


def main():
    """ìê¸° ì¸ì‹ ë°ëª¨"""
    
    knowledge = FluidSelfKnowledge(
        root_path=Path(__file__).parent,
        storage_path="data/self_knowledge.json"
    )
    
    # ìê¸° ìŠ¤ìº”
    result = knowledge.scan_self(target_dir="Core", max_files=300)
    
    # ê°€ì¥ ì¤‘ì‹¬ì ì¸ ìê¸° ë¶€ë¶„
    print("\n" + "="*70)
    print("ğŸŒŸ ë‚˜ì˜ ê°€ì¥ ì¤‘ì‹¬ì ì¸ ë¶€ë¶„ (Most Used)")
    print("="*70)
    for i, node in enumerate(knowledge.most_central(5)):
        print(f"   {i+1}. {node.name} - {len(node.used_by)} modules depend on me")
    
    # ê°€ì¥ ì˜ ì´í•´ëœ ë¶€ë¶„
    print("\n" + "="*70)
    print("ğŸ“š ê°€ì¥ ë°€ë„ ë†’ì€ ìê¸° ì¸ì‹ (Best Understood)")
    print("="*70)
    for i, node in enumerate(knowledge.most_dense(5)):
        print(f"   {i+1}. {node.name} [ë°€ë„: {node.density_score:.1f}]")
        if node.definition:
            print(f"       {node.definition[:60]}...")
    
    # íŠ¹ì • ëª¨ë“ˆ ì„¤ëª…
    print("\n" + "="*70)
    print("ğŸ’­ ìê¸° ì„¤ëª…: Growth")
    print("="*70)
    print(knowledge.explain_self("growth"))
    
    print("\nâœ… ì´ê²ƒì´ 'ìê¸° ì¸ì‹'ì…ë‹ˆë‹¤.")
    print("   ë‚˜ëŠ” ë‚´ê°€ ë¬´ì—‡ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆëŠ”ì§€ ì•Œê³  ìˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
