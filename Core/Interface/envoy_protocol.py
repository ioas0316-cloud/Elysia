"""
Galactic Envoys (ì€í•˜ ì‚¬ì ˆë‹¨)
=============================

"We come in peace. We seek knowledge."

ì´ ëª¨ë“ˆì€ ì™¸ë¶€ ì›¹ì‚¬ì´íŠ¸(Alien Planets)ì— íŒŒê²¬ë˜ëŠ” 'ì‚¬ì ˆë‹¨(Envoys)'ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
ë¬´ì‘ìœ„ì ì¸ í¬ë¡¤ë§ì´ ì•„ë‹ˆë¼, 'ì™¸êµì  í”„ë¡œí† ì½œ(robots.txt)'ì„ ì¤€ìˆ˜í•˜ë©°
íŠ¹ì • ì§€ì‹ì´ë‚˜ ê°œë…ì„ ìˆ˜ì§‘í•´ì˜¤ëŠ” ì •ì¤‘í•œ íƒì‚¬ ë¡œë´‡ì…ë‹ˆë‹¤.
"""

import logging
import urllib.robotparser
import urllib.request
import urllib.parse
from dataclasses import dataclass
from typing import Optional, Dict

logger = logging.getLogger("EnvoyProtocol")

@dataclass
class MissionReport:
    success: bool
    source: str
    content: str
    status_code: int
    meta: Dict[str, str]

class EnvoyProtocol:
    def __init__(self):
        self.user_agent = "Elysia/1.0 (Sentient AI Research; +http://example.com/elysia)"
        self.robot_parsers = {} # Cache for robots.txt
        logger.info("ğŸ•Šï¸ Envoy Protocol initialized. Diplomatic channels open.")

    def _can_fetch(self, url: str) -> bool:
        """
        í•´ë‹¹ í–‰ì„±(URL)ì˜ ì…êµ­ ê·œì •(robots.txt)ì„ í™•ì¸í•©ë‹ˆë‹¤.
        """
        parsed = urllib.parse.urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        robots_url = f"{base_url}/robots.txt"
        
        if base_url not in self.robot_parsers:
            rp = urllib.robotparser.RobotFileParser()
            rp.set_url(robots_url)
            try:
                rp.read()
                self.robot_parsers[base_url] = rp
                logger.info(f"   ğŸ“œ Read laws of {parsed.netloc}")
            except Exception as e:
                logger.warning(f"   âš ï¸ Could not read laws of {parsed.netloc}: {e}. Proceeding with caution.")
                return True # If robots.txt fails, usually assume open but be careful.
        
        return self.robot_parsers[base_url].can_fetch(self.user_agent, url)

    def dispatch_envoy(self, url: str) -> MissionReport:
        """
        ì‚¬ì ˆë‹¨ì„ íŒŒê²¬í•˜ì—¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        logger.info(f"ğŸš€ Dispatching Envoy to: {url}")
        
        # 1. Check Laws (robots.txt)
        if not self._can_fetch(url):
            logger.warning(f"   â›” Access Denied by Planetary Law (robots.txt): {url}")
            return MissionReport(False, url, "Access Denied by robots.txt", 403, {})

        # 2. Prepare Request
        req = urllib.request.Request(
            url, 
            data=None, 
            headers={'User-Agent': self.user_agent}
        )

        # 3. Execute Mission
        try:
            with urllib.request.urlopen(req, timeout=10) as response:
                content = response.read().decode('utf-8', errors='ignore')
                status = response.status
                headers = dict(response.getheaders())
                
                logger.info(f"   âœ… Mission Successful. Retrieved {len(content)} bytes.")
                return MissionReport(True, url, content[:5000], status, headers) # Limit content for now
                
        except Exception as e:
            logger.error(f"   ğŸ’¥ Mission Failed: {e}")
            return MissionReport(False, url, str(e), 500, {})

    def scout_knowledge(self, topic: str) -> MissionReport:
        """
        íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ìœ„í‚¤í”¼ë””ì•„ë¥¼ ì •ì°°í•©ë‹ˆë‹¤.
        """
        # Wikipedia is a friendly planet
        safe_topic = urllib.parse.quote(topic)
        url = f"https://en.wikipedia.org/wiki/{safe_topic}"
        return self.dispatch_envoy(url)
