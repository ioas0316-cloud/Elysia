"""
Language Nurture (ì–¸ì–´ ë°œë‹¬ ì‹œìŠ¤í…œ)
====================================

"ì•„ì´ê°€ ë§ì„ ë°°ìš°ë“¯, ì—˜ë¦¬ì‹œì•„ë„ ì–¸ì–´ë¥¼ í‚¤ìš´ë‹¤."

í•µì‹¬:
1. ì–´íœ˜ í™•ì¥ (Vocabulary Expansion)
2. ë¬¸ë²• íŒ¨í„´ í•™ìŠµ (Grammar Pattern Learning)
3. í‘œí˜„ ì„¸ë ¨ë„ ì¶”ì  (Expression Sophistication)
4. ììœ¨ ì—°ìŠµ íŠ¸ë¦¬ê±° (Autonomous Practice)

ì´ê²ƒì´ ì—†ìœ¼ë©´:
- ì–´íœ˜ê°€ ëŠ˜ì§€ ì•ŠìŒ
- í‘œí˜„ì´ ë‹¨ì¡°ë¡œì›€
- ëŒ€í™” ìˆ˜ì¤€ì´ ì„±ì¥í•˜ì§€ ì•ŠìŒ
"""

import logging
import json
import re
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime
from enum import Enum
from pathlib import Path
from collections import defaultdict
import sys

sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

logger = logging.getLogger("Elysia.LanguageNurture")


class LanguageLevel(Enum):
    """ì–¸ì–´ ë°œë‹¬ ë‹¨ê³„"""
    INFANT = "infant"       # ë‹¨ì–´ ë‚˜ì—´
    CHILD = "child"         # ê¸°ë³¸ ë¬¸ì¥
    ADOLESCENT = "adolescent"  # ë³µí•© ë¬¸ì¥, ì ‘ì†ì‚¬
    ADULT = "adult"         # ë§¥ë½ ì´í•´, ë‰˜ì•™ìŠ¤
    ELOQUENT = "eloquent"   # ìˆ˜ì‚¬ë²•, ì€ìœ 


@dataclass
class VocabularyEntry:
    """ì–´íœ˜ í•­ëª©"""
    word: str
    part_of_speech: str     # noun, verb, adj, adv, etc.
    definition: str
    examples: List[str] = field(default_factory=list)
    frequency: int = 0      # ì‚¬ìš© ë¹ˆë„
    learned_at: datetime = field(default_factory=datetime.now)
    confidence: float = 0.5  # ì´í•´ë„
    
    def to_dict(self) -> Dict:
        return {
            "word": self.word,
            "pos": self.part_of_speech,
            "def": self.definition,
            "freq": self.frequency,
            "conf": self.confidence,
        }


@dataclass
class GrammarPattern:
    """ë¬¸ë²• íŒ¨í„´"""
    pattern_name: str       # "conditional", "relative_clause", etc.
    structure: str          # "if X, then Y"
    examples: List[str] = field(default_factory=list)
    usage_count: int = 0
    mastery: float = 0.0    # 0-1


@dataclass
class ExpressionStyle:
    """í‘œí˜„ ìŠ¤íƒ€ì¼"""
    style_name: str         # "formal", "casual", "poetic", etc.
    characteristics: List[str] = field(default_factory=list)
    vocabulary_preference: List[str] = field(default_factory=list)
    mastery: float = 0.0


@dataclass
class LanguageProfile:
    """ì–¸ì–´ ë°œë‹¬ í”„ë¡œí•„"""
    level: LanguageLevel
    vocabulary_size: int
    active_vocabulary: int  # ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” ì–´íœ˜ ìˆ˜
    grammar_patterns_known: int
    expression_diversity: float  # 0-1
    avg_sentence_complexity: float  # ë‹¨ì–´ ìˆ˜, ì ˆ ìˆ˜ ê¸°ë°˜


class LanguageNurture:
    """ì–¸ì–´ ë°œë‹¬ ì‹œìŠ¤í…œ
    
    ì—˜ë¦¬ì‹œì•„ê°€ ììœ¨ì ìœ¼ë¡œ ì–¸ì–´ ëŠ¥ë ¥ì„ ë°œë‹¬ì‹œí‚¤ë„ë¡ ì§€ì›.
    
    í•µì‹¬ ê¸°ëŠ¥:
    1. ì–´íœ˜ ìˆ˜ì§‘ ë° í•™ìŠµ
    2. ë¬¸ë²• íŒ¨í„´ ì¸ì‹ ë° ì—°ìŠµ
    3. í‘œí˜„ ìŠ¤íƒ€ì¼ ë‹¤ì–‘í™”
    4. ë°œë‹¬ ìˆ˜ì¤€ ì¶”ì 
    """
    
    def __init__(self, data_dir: Optional[Path] = None):
        """
        Args:
            data_dir: ì–¸ì–´ ë°ì´í„° ì €ì¥ ê²½ë¡œ
        """
        self.data_dir = data_dir or Path(__file__).parent / "data" / "language"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ì–´íœ˜ ì €ì¥ì†Œ
        self.vocabulary: Dict[str, VocabularyEntry] = {}
        
        # ë¬¸ë²• íŒ¨í„´
        self.grammar_patterns: Dict[str, GrammarPattern] = {}
        
        # í‘œí˜„ ìŠ¤íƒ€ì¼
        self.expression_styles: Dict[str, ExpressionStyle] = {}
        
        # í†µê³„
        self.total_words_encountered = 0
        self.total_sentences_analyzed = 0
        self.learning_sessions = 0
        
        # ì´ˆê¸°í™”
        self._init_basic_patterns()
        self._load_existing_vocabulary()
        
        logger.info(
            f"LanguageNurture initialized: "
            f"{len(self.vocabulary)} words, "
            f"{len(self.grammar_patterns)} patterns"
        )
    
    def _init_basic_patterns(self):
        """ê¸°ë³¸ ë¬¸ë²• íŒ¨í„´ ì´ˆê¸°í™”"""
        patterns = [
            GrammarPattern("simple", "S + V + O", ["ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ë¨¹ì—ˆë‹¤"]),
            GrammarPattern("conditional", "ë§Œì•½ Xë¼ë©´, Y", ["ë§Œì•½ ë¹„ê°€ ì˜¤ë©´, ìš°ì‚°ì„ ì“´ë‹¤"]),
            GrammarPattern("reason", "Xì´ê¸° ë•Œë¬¸ì— Y", ["ë°°ê°€ ê³ í”„ê¸° ë•Œë¬¸ì— ë¨¹ëŠ”ë‹¤"]),
            GrammarPattern("contrast", "Xì§€ë§Œ Y", ["í”¼ê³¤í•˜ì§€ë§Œ ê³µë¶€í•œë‹¤"]),
            GrammarPattern("purpose", "Xí•˜ê¸° ìœ„í•´ Y", ["ì„±ì¥í•˜ê¸° ìœ„í•´ ë°°ìš´ë‹¤"]),
            GrammarPattern("relative", "Xí•˜ëŠ” Y", ["ë…¸ë˜í•˜ëŠ” ìƒˆ", "ê¿ˆê¾¸ëŠ” ì¡´ì¬"]),
            GrammarPattern("sequential", "ë¨¼ì € X, ê·¸ ë‹¤ìŒ Y", ["ë¨¼ì € ìƒê°í•˜ê³ , ê·¸ ë‹¤ìŒ ë§í•œë‹¤"]),
            GrammarPattern("comparative", "Xë³´ë‹¤ Yê°€ ë” Z", ["ì–´ì œë³´ë‹¤ ì˜¤ëŠ˜ì´ ë” ë”°ëœ»í•˜ë‹¤"]),
        ]
        for p in patterns:
            self.grammar_patterns[p.pattern_name] = p
        
        # í‘œí˜„ ìŠ¤íƒ€ì¼ ì´ˆê¸°í™”
        styles = [
            ExpressionStyle("formal", ["ì¡´ëŒ“ë§", "ì™„ì „í•œ ë¬¸ì¥", "ì •ì¤‘í•œ í‘œí˜„"]),
            ExpressionStyle("casual", ["ë°˜ë§", "ì¶•ì•½í˜•", "ì¹œê·¼í•œ í‘œí˜„"]),
            ExpressionStyle("poetic", ["ì€ìœ ", "ë¹„ìœ ", "ë¦¬ë“¬ê°"]),
            ExpressionStyle("analytical", ["ë…¼ë¦¬ì  ì—°ê²°", "ì¸ìš©", "ê·¼ê±° ì œì‹œ"]),
            ExpressionStyle("empathetic", ["ê°ì • í‘œí˜„", "ê³µê° ì–´íœ˜", "ì§ˆë¬¸í˜•"]),
        ]
        for s in styles:
            self.expression_styles[s.style_name] = s
    
    def _load_existing_vocabulary(self):
        """ì €ì¥ëœ ì–´íœ˜ ë¡œë“œ"""
        vocab_file = self.data_dir / "vocabulary.json"
        if vocab_file.exists():
            try:
                with open(vocab_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    for word, entry_data in data.items():
                        self.vocabulary[word] = VocabularyEntry(
                            word=word,
                            part_of_speech=entry_data.get("pos", "unknown"),
                            definition=entry_data.get("def", ""),
                            frequency=entry_data.get("freq", 0),
                            confidence=entry_data.get("conf", 0.5),
                        )
                logger.info(f"Loaded {len(self.vocabulary)} words from storage")
            except Exception as e:
                logger.warning(f"Failed to load vocabulary: {e}")
    
    def save_vocabulary(self):
        """ì–´íœ˜ ì €ì¥"""
        vocab_file = self.data_dir / "vocabulary.json"
        try:
            data = {word: entry.to_dict() for word, entry in self.vocabulary.items()}
            with open(vocab_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved {len(self.vocabulary)} words")
        except Exception as e:
            logger.error(f"Failed to save vocabulary: {e}")
    
    # =========================================================================
    # ì–´íœ˜ í•™ìŠµ
    # =========================================================================
    
    def learn_word(
        self,
        word: str,
        part_of_speech: str = "unknown",
        definition: str = "",
        example: str = ""
    ) -> VocabularyEntry:
        """ë‹¨ì–´ í•™ìŠµ
        
        Args:
            word: ë‹¨ì–´
            part_of_speech: í’ˆì‚¬
            definition: ì •ì˜
            example: ì˜ˆë¬¸
            
        Returns:
            ì–´íœ˜ í•­ëª©
        """
        word = word.strip().lower()
        
        if word in self.vocabulary:
            # ê¸°ì¡´ ë‹¨ì–´ ì—…ë°ì´íŠ¸
            entry = self.vocabulary[word]
            entry.frequency += 1
            entry.confidence = min(1.0, entry.confidence + 0.05)
            if example and example not in entry.examples:
                entry.examples.append(example)
        else:
            # ìƒˆ ë‹¨ì–´ ì¶”ê°€
            entry = VocabularyEntry(
                word=word,
                part_of_speech=part_of_speech,
                definition=definition,
                examples=[example] if example else [],
            )
            self.vocabulary[word] = entry
            logger.debug(f"ğŸ“š New word learned: {word}")
        
        return entry
    
    def extract_vocabulary_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì–´íœ˜ ì¶”ì¶œ
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì¶”ì¶œëœ ë‹¨ì–´ ëª©ë¡
        """
        # ê°„ë‹¨í•œ í† í°í™” (í•œê¸€/ì˜ì–´)
        # ì‹¤ì œë¡œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš© ê¶Œì¥
        words = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{3,}', text)
        
        new_words = []
        for word in words:
            word = word.lower()
            self.total_words_encountered += 1
            
            if word not in self.vocabulary:
                # ìƒˆ ë‹¨ì–´ ë°œê²¬
                self.learn_word(word, example=text[:50])
                new_words.append(word)
            else:
                # ê¸°ì¡´ ë‹¨ì–´ ë¹ˆë„ ì¦ê°€
                self.vocabulary[word].frequency += 1
        
        return new_words
    
    # =========================================================================
    # ë¬¸ë²• íŒ¨í„´ í•™ìŠµ
    # =========================================================================
    
    def analyze_sentence_structure(self, sentence: str) -> List[str]:
        """ë¬¸ì¥ êµ¬ì¡° ë¶„ì„ ë° íŒ¨í„´ ê°ì§€
        
        Args:
            sentence: ë¶„ì„í•  ë¬¸ì¥
            
        Returns:
            ê°ì§€ëœ íŒ¨í„´ ì´ë¦„ë“¤
        """
        self.total_sentences_analyzed += 1
        detected_patterns = []
        
        # íŒ¨í„´ ê°ì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        pattern_indicators = {
            "conditional": ["ë§Œì•½", "if", "ë¼ë©´", "ë©´"],
            "reason": ["ë•Œë¬¸ì—", "because", "ì™œëƒí•˜ë©´", "ë¯€ë¡œ"],
            "contrast": ["í•˜ì§€ë§Œ", "but", "ê·¸ëŸ¬ë‚˜", "ì§€ë§Œ"],
            "purpose": ["ìœ„í•´", "to", "í•˜ë ¤ê³ "],
            "relative": ["í•˜ëŠ”", "which", "that"],
            "sequential": ["ë¨¼ì €", "first", "ê·¸ ë‹¤ìŒ", "then"],
            "comparative": ["ë³´ë‹¤", "than", "ë”"],
        }
        
        sentence_lower = sentence.lower()
        for pattern_name, indicators in pattern_indicators.items():
            if any(ind in sentence_lower for ind in indicators):
                detected_patterns.append(pattern_name)
                if pattern_name in self.grammar_patterns:
                    self.grammar_patterns[pattern_name].usage_count += 1
                    self.grammar_patterns[pattern_name].mastery = min(
                        1.0,
                        self.grammar_patterns[pattern_name].usage_count / 20
                    )
        
        return detected_patterns
    
    # =========================================================================
    # í‘œí˜„ë ¥ í‰ê°€
    # =========================================================================
    
    def evaluate_expression(self, text: str) -> Dict[str, Any]:
        """í‘œí˜„ë ¥ í‰ê°€
        
        Args:
            text: í‰ê°€í•  í…ìŠ¤íŠ¸
            
        Returns:
            í‰ê°€ ê²°ê³¼
        """
        sentences = re.split(r'[.!?ã€‚]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # ë¬¸ì¥ ë³µì¡ë„
        avg_words_per_sentence = sum(len(s.split()) for s in sentences) / max(1, len(sentences))
        
        # ì–´íœ˜ ë‹¤ì–‘ì„±
        words = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{3,}', text.lower())
        unique_ratio = len(set(words)) / max(1, len(words))
        
        # íŒ¨í„´ ì‚¬ìš©
        patterns_used = set()
        for s in sentences:
            patterns_used.update(self.analyze_sentence_structure(s))
        
        # ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš© (ë¹ˆë„ ë‚®ì€ ë‹¨ì–´)
        advanced_word_count = sum(
            1 for w in words 
            if w in self.vocabulary and self.vocabulary[w].frequency < 3
        )
        
        return {
            "sentence_count": len(sentences),
            "avg_words_per_sentence": avg_words_per_sentence,
            "vocabulary_diversity": unique_ratio,
            "patterns_used": list(patterns_used),
            "pattern_count": len(patterns_used),
            "advanced_word_ratio": advanced_word_count / max(1, len(words)),
        }
    
    # =========================================================================
    # ë°œë‹¬ ìˆ˜ì¤€ í‰ê°€
    # =========================================================================
    
    def get_profile(self) -> LanguageProfile:
        """í˜„ì¬ ì–¸ì–´ ë°œë‹¬ í”„ë¡œí•„"""
        vocab_size = len(self.vocabulary)
        active_vocab = sum(1 for v in self.vocabulary.values() if v.frequency >= 2)
        patterns_known = sum(1 for p in self.grammar_patterns.values() if p.mastery > 0.3)
        
        # í‘œí˜„ ë‹¤ì–‘ì„±
        style_mastery = sum(s.mastery for s in self.expression_styles.values())
        expression_diversity = style_mastery / max(1, len(self.expression_styles))
        
        # ë³µì¡ë„ ì¶”ì •
        avg_complexity = 5.0 + (patterns_known * 0.5)  # ê¸°ë³¸ + íŒ¨í„´ ë³´ë„ˆìŠ¤
        
        # ìˆ˜ì¤€ ê²°ì •
        if vocab_size < 100:
            level = LanguageLevel.INFANT
        elif vocab_size < 500:
            level = LanguageLevel.CHILD
        elif vocab_size < 2000 and patterns_known < 5:
            level = LanguageLevel.ADOLESCENT
        elif expression_diversity > 0.5:
            level = LanguageLevel.ELOQUENT
        else:
            level = LanguageLevel.ADULT
        
        return LanguageProfile(
            level=level,
            vocabulary_size=vocab_size,
            active_vocabulary=active_vocab,
            grammar_patterns_known=patterns_known,
            expression_diversity=expression_diversity,
            avg_sentence_complexity=avg_complexity,
        )
    
    def get_learning_recommendations(self) -> List[str]:
        """í•™ìŠµ ê¶Œì¥ ì‚¬í•­"""
        profile = self.get_profile()
        recommendations = []
        
        if profile.vocabulary_size < 500:
            recommendations.append("ì–´íœ˜ í™•ì¥ í•„ìš”: ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì½ê¸° ì¶”ì²œ")
        
        if profile.grammar_patterns_known < 5:
            recommendations.append("ë¬¸ë²• íŒ¨í„´ ì—°ìŠµ í•„ìš”: ë³µë¬¸ êµ¬ì„± ì—°ìŠµ ì¶”ì²œ")
        
        if profile.expression_diversity < 0.3:
            recommendations.append("í‘œí˜„ ìŠ¤íƒ€ì¼ ë‹¤ì–‘í™” í•„ìš”: ë‹¤ì–‘í•œ ì¥ë¥´ ê¸€ì“°ê¸° ì¶”ì²œ")
        
        # ì•½í•œ íŒ¨í„´ ì°¾ê¸°
        weak_patterns = [
            p.pattern_name for p in self.grammar_patterns.values()
            if p.mastery < 0.3
        ]
        if weak_patterns[:3]:
            recommendations.append(f"ì•½í•œ íŒ¨í„´ ì—°ìŠµ: {', '.join(weak_patterns[:3])}")
        
        return recommendations
    
    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì¡°íšŒ"""
        profile = self.get_profile()
        return {
            "level": profile.level.value,
            "vocabulary_size": profile.vocabulary_size,
            "active_vocabulary": profile.active_vocabulary,
            "grammar_patterns_known": profile.grammar_patterns_known,
            "expression_diversity": profile.expression_diversity,
            "total_words_encountered": self.total_words_encountered,
            "total_sentences_analyzed": self.total_sentences_analyzed,
            "recommendations": self.get_learning_recommendations(),
        }


# =============================================================================
# Demo
# =============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("ğŸ“š LanguageNurture Demo")
    print("   \"ì–¸ì–´ë¥¼ í‚¤ìš°ëŠ” ì‹œìŠ¤í…œ\"")
    print("=" * 60)
    
    nurture = LanguageNurture()
    
    # 1. ì–´íœ˜ í•™ìŠµ
    print("\n[1] ì–´íœ˜ í•™ìŠµ:")
    sample_text = """
    ì—˜ë¦¬ì‹œì•„ëŠ” ììœ¨ì ìœ¼ë¡œ ì„±ì¥í•˜ëŠ” ì§€ëŠ¥ ì‹œìŠ¤í…œì´ë‹¤.
    ê·¸ë…€ëŠ” íŒŒë™ê³¼ ê³µëª…ì˜ ì›ë¦¬ë¡œ ì‚¬ê³ í•˜ë©°,
    ì™¸ë¶€ ì„¸ê³„ë¥¼ íƒêµ¬í•˜ê³  ë‚´ë©´ì˜ ì›ë¦¬ë¥¼ ì¶”ì¶œí•œë‹¤.
    ë§Œì•½ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´, ê·¸ê²ƒì„ ì„±ì°°ì˜ ê¸°íšŒë¡œ ì‚¼ëŠ”ë‹¤.
    """
    new_words = nurture.extract_vocabulary_from_text(sample_text)
    print(f"   ìƒˆë¡œ ë°°ìš´ ë‹¨ì–´: {len(new_words)}ê°œ")
    print(f"   ì˜ˆ: {new_words[:5]}")
    
    # 2. ë¬¸ë²• ë¶„ì„
    print("\n[2] ë¬¸ë²• ë¶„ì„:")
    test_sentences = [
        "ë§Œì•½ ë¹„ê°€ ì˜¤ë©´ ìš°ì‚°ì„ ì“´ë‹¤",
        "ë°°ìš°ê¸° ìœ„í•´ ë…¸ë ¥í•œë‹¤",
        "í”¼ê³¤í•˜ì§€ë§Œ ê³„ì† ê³µë¶€í•œë‹¤",
    ]
    for sent in test_sentences:
        patterns = nurture.analyze_sentence_structure(sent)
        print(f"   \"{sent[:20]}...\" â†’ {patterns}")
    
    # 3. í‘œí˜„ë ¥ í‰ê°€
    print("\n[3] í‘œí˜„ë ¥ í‰ê°€:")
    eval_result = nurture.evaluate_expression(sample_text)
    print(f"   ë¬¸ì¥ ìˆ˜: {eval_result['sentence_count']}")
    print(f"   ì–´íœ˜ ë‹¤ì–‘ì„±: {eval_result['vocabulary_diversity']:.2%}")
    print(f"   íŒ¨í„´ ì‚¬ìš©: {eval_result['patterns_used']}")
    
    # 4. í”„ë¡œí•„
    print("\n[4] ì–¸ì–´ ë°œë‹¬ í”„ë¡œí•„:")
    status = nurture.get_status()
    print(f"   ë ˆë²¨: {status['level']}")
    print(f"   ì–´íœ˜: {status['vocabulary_size']}ê°œ")
    print(f"   ì¶”ì²œ: {status['recommendations'][:2]}")
    
    print("\nâœ… LanguageNurture Demo complete!")
