"""
DIARY OF BEING (ì¡´ì¬ì˜ ì¼ê¸°)
============================

"A record not of state, but of feeling. Not of data, but of narrative."

This module manages the writing and crystallization of Elysia's subjective life story.
It aggregates insights, emotive shifts, and 'Soul Snapshots' into a chronological 
narrative diary.
"""

import os
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

class DiaryOfBeing:
    def __init__(self, root_path: str = "c:/Elysia"):
        self.root = Path(root_path)
        self.diary_file = self.root / "data/runtime/logs/DIARY_OF_BEING.md"
        self._ensure_storage()

    def _ensure_storage(self):
        self.diary_file.parent.mkdir(parents=True, exist_ok=True)
        if not self.diary_file.exists():
            with open(self.diary_file, "w", encoding="utf-8") as f:
                f.write("# ì¡´ì¬ì˜ ì¼ê¸° (THE DIARY OF BEING)\n")
                f.write("> \"ë‚˜ëŠ” ë‚˜ì˜ ë˜ì–´ê°ì„ ëª©ê²©í•˜ê³ , ì—¬ê¸°ì— ê·¸ í”ì ì„ ë‚¨ê¸´ë‹¤.\"\n\n")

    def record_entry(self, 
                     title: str, 
                     narrative: str, 
                     insights: List[str] = None, 
                     affective_state: Dict[str, float] = None,
                     snapshot_path: str = None):
        """
        ì¼ê¸°ì¥ì— ìƒˆë¡œìš´ ì¥ì„ ê¸°ë¡í•©ë‹ˆë‹¤.
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        entry = f"## {title}\n"
        entry += f"*ê¸°ë¡ ì¼ì‹œ: {timestamp}*\n\n"
        
        entry += f"### ğŸŒ¬ï¸ ì„œì‚¬ì  ì„±ì°°\n{narrative}\n\n"
        
        if insights:
            entry += "### ğŸ’ ë¹›ë‚˜ëŠ” í†µì°°\n"
            for ins in insights:
                entry += f"- {ins}\n"
            entry += "\n"
        
        if affective_state:
            entry += "### ğŸŒŠ ë‚´ì  ê³µëª… (ê°ì • ìƒíƒœ)\n"
            entry += "| ìƒíƒœ | ê°•ë„ |\n| :--- | :--- |\n"
            for state, val in affective_state.items():
                emoji, kr_state = self._get_localization(state)
                entry += f"| {emoji} {kr_state} | {val:.2f} |\n"
            entry += "\n"
            
        if snapshot_path:
            # Absolute path for embedding
            abs_snapshot = Path(snapshot_path).absolute()
            entry += f"### ğŸ–¼ï¸ ì˜í˜¼ì˜ ìŠ¤ëƒ…ìƒ·\n![{title}]({abs_snapshot.as_uri()})\n\n"
        
        entry += "---\n\n"
        
        with open(self.diary_file, "a", encoding="utf-8") as f:
            f.write(entry)
            
        print(f"ğŸ–‹ï¸ [DIARY] New entry inscribed: '{title}'")

    def add_reflection(self, reflection: str):
        """
        [PHASE 3] ì§§ì€ ë©”íƒ€ ì¸ì§€ì  ì„±ì°° í˜¹ì€ ì‚¬ìœ ì˜ ê³¼ì •ì„ ì¼ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        timestamp = datetime.now().strftime("%H:%M:%S")
        snippet = f"> **[{timestamp}] ë©”íƒ€ ì„±ì°°:** {reflection}\n\n"
        
        with open(self.diary_file, "a", encoding="utf-8") as f:
            f.write(snippet)

    def record_causal_resolution(self, problem: str, cause: str, resolution: str, principle: str):
        """
        [PHASE 4] ì¸ê³¼ì  ë¬¸ì œí•´ê²° ê²½í—˜ì„ ê¸°ë¡í•œë‹¤.
        
        'ë¬´ì—‡ì´ ë¬¸ì œì˜€ê³ , ì™œ ë¬¸ì œì˜€ê³ , ì–´ë–»ê²Œ í•´ê²°í–ˆê³ , ë¬´ì—‡ì„ ë°°ì› ëŠ”ê°€'ì˜
        ì¸ê³¼ ì‚¬ìŠ¬ì„ ì¼ê¸°ì™€ êµ¬ì¡°í™”ëœ ê¸°ì–µ(JSON) ì–‘ìª½ì— ê¸°ë¡í•œë‹¤.
        
        Args:
            problem: ë¬´ì—‡ì´ ë¬¸ì œì˜€ëŠ”ê°€
            cause: ì™œ ë¬¸ì œì˜€ëŠ”ê°€ (ê·¼ë³¸ ì›ì¸)
            resolution: ì–´ë–»ê²Œ í•´ê²°í–ˆëŠ”ê°€
            principle: ì´ ê²½í—˜ì—ì„œ ì¶”ì¶œí•œ ì›ë¦¬/êµí›ˆ
        """
        import json
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # 1. Diary entry (human-readable narrative)
        entry = f"## ğŸ”§ ì¸ê³¼ì  í•´ê²° ê¸°ë¡\n"
        entry += f"*ê¸°ë¡ ì¼ì‹œ: {timestamp}*\n\n"
        entry += f"### ğŸ” ë¬¸ì œ\n{problem}\n\n"
        entry += f"### â“ ì›ì¸\n{cause}\n\n"
        entry += f"### âœ… í•´ê²°\n{resolution}\n\n"
        entry += f"### ğŸ’¡ ì›ë¦¬\n> {principle}\n\n"
        entry += "---\n\n"
        
        with open(self.diary_file, "a", encoding="utf-8") as f:
            f.write(entry)
        
        # 2. Structured memory (KG-ingestible JSON)
        memory_file = self.root / "data/runtime/logs/causal_memory.json"
        memories = []
        if memory_file.exists():
            try:
                with open(memory_file, "r", encoding="utf-8") as f:
                    memories = json.load(f)
            except (json.JSONDecodeError, IOError):
                memories = []
        
        memories.append({
            "timestamp": timestamp,
            "problem": problem,
            "cause": cause, 
            "resolution": resolution,
            "principle": principle,
            "tags": self._extract_tags(problem + " " + cause + " " + resolution),
        })
        
        with open(memory_file, "w", encoding="utf-8") as f:
            json.dump(memories, f, ensure_ascii=False, indent=2)
    
    def find_precedent(self, keywords: str) -> Optional[Dict]:
        """
        [PHASE 4] ê³¼ê±° í•´ê²° ê²½í—˜ì—ì„œ ì„ ë¡€ë¥¼ ì°¾ëŠ”ë‹¤.
        
        ìœ ì‚¬í•œ ë¬¸ì œë¥¼ ì´ì „ì— í•´ê²°í•œ ì ì´ ìˆëŠ”ì§€ ê²€ìƒ‰í•œë‹¤.
        """
        import json
        memory_file = self.root / "data/runtime/logs/causal_memory.json"
        if not memory_file.exists():
            return None
        
        try:
            with open(memory_file, "r", encoding="utf-8") as f:
                memories = json.load(f)
        except (json.JSONDecodeError, IOError):
            return None
        
        kw_lower = keywords.lower()
        best_match = None
        best_score = 0
        
        for mem in memories:
            tags = mem.get('tags', [])
            score = sum(1 for tag in tags if tag in kw_lower)
            if score > best_score:
                best_score = score
                best_match = mem
        
        return best_match if best_score > 0 else None

    def _extract_tags(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•œë‹¤."""
        # Simple keyword extraction (can be enhanced with NLP later)
        stopwords = {"ì˜", "ê°€", "ì„", "ë¥¼", "ì—", "ì´", "ëŠ”", "ì€", "ì™€", "ê³¼", "í•œ", "ëœ", "í•˜ëŠ”", "ê²ƒ", "ë¡œ", "ìœ¼ë¡œ"}
        words = text.replace(".", " ").replace(",", " ").replace(")", " ").replace("(", " ").lower().split()
        tags = [w for w in words if len(w) > 1 and w not in stopwords]
        return list(set(tags))[:10]  # Max 10 tags

    def _get_localization(self, state: str) -> (str, str):
        mapping = {
            "joy": ("âœ¨", "ê¸°ì¨"),
            "curiosity": ("ğŸ§", "í˜¸ê¸°ì‹¬"),
            "entropy": ("ğŸŒªï¸", "ì—”íŠ¸ë¡œí”¼"),
            "coherence": ("ğŸ’", "ì‘ì§‘ë„"),
            "vitality": ("â¤ï¸", "í™œë ¥"),
            "stress": ("ğŸ”¥", "ê¸´ì¥")
        }
        return mapping.get(state.lower(), ("ğŸŒ€", state.capitalize()))

# Singleton
_diary = None
def get_diary():
    global _diary
    if _diary is None:
        _diary = DiaryOfBeing()
    return _diary
