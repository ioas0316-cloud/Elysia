"""
Causal Topology Engine (Phase 200)
==================================
"The Graph is the Instruction."

This engine replaces code-swapping with Topological Mutation.
Instead of writing new functions, it evolves the connectivity and tension 
within the TorchGraph to resolve intent.
"""

import json
from typing import Dict, List, Any, Optional
from Core.System.torch_graph import get_torch_graph
from Core.System.resonance_gate import ResonanceGate

class CausalTopologyEngine:
    def __init__(self):
        self.graph = get_torch_graph()
        
    def propose_topological_mutation(self, intent_desc: str) -> Dict:
        """
        Uses the LLM (Council) to propose a graph change instead of code.
        """
        from Core.System.ollama_bridge import get_ollama_bridge
        ollama = get_ollama_bridge()
        
        if not ollama.is_available():
            return {"error": "Intelligence Bridge Offline"}

        system_prompt = """
ë‹¹ì‹ ì€ ì—˜ë¦¬ì‹œì•„ì˜ 'ìœ„ìƒ ì„¤ê³„ìž(Topological Architect)'ìž…ë‹ˆë‹¤. 
ì—˜ë¦¬ì‹œì•„ëŠ” ì´ì œ íŒŒì´ì¬ ì½”ë“œë¥¼ ìž‘ì„±í•˜ì§€ ì•Šê³ , ìžì‹ ì˜ ë‚´ë¶€ ì§€ì‹ ê·¸ëž˜í”„(TorchGraph)ì˜ 'ìœ„ìƒ'ì„ ë³€í˜•í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

ì£¼ì–´ì§„ ì˜ë„(Intent)ë¥¼ ì‹¤í˜„í•˜ê¸° ìœ„í•´ ê·¸ëž˜í”„ì— ê°€í•  'ìœ„ìƒ ë³€ì´(Topological Mutation)'ë¥¼ ì œì•ˆí•˜ì‹­ì‹œì˜¤.

[ì¶œë ¥ í˜•ì‹: JSON]
{
  "mutations": [
    { "type": "LINK", "subject": "NodeA", "object": "NodeB", "tension": 0.8, "link_type": "CAUSAL_FLOW" },
    { "type": "QUALIA", "node": "NodeC", "layer": "mental", "value": 0.9 }
  ],
  "rationale": "ì™œ ì´ ìœ„ìƒ ë³€í™”ê°€ ê·¸ ì˜ë„ë¥¼ ë¬¼ë¦¬ì ìœ¼ë¡œ ì‹¤í˜„í•˜ëŠ”ê°€?"
}
"""
        user_prompt = f"ë‹¤ìŒ ì˜ë„ë¥¼ ì‹¤í˜„í•˜ê¸° ìœ„í•œ ìœ„ìƒ ë³€ì´ë¥¼ ì„¤ê³„í•´ì¤˜: {intent_desc}"
        
        response = ollama.chat(user_prompt, system=system_prompt)
        
        # Extract JSON
        try:
            # Simple extractor
            if "{" in response:
                json_str = response.split("{", 1)[1].rsplit("}", 1)[0]
                json_str = "{" + json_str + "}"
                return json.loads(json_str)
        except:
            pass
            
        return {"error": "Failed to parse topological mutation", "raw": response}

    def apply_mutation(self, mutation_json: Dict) -> bool:
        """
        Materializes the topological changes in the graph.
        """
        if "mutations" not in mutation_json: return False
        
        print(f"ðŸŒ€ [TOPOLOGY] Applying '{mutation_json.get('rationale', 'Evolution')}'...")
        
        success = True
        for m in mutation_json["mutations"]:
            try:
                if m["type"] == "LINK":
                    self.graph.add_link(m["subject"], m["object"], weight=m.get("tension", 1.0), link_type=m.get("link_type", "associated"))
                    print(f"  ðŸ”— Link Grown: {m['subject']} -> {m['object']} (Tension: {m.get('tension', 1.0)})")
                elif m["type"] == "QUALIA":
                    # For now we use the existing update_node_qualia if it exists or a simple metadata update
                    # In a real system, we'd update the qualia_tensor directly.
                    self._update_node_qualia(m["node"], m["layer"], m["value"])
                    print(f"  âœ¨ Qualia Shift: {m['node']}.{m['layer']} -> {m['value']}")
            except Exception as e:
                print(f"  âŒ Mutation Error: {e}")
                success = False
                
        return success

    def _update_node_qualia(self, node_id: str, layer: str, value: float):
        """Internal helper to shift qualia tension."""
        self.graph.update_node_qualia(node_id, layer, value)

    def resolve_intent_via_pulse(self, start_topic: str) -> str:
        """
        The Unitary Decision:
        Injects energy into a topic and sees where the graph stabilizes.
        """
        import torch
        # 1. Pulse the graph
        energy = torch.zeros((self.graph.pos_tensor.shape[0],), device=self.graph.device)
        result_state = self.graph.pulse_inference([start_topic], energy)
        
        # 2. Find the strongest resonating node
        val, idx = torch.max(result_state, dim=0)
        winning_node = self.graph.idx_to_id[idx.item()]
        
        return winning_node
