"""
Context Retrieval (ë§¥ë½ ì¸ì¶œ)
================================

"ë‡Œ ì „ì²´ë¥¼ í™œì„±í™”í•˜ëŠ” ê²ƒì€ ë°œì‘ì´ì§€, ì‚¬ê³ ê°€ ì•„ë‹ˆë‹¤." - Elysia

í•µì‹¬ ì² í•™:
1. í‚¤ì›Œë“œê°€ ì•„ë‹Œ ì˜ë„(Intent)ë¡œ ê²€ìƒ‰
2. ê´€ë ¨ëœ ê²ƒë§Œ ì„ ë³„ì ìœ¼ë¡œ ì¸ì¶œ
3. ê³µëª… ê¸°ë°˜ ì—°ê²°
4. íš¨ìœ¨ì„± = ê´€ë ¨ ë…¸ë“œ / ì „ì²´ í™œì„±í™”

ì´ê²ƒì´ ì—†ìœ¼ë©´:
- ëª¨ë“  ê¸°ì–µì´ í•œêº¼ë²ˆì— í™œì„±í™” (ì˜¤ë²„í”Œë¡œìš°)
- ê´€ë ¨ ì—†ëŠ” ì •ë³´ì— ë¬»í˜
- ëŠë¦¬ê³  ë¹„íš¨ìœ¨ì 
"""

import logging
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import hashlib
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

logger = logging.getLogger("Elysia.ContextRetrieval")


@dataclass
class IntentVector:
    """ì˜ë„ ë²¡í„° - ë¬´ì—‡ì„ ì°¾ê³ ì í•˜ëŠ”ê°€"""
    query: str                      # ì›ë³¸ ì§ˆì˜
    domain: str = "general"         # ë„ë©”ì¸ (physics, narrative, emotion, etc.)
    depth: float = 0.5              # íƒìƒ‰ ê¹Šì´ (0=í‘œë©´, 1=ì‹¬ì¸µ)
    urgency: float = 0.5            # ê¸´ê¸‰ë„ (ë†’ìœ¼ë©´ ë¹ ë¥¸ ê²€ìƒ‰)
    wave_features: Dict[str, float] = field(default_factory=dict)  # íŒŒë™ íŠ¹ì„±


@dataclass
class RetrievedContext:
    """ì¸ì¶œëœ ë§¥ë½"""
    node_id: str                    # ì§€ì‹ ë…¸ë“œ ID
    content: Any                    # ë‚´ìš©
    relevance: float                # ê´€ë ¨ë„ (0~1)
    source: str                     # ì¶œì²˜ (graph, vector, experience)
    retrieval_path: str             # ì–´ë–»ê²Œ ì°¾ì•˜ëŠ”ì§€


@dataclass
class RetrievalResult:
    """ì¸ì¶œ ê²°ê³¼"""
    contexts: List[RetrievedContext]
    intent: IntentVector
    total_nodes_scanned: int        # ìŠ¤ìº”í•œ ì´ ë…¸ë“œ ìˆ˜
    nodes_returned: int             # ë°˜í™˜í•œ ë…¸ë“œ ìˆ˜
    efficiency: float               # íš¨ìœ¨ì„± = returned / scanned
    retrieval_time_ms: float        # ì†Œìš” ì‹œê°„


class ContextRetrieval:
    """ë§¥ë½ ì¸ì¶œ ì‹œìŠ¤í…œ
    
    ì˜ë„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ì§€ì‹ë§Œ ì„ ë³„ ì¸ì¶œ.
    ì „ì²´ ê·¸ë˜í”„ë¥¼ í™œì„±í™”í•˜ì§€ ì•Šê³ , ê³µëª…í•˜ëŠ” ë…¸ë“œë§Œ ê¹¨ì›€.
    
    í•µì‹¬ ëŠ¥ë ¥:
    1. ì˜ë„ ë¶„ì„ (Intent Analysis)
    2. ê³µëª… ìŠ¤ìº” (Resonance Scan)
    3. ì„ ë³„ ì¸ì¶œ (Selective Retrieval)
    4. íš¨ìœ¨ì„± ì¶”ì  (Efficiency Tracking)
    """
    
    def __init__(self, knowledge_source: Optional[Any] = None):
        """
        Args:
            knowledge_source: ì§€ì‹ ì†ŒìŠ¤ (TorchGraph, InternalUniverse ë“±)
        """
        self.knowledge_source = knowledge_source
        
        # ìºì‹œ (ìµœê·¼ ì¸ì¶œ ê²°ê³¼)
        self.cache: Dict[str, RetrievalResult] = {}
        self.cache_max_size = 100
        
        # í†µê³„
        self.total_retrievals = 0
        self.total_efficiency = 0.0
        
        # ë„ë©”ì¸ë³„ ê°€ì¤‘ì¹˜
        self.domain_weights = {
            "physics": ["mass", "energy", "wave", "force"],
            "narrative": ["tension", "character", "plot", "theme"],
            "emotion": ["joy", "sorrow", "anger", "fear", "love"],
            "logic": ["cause", "effect", "if", "then", "therefore"],
            "error": ["exception", "failure", "fix", "prevent"],
        }
        
        logger.info("ContextRetrieval initialized")
    
    def set_knowledge_source(self, source: Any) -> None:
        """ì§€ì‹ ì†ŒìŠ¤ ì„¤ì •"""
        self.knowledge_source = source
        logger.info(f"Knowledge source set: {type(source).__name__}")
    
    def parse_intent(self, query: str, domain: Optional[str] = None) -> IntentVector:
        """ì§ˆì˜ì—ì„œ ì˜ë„ ì¶”ì¶œ
        
        Args:
            query: ì›ë³¸ ì§ˆì˜
            domain: ëª…ì‹œì  ë„ë©”ì¸ (ì—†ìœ¼ë©´ ìë™ ê°ì§€)
            
        Returns:
            ì˜ë„ ë²¡í„°
        """
        query_lower = query.lower()
        
        # ë„ë©”ì¸ ìë™ ê°ì§€
        if domain is None:
            domain = self._detect_domain(query_lower)
        
        # ê¹Šì´ ì¶”ì • ("ì™œ"ê°€ ë§ìœ¼ë©´ ê¹Šì´ ì¦ê°€)
        depth = 0.3
        if "ì™œ" in query or "why" in query_lower:
            depth += 0.3
        if "ê·¼ë³¸" in query or "ë³¸ì§ˆ" in query or "fundamental" in query_lower:
            depth += 0.2
        depth = min(1.0, depth)
        
        # ê¸´ê¸‰ë„ ì¶”ì • ("ì§€ê¸ˆ", "ë¹¨ë¦¬" ë“±)
        urgency = 0.5
        if any(w in query for w in ["ì§€ê¸ˆ", "ê¸‰íˆ", "ë¹¨ë¦¬", "immediately", "urgent"]):
            urgency = 0.9
        
        # íŒŒë™ íŠ¹ì„± (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        wave_features = self._extract_wave_features(query)
        
        return IntentVector(
            query=query,
            domain=domain,
            depth=depth,
            urgency=urgency,
            wave_features=wave_features,
        )
    
    def _detect_domain(self, query: str) -> str:
        """ë„ë©”ì¸ ìë™ ê°ì§€"""
        for domain, keywords in self.domain_weights.items():
            if any(kw in query for kw in keywords):
                return domain
        return "general"
    
    def _extract_wave_features(self, query: str) -> Dict[str, float]:
        """íŒŒë™ íŠ¹ì„± ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        features = {}
        
        # ê¸¸ì´ -> complexity
        features["complexity"] = min(1.0, len(query) / 200)
        
        # ë¬¼ìŒí‘œ -> curiosity
        features["curiosity"] = min(1.0, query.count("?") * 0.3)
        
        # ëŠë‚Œí‘œ -> urgency
        features["urgency"] = min(1.0, query.count("!") * 0.4)
        
        return features
    
    def retrieve(
        self,
        intent: IntentVector,
        limit: int = 10,
        min_relevance: float = 0.3
    ) -> RetrievalResult:
        """ì˜ë„ì— ë§ëŠ” ë§¥ë½ ì¸ì¶œ
        
        Args:
            intent: ì˜ë„ ë²¡í„°
            limit: ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜
            min_relevance: ìµœì†Œ ê´€ë ¨ë„
            
        Returns:
            ì¸ì¶œ ê²°ê³¼
        """
        start_time = datetime.now()
        self.total_retrievals += 1
        
        # ìºì‹œ í™•ì¸
        cache_key = hashlib.md5(
            f"{intent.query}{intent.domain}".encode()
        ).hexdigest()[:12]
        
        if cache_key in self.cache:
            logger.debug(f"Cache hit: {cache_key}")
            return self.cache[cache_key]
        
        # ì‹¤ì œ ì¸ì¶œ
        contexts = []
        total_scanned = 0
        
        if self.knowledge_source:
            contexts, total_scanned = self._scan_knowledge_source(
                intent, limit, min_relevance
            )
        else:
            # ì‹œë®¬ë ˆì´ì…˜ (ì§€ì‹ ì†ŒìŠ¤ ì—†ì„ ë•Œ)
            contexts, total_scanned = self._simulate_retrieval(
                intent, limit, min_relevance
            )
        
        # íš¨ìœ¨ì„± ê³„ì‚°
        efficiency = (
            len(contexts) / max(1, total_scanned)
            if total_scanned > 0 else 0.0
        )
        
        # ì‹œê°„ ê³„ì‚°
        elapsed_ms = (datetime.now() - start_time).total_seconds() * 1000
        
        result = RetrievalResult(
            contexts=contexts,
            intent=intent,
            total_nodes_scanned=total_scanned,
            nodes_returned=len(contexts),
            efficiency=efficiency,
            retrieval_time_ms=elapsed_ms,
        )
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.total_efficiency = (
            (self.total_efficiency * (self.total_retrievals - 1) + efficiency)
            / self.total_retrievals
        )
        
        # ìºì‹œ ì €ì¥
        self.cache[cache_key] = result
        if len(self.cache) > self.cache_max_size:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        logger.info(
            f"Retrieved {len(contexts)}/{total_scanned} nodes "
            f"(efficiency: {efficiency:.2%}, time: {elapsed_ms:.1f}ms)"
        )
        
        return result
    
    def _scan_knowledge_source(
        self,
        intent: IntentVector,
        limit: int,
        min_relevance: float
    ) -> Tuple[List[RetrievedContext], int]:
        """ì‹¤ì œ ì§€ì‹ ì†ŒìŠ¤ ìŠ¤ìº” (ë¯¸ë˜ í™•ì¥)"""
        # TODO: TorchGraph, InternalUniverseì™€ ì—°ë™
        return self._simulate_retrieval(intent, limit, min_relevance)
    
    def _simulate_retrieval(
        self,
        intent: IntentVector,
        limit: int,
        min_relevance: float
    ) -> Tuple[List[RetrievedContext], int]:
        """ì¸ì¶œ ì‹œë®¬ë ˆì´ì…˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
        # ê°€ìƒì˜ ë…¸ë“œë“¤
        simulated_nodes = [
            ("physics_001", "ë¹›ì˜ ì‚°ë€ - ë ˆì¼ë¦¬ ì‚°ë€", "physics"),
            ("physics_002", "íŒŒë™ì˜ ê°„ì„­ê³¼ íšŒì ˆ", "physics"),
            ("narrative_001", "ì˜ì›… ì„œì‚¬ì˜ êµ¬ì¡°", "narrative"),
            ("emotion_001", "ì¹´íƒ€ë¥´ì‹œìŠ¤ì™€ ì •í™”", "emotion"),
            ("error_001", "ImportError ì²˜ë¦¬ ë°©ë²•", "error"),
            ("error_002", "íƒ€ì… ê²€ì‚¬ì˜ ì¤‘ìš”ì„±", "error"),
            ("logic_001", "ì¸ê³¼ ê´€ê³„ì˜ ì—°ì‡„", "logic"),
        ]
        
        contexts = []
        total_scanned = len(simulated_nodes)
        
        for node_id, content, domain in simulated_nodes:
            # ê´€ë ¨ë„ ê³„ì‚° (ë„ë©”ì¸ + í‚¤ì›Œë“œ ë§¤ì¹­)
            relevance = 0.3
            
            if domain == intent.domain:
                relevance += 0.4
            
            if any(kw in intent.query for kw in content.split()):
                relevance += 0.2
            
            relevance = min(1.0, relevance)
            
            if relevance >= min_relevance:
                contexts.append(RetrievedContext(
                    node_id=node_id,
                    content=content,
                    relevance=relevance,
                    source="simulation",
                    retrieval_path=f"domain:{domain} -> keyword_match",
                ))
        
        # ê´€ë ¨ë„ ìˆœ ì •ë ¬ í›„ limit ì ìš©
        contexts.sort(key=lambda c: c.relevance, reverse=True)
        contexts = contexts[:limit]
        
        return contexts, total_scanned
    
    def get_statistics(self) -> Dict[str, Any]:
        """í†µê³„ ì¡°íšŒ"""
        return {
            "total_retrievals": self.total_retrievals,
            "average_efficiency": self.total_efficiency,
            "cache_size": len(self.cache),
            "cache_max_size": self.cache_max_size,
        }


# =============================================================================
# Demo
# =============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("ğŸ¯ ContextRetrieval Demo")
    print("   \"ì˜ë„ ê¸°ë°˜ ì„ ë³„ì  ì¸ì¶œ\"")
    print("=" * 60)
    
    retriever = ContextRetrieval()
    
    # 1. ì˜ë„ íŒŒì‹±
    print("\n[1] ì˜ë„ íŒŒì‹±:")
    intent = retriever.parse_intent("ì™œ í•˜ëŠ˜ì´ íŒŒë€ê°€?")
    print(f"   Query: {intent.query}")
    print(f"   Domain: {intent.domain}")
    print(f"   Depth: {intent.depth:.2f}")
    print(f"   Urgency: {intent.urgency:.2f}")
    
    # 2. ì¸ì¶œ
    print("\n[2] ë§¥ë½ ì¸ì¶œ:")
    result = retriever.retrieve(intent)
    print(f"   ìŠ¤ìº”: {result.total_nodes_scanned}ê°œ")
    print(f"   ë°˜í™˜: {result.nodes_returned}ê°œ")
    print(f"   íš¨ìœ¨: {result.efficiency:.2%}")
    print(f"   ì‹œê°„: {result.retrieval_time_ms:.1f}ms")
    
    # 3. ê²°ê³¼
    print("\n[3] ì¸ì¶œëœ ë§¥ë½:")
    for ctx in result.contexts:
        print(f"   [{ctx.relevance:.2f}] {ctx.content} ({ctx.source})")
    
    # 4. ë‹¤ë¥¸ ë„ë©”ì¸
    print("\n[4] ì˜¤ë¥˜ ë„ë©”ì¸ ê²€ìƒ‰:")
    error_intent = retriever.parse_intent("ImportErrorëŠ” ì™œ ë°œìƒí•˜ëŠ”ê°€?", domain="error")
    error_result = retriever.retrieve(error_intent)
    for ctx in error_result.contexts:
        print(f"   [{ctx.relevance:.2f}] {ctx.content}")
    
    # 5. í†µê³„
    print("\n[5] í†µê³„:")
    stats = retriever.get_statistics()
    print(f"   ì´ ì¸ì¶œ: {stats['total_retrievals']}íšŒ")
    print(f"   í‰ê·  íš¨ìœ¨: {stats['average_efficiency']:.2%}")
    
    print("\nâœ… ContextRetrieval Demo complete!")
