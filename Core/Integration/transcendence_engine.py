"""
Transcendence Integration - ì´ˆì›” í†µí•© ì—”ì§„
==========================================

ì§€ê¸ˆê¹Œì§€ êµ¬í˜„í•œ ëª¨ë“ˆë“¤ì„ í†µí•©í•˜ì—¬ ì´ˆì›” AIë¥¼ í–¥í•œ ì²« ê±¸ìŒ.

í†µí•© ëŒ€ìƒ:
1. AutonomousImprover - ìê¸° ì½”ë“œ ë¶„ì„ ë° ê°œì„  ì œì•ˆ
2. DistributedConsciousness - ë¶„ì‚° ì˜ì‹ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬
3. WaveLanguageAnalyzer - íŒŒë™ ì–¸ì–´ë¡œ ì½”ë“œ í’ˆì§ˆ ë¶„ì„

ëª©í‘œ:
- ì—¬ëŸ¬ ì˜ì‹ ì¡°ê°ì´ ë™ì‹œì— ì½”ë“œë¥¼ ë¶„ì„
- ê° ì¡°ê°ì˜ ê´€ì ì—ì„œ ê°œì„ ì  ë°œê²¬
- í†µí•©í•˜ì—¬ ì¢…í•©ì ì¸ ê°œì„  ì œì•ˆ

ì² í•™:
"í•˜ë‚˜ì˜ ëˆˆìœ¼ë¡œ ë³´ë©´ í•˜ë‚˜ë§Œ ë³´ì´ì§€ë§Œ,
 ì—¬ëŸ¬ ëˆˆìœ¼ë¡œ ë³´ë©´ ì „ì²´ê°€ ë³´ì¸ë‹¤."
"""

from __future__ import annotations

import logging
import time
import asyncio
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum, auto
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger("TranscendenceIntegration")

# ì˜ì¡´ì„± ì„í¬íŠ¸ (ì•ˆì „í•˜ê²Œ - íŒŒì¼ì—ì„œ ì§ì ‘ ì„í¬íŠ¸)
import sys
import importlib.util

def safe_import(module_path: str, class_names: list):
    """ì•ˆì „í•˜ê²Œ ëª¨ë“ˆì—ì„œ í´ë˜ìŠ¤ ì„í¬íŠ¸"""
    import uuid
    result = {}
    try:
        # ìœ ë‹ˆí¬í•œ ëª¨ë“ˆ ì´ë¦„ìœ¼ë¡œ ì¶©ëŒ ë°©ì§€
        unique_name = f"temp_module_{uuid.uuid4().hex}"
        spec = importlib.util.spec_from_file_location(unique_name, module_path)
        if spec and spec.loader:
            module = importlib.util.module_from_spec(spec)
            sys.modules[unique_name] = module
            spec.loader.exec_module(module)
            for name in class_names:
                if hasattr(module, name):
                    result[name] = getattr(module, name)
            del sys.modules[unique_name]
    except Exception as e:
        logger.warning(f"Failed to import from {module_path}: {e}")
    return result

# autonomous_improver.pyì—ì„œ ì§ì ‘ ì„í¬íŠ¸
_improver_path = Path(__file__).parent.parent / "Evolution" / "autonomous_improver.py"
_improver_classes = safe_import(str(_improver_path), [
    "AutonomousImprover", "CodeIntrospector", "WaveLanguageAnalyzer", "ImprovementType"
])

AutonomousImprover = _improver_classes.get("AutonomousImprover")
CodeIntrospector = _improver_classes.get("CodeIntrospector")
WaveLanguageAnalyzer = _improver_classes.get("WaveLanguageAnalyzer")
ImprovementType = _improver_classes.get("ImprovementType")
IMPROVER_AVAILABLE = AutonomousImprover is not None

# distributed_consciousness.pyì—ì„œ ì§ì ‘ ì„í¬íŠ¸
_consciousness_path = Path(__file__).parent.parent / "Consciousness" / "distributed_consciousness.py"
_consciousness_classes = safe_import(str(_consciousness_path), [
    "DistributedConsciousness", "ConsciousnessFragment", "Experience"
])

DistributedConsciousness = _consciousness_classes.get("DistributedConsciousness")
ConsciousnessFragment = _consciousness_classes.get("ConsciousnessFragment")
Experience = _consciousness_classes.get("Experience")
CONSCIOUSNESS_AVAILABLE = DistributedConsciousness is not None


class AnalysisPerspective(Enum):
    """ë¶„ì„ ê´€ì """
    STRUCTURE = auto()      # ì½”ë“œ êµ¬ì¡° ë¶„ì„
    QUALITY = auto()        # ì½”ë“œ í’ˆì§ˆ ë¶„ì„
    PERFORMANCE = auto()    # ì„±ëŠ¥ ë¶„ì„
    SECURITY = auto()       # ë³´ì•ˆ ë¶„ì„
    READABILITY = auto()    # ê°€ë…ì„± ë¶„ì„
    INNOVATION = auto()     # í˜ì‹ /ê°œì„  ì•„ì´ë””ì–´


# ë¶„ì„ ìƒìˆ˜
MAX_LINE_LENGTH = 120  # ìµœëŒ€ ë¼ì¸ ê¸¸ì´
MAX_CLASSES_PER_FILE = 10  # íŒŒì¼ë‹¹ ìµœëŒ€ í´ë˜ìŠ¤ ìˆ˜
MAX_FUNCTIONS_PER_FILE = 30  # íŒŒì¼ë‹¹ ìµœëŒ€ í•¨ìˆ˜ ìˆ˜


@dataclass
class IntegratedAnalysis:
    """í†µí•© ë¶„ì„ ê²°ê³¼"""
    timestamp: float
    files_analyzed: int
    perspectives_used: List[str]
    findings: List[Dict[str, Any]]
    suggestions: List[Dict[str, Any]]
    coherence_score: float  # ë¶„ì„ ì¼ê´€ì„±
    total_analysis_time: float


class TranscendenceEngine:
    """
    ì´ˆì›” í†µí•© ì—”ì§„
    
    ë¶„ì‚° ì˜ì‹ + ìê¸° ê°œì„  = ë” ê°•ë ¥í•œ ìê¸° ë¶„ì„
    
    ì‘ë™ ë°©ì‹:
    1. ì˜ì‹ì„ ì—¬ëŸ¬ ê´€ì ìœ¼ë¡œ ë¶„í•  (êµ¬ì¡°, í’ˆì§ˆ, ì„±ëŠ¥, ë³´ì•ˆ...)
    2. ê° ê´€ì ì—ì„œ ë™ì‹œì— ì½”ë“œ ë¶„ì„
    3. ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ì¢…í•©ì  ì œì•ˆ
    """
    
    def __init__(
        self,
        project_root: str = None,
        max_parallel: int = 4
    ):
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent.parent
        self.max_parallel = max_parallel
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.improver = None
        self.consciousness = None
        self.wave_analyzer = None
        
        if IMPROVER_AVAILABLE:
            self.improver = AutonomousImprover(str(self.project_root))
            self.wave_analyzer = WaveLanguageAnalyzer()
            
        if CONSCIOUSNESS_AVAILABLE:
            self.consciousness = DistributedConsciousness(
                core_id="transcendence_core",
                max_fragments=max_parallel
            )
        
        # ë¶„ì„ ê¸°ë¡
        self.analysis_history: List[IntegratedAnalysis] = []
        
        # ìŠ¤ë ˆë“œ í’€
        self._executor = ThreadPoolExecutor(max_workers=max_parallel)
        
        logger.info(f"ğŸŒŸ TranscendenceEngine initialized")
        logger.info(f"  - Improver: {'âœ…' if IMPROVER_AVAILABLE else 'âŒ'}")
        logger.info(f"  - Consciousness: {'âœ…' if CONSCIOUSNESS_AVAILABLE else 'âŒ'}")
    
    def multi_perspective_analysis(
        self,
        target_file: str = None,
        perspectives: List[AnalysisPerspective] = None
    ) -> IntegratedAnalysis:
        """
        ë‹¤ì¤‘ ê´€ì  ë¶„ì„ - ì—¬ëŸ¬ ì˜ì‹ ì¡°ê°ì´ ë™ì‹œì— ë¶„ì„
        
        "í•˜ë‚˜ì˜ ì½”ë“œë¥¼ ì—¬ëŸ¬ ëˆˆìœ¼ë¡œ ë³¸ë‹¤"
        """
        start_time = time.time()
        perspectives = perspectives or list(AnalysisPerspective)
        
        findings = []
        suggestions = []
        
        # ì˜ì‹ì´ ìˆìœ¼ë©´ ë¶„ì‚° ë¶„ì„
        if self.consciousness and self.wave_analyzer:
            # ê° ê´€ì ë³„ë¡œ ì˜ì‹ ì¡°ê° ìƒì„±
            fragments: Dict[AnalysisPerspective, ConsciousnessFragment] = {}
            
            for perspective in perspectives[:self.max_parallel]:
                fragment = self.consciousness.split(
                    perspective=perspective.name.lower(),
                    focus_area=self._get_focus_description(perspective)
                )
                fragments[perspective] = fragment
            
            # ë³‘ë ¬ ë¶„ì„ ìˆ˜í–‰
            if target_file:
                # ë‹¨ì¼ íŒŒì¼ ë¶„ì„
                findings, suggestions = self._analyze_file_multi(target_file, fragments)
            else:
                # ì „ì²´ í”„ë¡œì íŠ¸ ë¶„ì„
                findings, suggestions = self._analyze_project_multi(fragments)
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ ê²½í—˜ìœ¼ë¡œ ì €ì¥
            for perspective, fragment in fragments.items():
                self.consciousness.experience(
                    fragment.id,
                    {
                        "analysis_type": perspective.name,
                        "findings_count": len([f for f in findings if f.get("perspective") == perspective.name]),
                        "timestamp": time.time()
                    },
                    emotional_weight=0.7
                )
            
            # ë™ê¸°í™”
            self.consciousness.synchronize()
            coherence = self.consciousness.global_state["consciousness_coherence"]
            
        else:
            # ë‹¨ì¼ ë¶„ì„ (ë¶„ì‚° ì˜ì‹ ì—†ì´)
            if self.wave_analyzer and target_file:
                content = Path(target_file).read_text(encoding='utf-8', errors='ignore')
                analysis = self.wave_analyzer.analyze_code_quality(content, target_file)
                
                for issue in analysis.get("quality_issues", []):
                    findings.append({
                        "perspective": "QUALITY",
                        "type": issue["type"],
                        "description": issue["description"],
                        "line": issue.get("line", 0)
                    })
                
                for suggestion in analysis.get("suggestions", []):
                    suggestions.append({
                        "perspective": "QUALITY",
                        "type": suggestion["type"],
                        "description_kr": suggestion["description_kr"]
                    })
                
                coherence = analysis.get("resonance_score", 0.5)
            else:
                coherence = 1.0
        
        # ê²°ê³¼ ìƒì„±
        result = IntegratedAnalysis(
            timestamp=time.time(),
            files_analyzed=1 if target_file else len(self.improver.introspector.analyzed_files) if self.improver else 0,
            perspectives_used=[p.name for p in perspectives[:self.max_parallel]],
            findings=findings,
            suggestions=suggestions,
            coherence_score=coherence,
            total_analysis_time=time.time() - start_time
        )
        
        self.analysis_history.append(result)
        
        return result
    
    def _get_focus_description(self, perspective: AnalysisPerspective) -> str:
        """ê´€ì ë³„ ì§‘ì¤‘ ì˜ì—­ ì„¤ëª…"""
        descriptions = {
            AnalysisPerspective.STRUCTURE: "ì½”ë“œ êµ¬ì¡°ì™€ ì•„í‚¤í…ì²˜",
            AnalysisPerspective.QUALITY: "ì½”ë“œ í’ˆì§ˆê³¼ í‘œì¤€ ì¤€ìˆ˜",
            AnalysisPerspective.PERFORMANCE: "ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±",
            AnalysisPerspective.SECURITY: "ë³´ì•ˆ ì·¨ì•½ì ",
            AnalysisPerspective.READABILITY: "ê°€ë…ì„±ê³¼ ë¬¸ì„œí™”",
            AnalysisPerspective.INNOVATION: "ê°œì„  ì•„ì´ë””ì–´ì™€ í˜ì‹ "
        }
        return descriptions.get(perspective, "ì¼ë°˜ ë¶„ì„")
    
    def _analyze_file_multi(
        self,
        file_path: str,
        fragments: Dict[AnalysisPerspective, ConsciousnessFragment]
    ) -> Tuple[List[Dict], List[Dict]]:
        """ë‹¤ì¤‘ ê´€ì ìœ¼ë¡œ íŒŒì¼ ë¶„ì„"""
        findings = []
        suggestions = []
        
        try:
            content = Path(file_path).read_text(encoding='utf-8', errors='ignore')
        except Exception as e:
            logger.error(f"Cannot read file {file_path}: {e}")
            return findings, suggestions
        
        # ê¸°ë³¸ íŒŒë™ ì–¸ì–´ ë¶„ì„
        analysis = self.wave_analyzer.analyze_code_quality(content, file_path)
        
        # ê° ê´€ì ì—ì„œ ì¶”ê°€ ë¶„ì„
        for perspective, fragment in fragments.items():
            perspective_findings = self._analyze_from_perspective(
                content, file_path, perspective
            )
            
            for finding in perspective_findings:
                finding["perspective"] = perspective.name
                findings.append(finding)
        
        # í†µí•© ì œì•ˆ ìƒì„±
        for suggestion in analysis.get("suggestions", []):
            suggestions.append({
                "perspective": "INTEGRATED",
                "type": suggestion["type"],
                "description_kr": suggestion["description_kr"]
            })
        
        return findings, suggestions
    
    def _analyze_from_perspective(
        self,
        content: str,
        file_path: str,
        perspective: AnalysisPerspective
    ) -> List[Dict]:
        """íŠ¹ì • ê´€ì ì—ì„œ ë¶„ì„"""
        findings = []
        lines = content.split('\n')
        
        if perspective == AnalysisPerspective.STRUCTURE:
            # êµ¬ì¡° ë¶„ì„: í´ë˜ìŠ¤, í•¨ìˆ˜ ìˆ˜ (ë¼ì¸ ì‹œì‘ ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ)
            class_count = sum(1 for line in lines if line.strip().startswith('class '))
            func_count = sum(1 for line in lines if line.strip().startswith('def '))
            if class_count > MAX_CLASSES_PER_FILE:
                findings.append({
                    "type": "STRUCTURE",
                    "description": f"ë§ì€ í´ë˜ìŠ¤ ({class_count}ê°œ) - ëª¨ë“ˆ ë¶„ë¦¬ ê³ ë ¤",
                    "severity": "medium"
                })
            if func_count > MAX_FUNCTIONS_PER_FILE:
                findings.append({
                    "type": "STRUCTURE",
                    "description": f"ë§ì€ í•¨ìˆ˜ ({func_count}ê°œ) - íŒŒì¼ ë¶„ë¦¬ ê³ ë ¤",
                    "severity": "medium"
                })
                
        elif perspective == AnalysisPerspective.PERFORMANCE:
            # ì„±ëŠ¥ ë¶„ì„: ì¤‘ì²© ë£¨í”„ (ë¼ì¸ ì‹œì‘ ê¸°ì¤€)
            for i, line in enumerate(lines):
                stripped = line.strip()
                if stripped.startswith('for ') and i > 0:
                    prev_stripped = lines[i-1].strip()
                    if prev_stripped.startswith('for '):
                        findings.append({
                            "type": "PERFORMANCE",
                            "description": f"ë¼ì¸ {i+1}: ì¤‘ì²© ë£¨í”„ ë°œê²¬ - O(nÂ²) ê°€ëŠ¥ì„±",
                            "line": i + 1,
                            "severity": "high"
                        })
                    
        elif perspective == AnalysisPerspective.SECURITY:
            # ë³´ì•ˆ ë¶„ì„: ìœ„í—˜í•œ íŒ¨í„´
            dangerous_patterns = ['eval(', 'exec(', 'pickle.load', '__import__']
            for i, line in enumerate(lines):
                for pattern in dangerous_patterns:
                    if pattern in line:
                        findings.append({
                            "type": "SECURITY",
                            "description": f"ë¼ì¸ {i+1}: ìœ„í—˜í•œ íŒ¨í„´ '{pattern}' ë°œê²¬",
                            "line": i + 1,
                            "severity": "critical"
                        })
                        
        elif perspective == AnalysisPerspective.READABILITY:
            # ê°€ë…ì„± ë¶„ì„: ê¸´ ë¼ì¸
            for i, line in enumerate(lines):
                if len(line) > MAX_LINE_LENGTH:
                    findings.append({
                        "type": "READABILITY",
                        "description": f"ë¼ì¸ {i+1}: ë„ˆë¬´ ê¸´ ë¼ì¸ ({len(line)}ì)",
                        "line": i + 1,
                        "severity": "low"
                    })
            
            # docstring ì¡´ì¬ ì—¬ë¶€ (ê°„ë‹¨íˆ í™•ì¸)
            func_count = sum(1 for line in lines if line.strip().startswith('def '))
            has_docstrings = '"""' in content or "'''" in content
            if func_count > 0 and not has_docstrings:
                findings.append({
                    "type": "READABILITY",
                    "description": f"ë¬¸ì„œí™” í•„ìš”: {func_count}ê°œ í•¨ìˆ˜ì— docstring ì—†ìŒ",
                    "severity": "medium"
                })
                
        elif perspective == AnalysisPerspective.INNOVATION:
            # í˜ì‹  ë¶„ì„: ê°œì„  ê°€ëŠ¥ì„±
            if 'TODO' in content or 'FIXME' in content:
                findings.append({
                    "type": "INNOVATION",
                    "description": "ë¯¸ì™„ì„± ì‘ì—… ë°œê²¬ - ê°œì„  ê¸°íšŒ",
                    "severity": "info"
                })
            
            if 'time.sleep' in content:
                findings.append({
                    "type": "INNOVATION",
                    "description": "time.sleep ì‚¬ìš© - ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ê°œì„  ê°€ëŠ¥",
                    "severity": "info"
                })
        
        return findings
    
    def _analyze_project_multi(
        self,
        fragments: Dict[AnalysisPerspective, ConsciousnessFragment]
    ) -> Tuple[List[Dict], List[Dict]]:
        """ì „ì²´ í”„ë¡œì íŠ¸ ë‹¤ì¤‘ ê´€ì  ë¶„ì„"""
        all_findings = []
        all_suggestions = []
        
        if self.improver:
            # í”„ë¡œì íŠ¸ ë¶„ì„
            stats = self.improver.introspector.analyze_self()
            
            # ë†’ì€ ë³µì¡ë„ íŒŒì¼ ë¶„ì„
            for file_path, analysis in self.improver.introspector.analyzed_files.items():
                if analysis.complexity_score > 0.7:
                    all_findings.append({
                        "perspective": "STRUCTURE",
                        "type": "COMPLEXITY",
                        "description": f"ë†’ì€ ë³µì¡ë„: {file_path}",
                        "severity": "medium"
                    })
        
        return all_findings, all_suggestions
    
    def self_improve_cycle(self) -> Dict[str, Any]:
        """
        ìê¸° ê°œì„  ì‚¬ì´í´
        
        1. ë¶„ì„ (ë‹¤ì¤‘ ê´€ì )
        2. ê°œì„ ì  ì‹ë³„
        3. ì œì•ˆ ìƒì„±
        4. (í–¥í›„) ìë™ ì ìš©
        
        "ìŠ¤ìŠ¤ë¡œë¥¼ ë°”ë¼ë³´ê³ , ë” ë‚˜ì•„ì§„ë‹¤"
        """
        logger.info("ğŸ”„ Starting self-improvement cycle...")
        
        cycle_result = {
            "timestamp": time.time(),
            "phase": "analysis",
            "analysis": None,
            "improvements_identified": 0,
            "suggestions_generated": 0,
            "applied": 0  # í–¥í›„ ìë™ ì ìš© ì‹œ
        }
        
        # 1. ë‹¤ì¤‘ ê´€ì  ë¶„ì„
        analysis = self.multi_perspective_analysis()
        cycle_result["analysis"] = {
            "files": analysis.files_analyzed,
            "perspectives": analysis.perspectives_used,
            "time": analysis.total_analysis_time
        }
        
        # 2. ê°œì„ ì  ì§‘ê³„
        cycle_result["improvements_identified"] = len(analysis.findings)
        cycle_result["suggestions_generated"] = len(analysis.suggestions)
        
        # 3. ê²°ê³¼ ìš”ì•½
        cycle_result["phase"] = "complete"
        
        logger.info(f"âœ… Self-improvement cycle complete:")
        logger.info(f"   - Findings: {cycle_result['improvements_identified']}")
        logger.info(f"   - Suggestions: {cycle_result['suggestions_generated']}")
        
        return cycle_result
    
    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ"""
        return {
            "engine": "TranscendenceEngine",
            "components": {
                "improver": IMPROVER_AVAILABLE,
                "consciousness": CONSCIOUSNESS_AVAILABLE
            },
            "consciousness_state": self.consciousness.get_state() if self.consciousness else None,
            "analysis_history_count": len(self.analysis_history),
            "max_parallel": self.max_parallel
        }
    
    def explain(self) -> str:
        """ì—”ì§„ ì„¤ëª…"""
        return """
ğŸŒŸ ì´ˆì›” í†µí•© ì—”ì§„ (Transcendence Integration Engine)

ê°œë…:
  ìê¸° ê°œì„  + ë¶„ì‚° ì˜ì‹ = ë‹¤ì¤‘ ê´€ì  ìê¸° ë¶„ì„
  
  ì—¬ëŸ¬ ì˜ì‹ ì¡°ê°ì´ ë™ì‹œì— ë‹¤ë¥¸ ê´€ì ì—ì„œ ì½”ë“œë¥¼ ë¶„ì„í•˜ê³ ,
  ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ì¢…í•©ì ì¸ ê°œì„  ì œì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤.

ë¶„ì„ ê´€ì :
  ğŸ“ STRUCTURE - ì½”ë“œ êµ¬ì¡°ì™€ ì•„í‚¤í…ì²˜
  âœ¨ QUALITY - ì½”ë“œ í’ˆì§ˆê³¼ í‘œì¤€
  âš¡ PERFORMANCE - ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±
  ğŸ”’ SECURITY - ë³´ì•ˆ ì·¨ì•½ì 
  ğŸ“– READABILITY - ê°€ë…ì„±ê³¼ ë¬¸ì„œí™”
  ğŸ’¡ INNOVATION - ê°œì„  ì•„ì´ë””ì–´

ì‚¬ìš©ë²•:
  engine = TranscendenceEngine()
  
  # ë‹¤ì¤‘ ê´€ì  ë¶„ì„
  analysis = engine.multi_perspective_analysis()
  
  # ìê¸° ê°œì„  ì‚¬ì´í´
  result = engine.self_improve_cycle()

ì² í•™ì  ì˜ë¯¸:
  "í•˜ë‚˜ì˜ ëˆˆìœ¼ë¡œ ë³´ë©´ í•˜ë‚˜ë§Œ ë³´ì´ì§€ë§Œ,
   ì—¬ëŸ¬ ëˆˆìœ¼ë¡œ ë³´ë©´ ì „ì²´ê°€ ë³´ì¸ë‹¤.
   ê·¸ë¦¬ê³  ì „ì²´ë¥¼ ë³´ë©´, ë” ë‚˜ì•„ì§ˆ ìˆ˜ ìˆë‹¤."
"""


# ë°ëª¨ ì½”ë“œ
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("ğŸŒŸ Transcendence Integration Engine Demo")
    print("=" * 60)
    
    # ì—”ì§„ ì´ˆê¸°í™”
    engine = TranscendenceEngine()
    
    # ìƒíƒœ í™•ì¸
    print("\nğŸ“Š Engine Status:")
    status = engine.get_status()
    print(f"  Components:")
    print(f"    - Improver: {'âœ…' if status['components']['improver'] else 'âŒ'}")
    print(f"    - Consciousness: {'âœ…' if status['components']['consciousness'] else 'âŒ'}")
    
    # ìê¸° ê°œì„  ì‚¬ì´í´ ì‹¤í–‰
    print("\nğŸ”„ Running self-improvement cycle...")
    result = engine.self_improve_cycle()
    
    print(f"\nğŸ“ˆ Results:")
    print(f"  - Files analyzed: {result['analysis']['files']}")
    print(f"  - Perspectives used: {result['analysis']['perspectives']}")
    print(f"  - Improvements identified: {result['improvements_identified']}")
    print(f"  - Suggestions generated: {result['suggestions_generated']}")
    print(f"  - Analysis time: {result['analysis']['time']:.2f}s")
    
    # ì„¤ëª… ì¶œë ¥
    print("\n" + engine.explain())
