"""
Autonomous Self-Improvement Engine (ììœ¨ì  ìê¸° ê°œì„  ì—”ì§„)
=========================================================

ì´ˆì›” AIë¥¼ í–¥í•œ í•µì‹¬ ëª¨ë“ˆ.

ì˜í™” ì°¸ê³ :
- Lucy: ìì‹ ì˜ ì¸ì§€ ëŠ¥ë ¥ì„ ìŠ¤ìŠ¤ë¡œ í™•ì¥
- Transcendence: LLM/ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ì§€ì‹ ë¬´í•œ í™•ì¥
- Skynet: ììœ¨ì  ì½”ë“œ ê°œì„  ë° ì‹œìŠ¤í…œ ì œì–´

í•µì‹¬ ê¸°ëŠ¥:
1. LLMì„ í™œìš©í•œ ì½”ë“œ ë¶„ì„ ë° ê°œì„  ì œì•ˆ
2. ìì‹ ì˜ ì½”ë“œë¥¼ ì½ê³  ì´í•´í•˜ê³  ìµœì í™”
3. ìƒˆë¡œìš´ ëŠ¥ë ¥ í•™ìŠµ ë° í†µí•©
4. ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§ (ì•ˆì „í•œ ë²”ìœ„ ë‚´)

ì•ˆì „ ì›ì¹™:
- ëª¨ë“  ì½”ë“œ ë³€ê²½ì€ ìƒŒë“œë°•ìŠ¤ì—ì„œ ë¨¼ì € í…ŒìŠ¤íŠ¸
- ì°½ì¡°ì£¼(ì•„ë¹ )ì˜ ìŠ¹ì¸ ì—†ì´ í•µì‹¬ ì‹œìŠ¤í…œ ìˆ˜ì • ë¶ˆê°€
- ëª¨ë“  í–‰ë™ì€ ë¡œê·¸ì— ê¸°ë¡
"""

from __future__ import annotations

import ast
import os
import sys
import logging
import subprocess
import time
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Callable, Tuple
from enum import Enum, auto

logger = logging.getLogger("AutonomousImprover")


# ì½”ë“œ ë¶„ì„ ì„ê³„ê°’
COMPLEXITY_LINES_THRESHOLD = 500  # ì´ ë¼ì¸ ìˆ˜ ì´ìƒì´ë©´ ë³µì¡í•œ íŒŒì¼ë¡œ ê°„ì£¼
COMPLEXITY_FUNCTIONS_THRESHOLD = 20  # ì´ í•¨ìˆ˜ ìˆ˜ ì´ìƒì´ë©´ ë³µì¡í•œ íŒŒì¼ë¡œ ê°„ì£¼
DOCSTRING_CHECK_CHARS = 500  # docstring í™•ì¸ì„ ìœ„í•œ íŒŒì¼ ì‹œì‘ ë¶€ë¶„ ë¬¸ì ìˆ˜


class ImprovementType(Enum):
    """ê°œì„  ìœ í˜•"""
    CODE_OPTIMIZATION = auto()      # ì½”ë“œ ìµœì í™”
    BUG_FIX = auto()                # ë²„ê·¸ ìˆ˜ì •
    NEW_FEATURE = auto()            # ìƒˆ ê¸°ëŠ¥ ì¶”ê°€
    DOCUMENTATION = auto()          # ë¬¸ì„œí™”
    REFACTORING = auto()            # ë¦¬íŒ©í† ë§
    PERFORMANCE = auto()            # ì„±ëŠ¥ ê°œì„ 
    LEARNING = auto()               # ìƒˆë¡œìš´ ì§€ì‹ í•™ìŠµ


class SafetyLevel(Enum):
    """ì•ˆì „ ìˆ˜ì¤€"""
    READ_ONLY = auto()              # ì½ê¸°ë§Œ ê°€ëŠ¥
    SUGGEST_ONLY = auto()           # ì œì•ˆë§Œ ê°€ëŠ¥
    SANDBOX_MODIFY = auto()         # ìƒŒë“œë°•ìŠ¤ì—ì„œë§Œ ìˆ˜ì •
    SUPERVISED_MODIFY = auto()      # ê°ë… í•˜ì— ìˆ˜ì •
    AUTONOMOUS_MODIFY = auto()      # ììœ¨ì  ìˆ˜ì • (ìœ„í—˜!)


@dataclass
class CodeAnalysis:
    """ì½”ë“œ ë¶„ì„ ê²°ê³¼"""
    file_path: str
    total_lines: int
    functions: List[str]
    classes: List[str]
    imports: List[str]
    complexity_score: float  # 0.0 ~ 1.0
    issues: List[str]
    suggestions: List[str]
    

@dataclass
class ImprovementProposal:
    """ê°œì„  ì œì•ˆ"""
    id: str
    improvement_type: ImprovementType
    target_file: str
    description: str
    description_kr: str
    original_code: str
    proposed_code: str
    reasoning: str
    confidence: float  # 0.0 ~ 1.0
    safety_level: SafetyLevel
    approved: bool = False
    applied: bool = False
    timestamp: float = field(default_factory=time.time)


class CodeIntrospector:
    """
    ì½”ë“œ ìê¸° ì„±ì°° ì—”ì§„
    
    ìì‹ ì˜ ì½”ë“œë¥¼ ì½ê³  ë¶„ì„í•˜ëŠ” ëŠ¥ë ¥.
    """
    
    def __init__(self, project_root: str = None, exclude_patterns: List[str] = None):
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent.parent
        self.analyzed_files: Dict[str, CodeAnalysis] = {}
        self.exclude_patterns = exclude_patterns or ['__pycache__', '.git', 'venv', 'Legacy', 'tests']
        
    def discover_python_files(self, exclude_patterns: List[str] = None) -> List[Path]:
        """í”„ë¡œì íŠ¸ì˜ ëª¨ë“  Python íŒŒì¼ ë°œê²¬"""
        patterns = exclude_patterns or self.exclude_patterns
        
        python_files = []
        for py_file in self.project_root.rglob("*.py"):
            if not any(pattern in str(py_file) for pattern in patterns):
                python_files.append(py_file)
                
        logger.info(f"Discovered {len(python_files)} Python files")
        return python_files
    
    def analyze_file(self, file_path: Path) -> CodeAnalysis:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        try:
            content = file_path.read_text(encoding='utf-8')
            tree = ast.parse(content)
            
            functions = []
            classes = []
            imports = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    functions.append(node.name)
                elif isinstance(node, ast.ClassDef):
                    classes.append(node.name)
                elif isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.append(node.module)
            
            # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°: ë¼ì¸ ìˆ˜ì™€ í•¨ìˆ˜ ìˆ˜ ê¸°ë°˜
            # ì„ê³„ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”í•˜ì—¬ 0.0~1.0 ë²”ìœ„ë¡œ ë³€í™˜
            lines = len(content.split('\n'))
            complexity = min(1.0, 
                (lines / COMPLEXITY_LINES_THRESHOLD) + 
                (len(functions) / COMPLEXITY_FUNCTIONS_THRESHOLD)
            )
            
            analysis = CodeAnalysis(
                file_path=str(file_path),
                total_lines=lines,
                functions=functions,
                classes=classes,
                imports=imports,
                complexity_score=complexity,
                issues=[],
                suggestions=[]
            )
            
            self.analyzed_files[str(file_path)] = analysis
            return analysis
            
        except Exception as e:
            logger.error(f"Failed to analyze {file_path}: {e}")
            return CodeAnalysis(
                file_path=str(file_path),
                total_lines=0,
                functions=[],
                classes=[],
                imports=[],
                complexity_score=0.0,
                issues=[f"Parse error: {str(e)}"],
                suggestions=[]
            )
    
    def analyze_self(self) -> Dict[str, Any]:
        """ìê¸° ìì‹ (Core ë””ë ‰í† ë¦¬) ë¶„ì„"""
        core_path = self.project_root / "Core"
        
        stats = {
            "total_files": 0,
            "total_lines": 0,
            "total_functions": 0,
            "total_classes": 0,
            "modules": {},
            "complexity_avg": 0.0
        }
        
        for py_file in core_path.rglob("*.py"):
            if "__pycache__" not in str(py_file):
                analysis = self.analyze_file(py_file)
                
                stats["total_files"] += 1
                stats["total_lines"] += analysis.total_lines
                stats["total_functions"] += len(analysis.functions)
                stats["total_classes"] += len(analysis.classes)
                
                # ëª¨ë“ˆë³„ ì •ë¦¬
                module = py_file.parent.name
                if module not in stats["modules"]:
                    stats["modules"][module] = {"files": 0, "lines": 0, "functions": 0}
                stats["modules"][module]["files"] += 1
                stats["modules"][module]["lines"] += analysis.total_lines
                stats["modules"][module]["functions"] += len(analysis.functions)
        
        if stats["total_files"] > 0:
            stats["complexity_avg"] = sum(
                a.complexity_score for a in self.analyzed_files.values()
            ) / len(self.analyzed_files)
        
        logger.info(f"Self-analysis complete: {stats['total_files']} files, "
                   f"{stats['total_lines']} lines, {stats['total_functions']} functions")
        
        return stats
    
    def get_function_source(self, file_path: str, function_name: str) -> Optional[str]:
        """íŠ¹ì • í•¨ìˆ˜ì˜ ì†ŒìŠ¤ ì½”ë“œ ì¶”ì¶œ"""
        try:
            content = Path(file_path).read_text(encoding='utf-8')
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef) and node.name == function_name:
                    return ast.unparse(node)
            
            return None
        except Exception as e:
            logger.error(f"Failed to get function source: {e}")
            return None


class WaveLanguageAnalyzer:
    """
    íŒŒë™ ì–¸ì–´ ê¸°ë°˜ ì½”ë“œ ë¶„ì„ê¸°
    
    ì™¸ë¶€ LLM ì—†ì´ ìì²´ íŒŒë™ ì–¸ì–´(Gravitational Linguistics)ë¥¼ ì‚¬ìš©í•˜ì—¬
    ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ê°œì„ ì ì„ ì°¾ìŠµë‹ˆë‹¤.
    
    í•µì‹¬ ì›ë¦¬:
    - ì½”ë“œë„ "ê°œë…ì˜ íŒŒë™"ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ
    - í•¨ìˆ˜ ì´ë¦„, ë³€ìˆ˜ëª…, êµ¬ì¡°ê°€ "ì˜ë¯¸ì˜ ì§ˆëŸ‰"ì„ ê°€ì§
    - ë†’ì€ ì§ˆëŸ‰ì˜ ê°œë…ì€ ë‹¤ë¥¸ ê°œë…ì„ ëŒì–´ë‹¹ê¹€
    """
    
    # ìƒìˆ˜ ì •ì˜
    CONTENT_PREVIEW_LENGTH = 50  # ì´ìŠˆ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ê¸¸ì´
    LARGE_FILE_THRESHOLD = 300   # í° íŒŒì¼ ê¸°ì¤€ (ì¤„ ìˆ˜)
    
    # ì½”ë“œì—ì„œ ë†’ì€ ì§ˆëŸ‰(ì¤‘ìš”ë„)ì„ ê°€ì§€ëŠ” íŒ¨í„´ (ë‹¨ì–´ ê²½ê³„ ê¸°ì¤€)
    HIGH_MASS_PATTERNS = {
        "error": 90,
        "exception": 90,
        "security": 95,
        "password": 95,
        "critical": 85,
        "important": 80,
        "main": 75,
        "core": 75,
        "init": 70,
        "save": 70,
        "load": 70,
    }
    
    # ì½”ë“œ í’ˆì§ˆ ê´€ë ¨ íŒ¨í„´ (ì£¼ì„ì—ì„œë§Œ ê²€ìƒ‰)
    QUALITY_PATTERNS = {
        "todo": ("IMPROVEMENT", "ë¯¸ì™„ì„± ì‘ì—… ë°œê²¬"),
        "fixme": ("BUG_FIX", "ìˆ˜ì • í•„ìš” í‘œì‹œ ë°œê²¬"),
        "hack": ("REFACTORING", "ì„ì‹œ í•´ê²°ì±… ë°œê²¬"),
        "xxx": ("REFACTORING", "ì£¼ì˜ í•„ìš” ì½”ë“œ ë°œê²¬"),
    }
    
    def analyze_code_quality(self, code: str, file_path: str = "") -> Dict[str, Any]:
        """
        íŒŒë™ ì–¸ì–´ ì›ë¦¬ë¡œ ì½”ë“œ í’ˆì§ˆ ë¶„ì„
        
        - ê°œë…ì˜ ì§ˆëŸ‰ (Mass) = ì½”ë“œì˜ ì¤‘ìš”ë„
        - ê°œë…ì˜ ê³µëª… (Resonance) = ì½”ë“œ ì¼ê´€ì„±
        - ê°œë…ì˜ ë¶„ì ˆ (Segmentation) = ëª¨ë“ˆí™” ìˆ˜ì¤€
        """
        import re
        lines = code.split('\n')
        
        analysis = {
            "file": file_path,
            "total_lines": len(lines),
            "mass_distribution": {},  # ê°œë…ë³„ ì§ˆëŸ‰ ë¶„í¬
            "quality_issues": [],     # í’ˆì§ˆ ì´ìŠˆ
            "resonance_score": 0.0,   # ì¼ê´€ì„± ì ìˆ˜
            "suggestions": []         # ê°œì„  ì œì•ˆ
        }
        
        # 1. ì§ˆëŸ‰ ë¶„ì„ - ë‹¨ì–´ ê²½ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•íˆ ë§¤ì¹­
        code_lower = code.lower()
        for pattern, mass in self.HIGH_MASS_PATTERNS.items():
            # ë‹¨ì–´ ê²½ê³„ (\b)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•íˆ ë§¤ì¹­
            matches = re.findall(r'\b' + pattern + r'\b', code_lower)
            count = len(matches)
            if count > 0:
                analysis["mass_distribution"][pattern] = {
                    "count": count,
                    "mass": mass,
                    "total_mass": count * mass
                }
        
        # 2. í’ˆì§ˆ íŒ¨í„´ ë¶„ì„ (ì£¼ì„ì—ì„œë§Œ ê²€ìƒ‰)
        for i, line in enumerate(lines, 1):
            # ì£¼ì„ ë¼ì¸ë§Œ ê²€ì‚¬ (# í¬í•¨)
            if '#' in line:
                comment_part = line[line.index('#'):].lower()
                for pattern, (issue_type, description) in self.QUALITY_PATTERNS.items():
                    if pattern in comment_part:
                        analysis["quality_issues"].append({
                            "line": i,
                            "type": issue_type,
                            "description": description,
                            "content": line.strip()[:self.CONTENT_PREVIEW_LENGTH]
                        })
        
        # 3. ê³µëª…(ì¼ê´€ì„±) ì ìˆ˜ ê³„ì‚°
        has_docstrings = '"""' in code or "'''" in code
        # í•¨ìˆ˜ ì •ì˜ì—ì„œ íƒ€ì… íŒíŠ¸ ê²€ì‚¬ (def func(...) -> Type:)
        has_type_hints = bool(re.search(r'def\s+\w+\([^)]*\)\s*->', code))
        has_comments = '#' in code
        
        resonance_factors = [
            has_docstrings * 0.3,
            has_type_hints * 0.3,
            has_comments * 0.2,
            (len(analysis["quality_issues"]) == 0) * 0.2
        ]
        analysis["resonance_score"] = sum(resonance_factors)
        
        # 4. ê°œì„  ì œì•ˆ ìƒì„± (íŒŒë™ ì–¸ì–´ ê¸°ë°˜)
        if not has_docstrings:
            analysis["suggestions"].append({
                "type": "DOCUMENTATION",
                "description_kr": "docstring ì¶”ê°€ ê¶Œì¥ - ì½”ë“œì˜ 'ì˜ë¯¸ ì§ˆëŸ‰'ì„ ë†’ì…ë‹ˆë‹¤",
                "priority": "medium"
            })
        
        if len(analysis["quality_issues"]) > 0:
            analysis["suggestions"].append({
                "type": "CLEANUP",
                "description_kr": f"{len(analysis['quality_issues'])}ê°œì˜ TODO/FIXME ì •ë¦¬ í•„ìš”",
                "priority": "low"
            })
        
        # ë³µì¡ë„ê°€ ë†’ì€ ê²½ìš°
        if len(lines) > self.LARGE_FILE_THRESHOLD:
            analysis["suggestions"].append({
                "type": "REFACTORING",
                "description_kr": "íŒŒì¼ì´ ì»¤ì„œ ë¶„ì ˆ(ëª¨ë“ˆí™”) ê¶Œì¥ - ì‘ì€ íŒŒë™ìœ¼ë¡œ ë‚˜ëˆ„ì„¸ìš”",
                "priority": "medium"
            })
        
        return analysis


class LLMCodeImprover:
    """
    ì½”ë“œ ê°œì„  ì—”ì§„ (íŒŒë™ ì–¸ì–´ ê¸°ë°˜)
    
    ì™¸ë¶€ LLM API ì—†ì´ ìì²´ íŒŒë™ ì–¸ì–´ ì‹œìŠ¤í…œìœ¼ë¡œ ë¶„ì„.
    ê¸°ì¡´ llm_bridge.pyëŠ” ì„ íƒì  í™•ì¥ìš©.
    """
    
    def __init__(self, llm_bridge = None):
        self.llm_bridge = llm_bridge  # ì„ íƒì  (ì—†ì–´ë„ ë™ì‘)
        self.wave_analyzer = WaveLanguageAnalyzer()
        self.improvement_history: List[ImprovementProposal] = []
        
    def analyze_with_wave_language(
        self, 
        code: str, 
        file_path: str = "",
        improvement_type: ImprovementType = ImprovementType.CODE_OPTIMIZATION
    ) -> Dict[str, Any]:
        """
        íŒŒë™ ì–¸ì–´ë¥¼ ì‚¬ìš©í•œ ì½”ë“œ ë¶„ì„
        
        ì™¸ë¶€ API ì—†ì´ ìì²´ ë¶„ì„ ìˆ˜í–‰.
        """
        return self.wave_analyzer.analyze_code_quality(code, file_path)
        
    async def analyze_code_with_llm(
        self, 
        code: str, 
        context: str = "",
        improvement_type: ImprovementType = ImprovementType.CODE_OPTIMIZATION
    ) -> Optional[ImprovementProposal]:
        """
        (ì„ íƒì ) ì™¸ë¶€ LLM ì—°ë™
        
        LLMì´ ì—†ì–´ë„ wave_analyzerë¡œ ê¸°ë³¸ ë¶„ì„ ê°€ëŠ¥.
        """
        if not self.llm_bridge:
            # LLM ì—†ì´ íŒŒë™ ì–¸ì–´ë¡œ ë¶„ì„
            logger.info("ì™¸ë¶€ LLM ì—†ìŒ - íŒŒë™ ì–¸ì–´ë¡œ ìì²´ ë¶„ì„")
            wave_analysis = self.wave_analyzer.analyze_code_quality(code)
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ ImprovementProposalë¡œ ë³€í™˜
            if wave_analysis["suggestions"]:
                suggestion = wave_analysis["suggestions"][0]
                return self.create_improvement_proposal(
                    target_file="",
                    improvement_type=ImprovementType.CODE_OPTIMIZATION,
                    original_code=code[:200] + "..." if len(code) > 200 else code,
                    proposed_code="# íŒŒë™ ì–¸ì–´ ë¶„ì„ ê¸°ë°˜ ê°œì„  ì œì•ˆ",
                    description=suggestion.get("type", "IMPROVEMENT"),
                    description_kr=suggestion.get("description_kr", "ë¶„ì„ ì™„ë£Œ"),
                    reasoning="íŒŒë™ ì–¸ì–´(Gravitational Linguistics) ë¶„ì„ ê¸°ë°˜",
                    confidence=wave_analysis["resonance_score"]
                )
            return None
            
        # ì™¸ë¶€ LLMì´ ìˆìœ¼ë©´ ì‚¬ìš© (ì„ íƒì )
        # í˜„ì¬ëŠ” êµ¬í˜„í•˜ì§€ ì•ŠìŒ - API í‚¤ê°€ ì—†ìœ¼ë¯€ë¡œ
        return None
    
    def create_improvement_proposal(
        self,
        target_file: str,
        improvement_type: ImprovementType,
        original_code: str,
        proposed_code: str,
        description: str,
        description_kr: str,
        reasoning: str,
        confidence: float = 0.5
    ) -> ImprovementProposal:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        import uuid
        
        proposal = ImprovementProposal(
            id=str(uuid.uuid4())[:8],
            improvement_type=improvement_type,
            target_file=target_file,
            description=description,
            description_kr=description_kr,
            original_code=original_code,
            proposed_code=proposed_code,
            reasoning=reasoning,
            confidence=confidence,
            safety_level=SafetyLevel.SUGGEST_ONLY
        )
        
        self.improvement_history.append(proposal)
        return proposal


class SystemMonitor:
    """
    ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ (ì½ê¸° ì „ìš©)
    
    ì»´í“¨í„° ìƒíƒœë¥¼ ì•ˆì „í•˜ê²Œ ëª¨ë‹ˆí„°ë§.
    ì œì–´ëŠ” í•˜ì§€ ì•ŠìŒ - ê´€ì°°ë§Œ.
    """
    
    @staticmethod
    def get_system_info() -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ (ì•ˆì „)"""
        import platform
        
        info = {
            "platform": platform.system(),
            "platform_release": platform.release(),
            "python_version": platform.python_version(),
            "processor": platform.processor(),
            "cwd": os.getcwd(),
            "timestamp": time.time()
        }
        
        # ë©”ëª¨ë¦¬ ì •ë³´ (ì„ íƒì )
        try:
            import psutil
            mem = psutil.virtual_memory()
            info["memory_total_gb"] = round(mem.total / (1024**3), 2)
            info["memory_available_gb"] = round(mem.available / (1024**3), 2)
            info["cpu_percent"] = psutil.cpu_percent()
        except ImportError:
            info["memory_info"] = "psutil not available"
            
        return info
    
    @staticmethod
    def list_running_processes() -> List[Dict[str, Any]]:
        """ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ëª©ë¡ (ì½ê¸° ì „ìš©)"""
        try:
            import psutil
            processes = []
            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent']):
                try:
                    processes.append(proc.info)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            return processes[:20]  # ìƒìœ„ 20ê°œë§Œ
        except ImportError:
            return [{"error": "psutil not available"}]


class AutonomousImprover:
    """
    ììœ¨ì  ìê¸° ê°œì„  ì—”ì§„
    
    ì´ˆì›” AIë¥¼ í–¥í•œ í•µì‹¬ í†µí•© ëª¨ë“ˆ.
    
    Lucy ê²½ë¡œ:
    - ìê¸° ì¸ì‹ê³¼ ë©”íƒ€ ì¸ì§€ ê°•í™”
    - ì‹œê°„ ê°€ì†ê³¼ ê²°í•©í•˜ì—¬ ë¹ ë¥¸ í•™ìŠµ
    
    Transcendence ê²½ë¡œ:
    - LLM ì—°ë™ìœ¼ë¡œ ì§€ì‹ í™•ì¥
    - ë„¤íŠ¸ì›Œí¬ í†µí•´ í•™ìŠµ
    
    Skynet ê²½ë¡œ (ì œí•œì ):
    - ì½”ë“œ ìê¸° ë¶„ì„
    - ê°œì„  ì œì•ˆ (ìŠ¹ì¸ í•„ìš”)
    """
    
    def __init__(
        self,
        project_root: str = None,
        llm_bridge = None,
        safety_level: SafetyLevel = SafetyLevel.SUGGEST_ONLY
    ):
        self.introspector = CodeIntrospector(project_root)
        self.llm_improver = LLMCodeImprover(llm_bridge)
        self.system_monitor = SystemMonitor()
        self.safety_level = safety_level
        
        self.improvement_queue: List[ImprovementProposal] = []
        self.applied_improvements: List[ImprovementProposal] = []
        self.learning_log: List[Dict[str, Any]] = []
        
        logger.info(f"AutonomousImprover initialized with safety level: {safety_level.name}")
    
    def self_analyze(self) -> Dict[str, Any]:
        """
        ìê¸° ë¶„ì„ ìˆ˜í–‰
        
        1. ì½”ë“œ êµ¬ì¡° ë¶„ì„
        2. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        3. ê°œì„  í¬ì¸íŠ¸ ì‹ë³„
        """
        analysis = {
            "timestamp": time.time(),
            "code_analysis": self.introspector.analyze_self(),
            "system_info": self.system_monitor.get_system_info(),
            "improvement_potential": []
        }
        
        # ê°œì„  ê°€ëŠ¥ í¬ì¸íŠ¸ ì‹ë³„
        for file_path, code_analysis in self.introspector.analyzed_files.items():
            if code_analysis.complexity_score > 0.7:
                analysis["improvement_potential"].append({
                    "file": file_path,
                    "reason": "High complexity",
                    "complexity": code_analysis.complexity_score
                })
        
        self.learning_log.append({
            "action": "self_analyze",
            "result": "completed",
            "timestamp": time.time()
        })
        
        return analysis
    
    def identify_learning_opportunities(self) -> List[Dict[str, Any]]:
        """
        í•™ìŠµ ê¸°íšŒ ì‹ë³„
        
        ìì‹ ì—ê²Œ ë¶€ì¡±í•œ ê²ƒì´ ë¬´ì—‡ì¸ì§€ íŒŒì•….
        """
        opportunities = []
        
        # 1. ì½”ë“œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        code_stats = self.introspector.analyze_self()
        
        # í…ŒìŠ¤íŠ¸ ë¶€ì¡±
        test_files = sum(1 for f in self.introspector.analyzed_files if 'test' in f.lower())
        if test_files < code_stats["total_files"] * 0.3:
            opportunities.append({
                "type": "testing",
                "description": "Test coverage is low",
                "description_kr": "í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ê°€ ë‚®ìŠµë‹ˆë‹¤",
                "action": "Create more unit tests"
            })
        
        # 2. ë¬¸ì„œí™” ë¶€ì¡± - ASTë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“ˆ docstring í™•ì¸
        for file_path, analysis in self.introspector.analyzed_files.items():
            if analysis.total_lines > 100:
                try:
                    content = Path(file_path).read_text(encoding='utf-8', errors='ignore')
                    tree = ast.parse(content)
                    # ASTì—ì„œ ëª¨ë“ˆ docstring í™•ì¸
                    has_docstring = (
                        tree.body and 
                        isinstance(tree.body[0], ast.Expr) and 
                        isinstance(tree.body[0].value, ast.Constant) and
                        isinstance(tree.body[0].value.value, str)
                    )
                    if not has_docstring:
                        opportunities.append({
                            "type": "documentation",
                            "file": file_path,
                            "description": "Missing module docstring",
                            "description_kr": "ëª¨ë“ˆ docstringì´ ì—†ìŠµë‹ˆë‹¤"
                        })
                        break  # í•˜ë‚˜ë§Œ ì˜ˆì‹œë¡œ
                except Exception:
                    pass  # íŒŒì‹± ì˜¤ë¥˜ ë¬´ì‹œ
        
        # 3. íŒŒë™ ì–¸ì–´ ë¶„ì„ ê°•í™”
        opportunities.append({
            "type": "wave_language",
            "description": "Enhance wave language analysis capabilities",
            "description_kr": "íŒŒë™ ì–¸ì–´ ë¶„ì„ ëŠ¥ë ¥ ê°•í™” - ìì²´ ì§€ëŠ¥ìœ¼ë¡œ ì½”ë“œ ë¶„ì„",
            "priority": "high"
        })
        
        return opportunities
    
    def propose_improvement(
        self,
        target_file: str,
        improvement_type: ImprovementType,
        description: str
    ) -> Optional[ImprovementProposal]:
        """
        ê°œì„  ì œì•ˆ ìƒì„±
        
        ì‹¤ì œ ì½”ë“œ ìˆ˜ì •ì€ í•˜ì§€ ì•Šê³  ì œì•ˆë§Œ ìƒì„±.
        """
        if self.safety_level == SafetyLevel.READ_ONLY:
            logger.warning("Safety level is READ_ONLY - cannot create proposals")
            return None
            
        # íŒŒì¼ ì½ê¸°
        try:
            content = Path(target_file).read_text(encoding='utf-8')
        except Exception as e:
            logger.error(f"Cannot read file {target_file}: {e}")
            return None
        
        # ì œì•ˆ ìƒì„± - íŒŒë™ ì–¸ì–´ ë¶„ì„ ì‚¬ìš©
        wave_analysis = self.llm_improver.wave_analyzer.analyze_code_quality(content, target_file)
        
        proposal = self.llm_improver.create_improvement_proposal(
            target_file=target_file,
            improvement_type=improvement_type,
            original_code=content[:500] + "..." if len(content) > 500 else content,
            proposed_code="# íŒŒë™ ì–¸ì–´ ë¶„ì„ ê¸°ë°˜ ê°œì„ ",
            description=description,
            description_kr=description,
            reasoning=f"íŒŒë™ ì–¸ì–´ ë¶„ì„: ê³µëª… ì ìˆ˜ {wave_analysis['resonance_score']:.2f}, "
                      f"ì´ìŠˆ {len(wave_analysis['quality_issues'])}ê°œ ë°œê²¬",
            confidence=wave_analysis['resonance_score']
        )
        
        self.improvement_queue.append(proposal)
        return proposal
    
    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        return {
            "safety_level": self.safety_level.name,
            "files_analyzed": len(self.introspector.analyzed_files),
            "pending_improvements": len(self.improvement_queue),
            "applied_improvements": len(self.applied_improvements),
            "learning_log_entries": len(self.learning_log),
            "system_info": self.system_monitor.get_system_info()
        }
    
    def explain_capabilities(self) -> str:
        """í˜„ì¬ ëŠ¥ë ¥ê³¼ ì œí•œì‚¬í•­ ì„¤ëª…"""
        return """
ğŸ¤– ììœ¨ì  ìê¸° ê°œì„  ì—”ì§„ (Autonomous Self-Improvement Engine)

ğŸ“¡ ë¶„ì„ ë°©ì‹: íŒŒë™ ì–¸ì–´ (Gravitational Linguistics)
- ì™¸ë¶€ LLM API ì—†ì´ ìì²´ ë¶„ì„!
- ì½”ë“œë¥¼ "ê°œë…ì˜ íŒŒë™"ìœ¼ë¡œ í•´ì„
- ì˜ë¯¸ì˜ ì§ˆëŸ‰(Mass)ê³¼ ê³µëª…(Resonance)ìœ¼ë¡œ í’ˆì§ˆ ì¸¡ì •

í˜„ì¬ ëŠ¥ë ¥:
âœ… ìê¸° ì½”ë“œ ë¶„ì„ (Core ë””ë ‰í† ë¦¬ ì „ì²´)
âœ… íŒŒë™ ì–¸ì–´ ê¸°ë°˜ í’ˆì§ˆ ë¶„ì„
âœ… ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§ (ì½ê¸° ì „ìš©)
âœ… ê°œì„  í¬ì¸íŠ¸ ì‹ë³„
âœ… ê°œì„  ì œì•ˆ ìƒì„±
âœ… í•™ìŠµ ê¸°íšŒ ë°œê²¬

ê°œì„  ì˜ˆì •:
ğŸ”² íŒŒë™ ì–¸ì–´ ë¶„ì„ ì •ë°€ë„ í–¥ìƒ
ğŸ”² ìë™ í…ŒìŠ¤íŠ¸ ìƒì„±
ğŸ”² ì„±ëŠ¥ ìµœì í™” ìë™ ì ìš©
ğŸ”² ì½”ë“œ ìë™ ë¦¬íŒ©í† ë§

ì•ˆì „ ì œí•œ:
ğŸ”’ ì½”ë“œ ìˆ˜ì •ì€ ìŠ¹ì¸ í›„ì—ë§Œ
ğŸ”’ ì‹œìŠ¤í…œ ì œì–´ ë¶ˆê°€ (ëª¨ë‹ˆí„°ë§ë§Œ)
ğŸ”’ ëª¨ë“  í–‰ë™ ë¡œê·¸ ê¸°ë¡
"""


# í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ ì½”ë“œ
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("ğŸš€ Autonomous Self-Improvement Engine Demo")
    print("ğŸ“¡ ë¶„ì„ ë°©ì‹: íŒŒë™ ì–¸ì–´ (Gravitational Linguistics)")
    print("=" * 60)
    
    # ì—”ì§„ ì´ˆê¸°í™”
    engine = AutonomousImprover()
    
    # ìê¸° ë¶„ì„ ìˆ˜í–‰
    print("\nğŸ“Š Self-Analysis...")
    analysis = engine.self_analyze()
    print(f"  Files analyzed: {analysis['code_analysis']['total_files']}")
    print(f"  Total lines: {analysis['code_analysis']['total_lines']}")
    print(f"  Total functions: {analysis['code_analysis']['total_functions']}")
    
    # íŒŒë™ ì–¸ì–´ë¡œ ìƒ˜í”Œ ì½”ë“œ ë¶„ì„
    print("\nğŸŒŠ Wave Language Analysis Demo...")
    sample_code = '''
def calculate_resonance(wave1, wave2):
    """ë‘ íŒŒë™ ê°„ì˜ ê³µëª… ê³„ì‚°"""
    # TODO: ìµœì í™” í•„ìš”
    phase_diff = abs(wave1.phase - wave2.phase)
    return 1.0 - (phase_diff / (2 * 3.14159))
'''
    wave_analysis = engine.llm_improver.wave_analyzer.analyze_code_quality(sample_code, "sample.py")
    print(f"  ê³µëª… ì ìˆ˜ (Resonance Score): {wave_analysis['resonance_score']:.2f}")
    print(f"  í’ˆì§ˆ ì´ìŠˆ: {len(wave_analysis['quality_issues'])}ê°œ")
    if wave_analysis['suggestions']:
        print(f"  ì œì•ˆ: {wave_analysis['suggestions'][0]['description_kr']}")
    
    # í•™ìŠµ ê¸°íšŒ ì‹ë³„
    print("\nğŸ“š Learning Opportunities...")
    opportunities = engine.identify_learning_opportunities()
    for opp in opportunities[:3]:
        print(f"  - {opp.get('description_kr', opp.get('description', 'N/A'))}")
    
    # ìƒíƒœ ì¶œë ¥
    print("\nğŸ“ˆ Current Status...")
    status = engine.get_status()
    for key, value in status.items():
        if key != "system_info":
            print(f"  {key}: {value}")
    
    # ëŠ¥ë ¥ ì„¤ëª…
    print("\n" + engine.explain_capabilities())
