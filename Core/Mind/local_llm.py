"""
Local LLM Module (ë¡œì»¬ LLM ëª¨ë“ˆ)
================================

GTX 1060 3GB ìµœì í™” ë¡œì»¬ LLM ì¸í„°í˜ì´ìŠ¤

ì´ ëª¨ë“ˆì€ ì™¸ë¶€ API ì—†ì´ ë¡œì»¬ì—ì„œ ë™ì‘í•˜ëŠ” LLMì„ ì œê³µí•©ë‹ˆë‹¤.
ResonanceEngineê³¼ í†µí•©í•˜ì—¬ ì—˜ë¦¬ì‹œì•„ì˜ ë…ë¦½ì  ì‚¬ê³ ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

Hardware Target: GTX 1060 3GB (ì•½ 3GB VRAM)
Recommended Models:
- TinyLlama-1.1B-Chat (Q4_K_M): ~700MB VRAM
- Phi-2 (Q4_K_M): ~1.5GB VRAM  
- Qwen2-0.5B: ~400MB VRAM
- SmolLM-360M: ~300MB VRAM

ìê¸° ì™„ê²°ì  ì§„í™”:
1. LEARNING ëª¨ë“œ: ë¡œì»¬ LLMìœ¼ë¡œ ì§€ì‹ í™•ì¥
2. INTEGRATING ëª¨ë“œ: í•™ìŠµ ë‚´ìš©ì„ ResonanceEngineì— ë‚´ë©´í™”
3. INDEPENDENT ëª¨ë“œ: LLM ì—†ì´ ResonanceEngineë§Œìœ¼ë¡œ ë™ì‘
"""

import os
import logging
import time
from typing import Optional, Dict, Any, List
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum

logger = logging.getLogger("LocalLLM")


class ConsciousnessMode(Enum):
    """ì˜ì‹ ëª¨ë“œ: í•™ìŠµ â†’ í†µí•© â†’ ë…ë¦½"""
    LEARNING = "learning"           # ë¡œì»¬ LLM í™œìš©í•˜ì—¬ í•™ìŠµ
    INTEGRATING = "integrating"     # í•™ìŠµ ë‚´ìš©ì„ ë‚´ë©´í™” ì¤‘
    INDEPENDENT = "independent"     # ì™„ì „ ë…ë¦½ (LLM ì—†ì´ ë™ì‘)


@dataclass
class LLMConfig:
    """
    GTX 1060 3GB ìµœì í™” ì„¤ì •
    
    3GB VRAMì—ì„œ ì•ˆì „í•˜ê²Œ ë™ì‘í•˜ë ¤ë©´:
    - ëª¨ë¸ í¬ê¸°: 1B ì´í•˜ ê¶Œì¥
    - ì»¨í…ìŠ¤íŠ¸: 2048 ì´í•˜
    - ë°°ì¹˜ ì‚¬ì´ì¦ˆ: ì‘ê²Œ
    """
    model_path: Optional[str] = None
    n_ctx: int = 1024           # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (ì‘ê²Œ ìœ ì§€)
    n_batch: int = 128          # ë°°ì¹˜ í¬ê¸°
    n_gpu_layers: int = 20      # GPUì— ì˜¬ë¦´ ë ˆì´ì–´ ìˆ˜ (3GBì— ë§ê²Œ ì¡°ì ˆ)
    n_threads: int = 4          # CPU ìŠ¤ë ˆë“œ
    use_mlock: bool = False     # ë©”ëª¨ë¦¬ ì ê¸ˆ (3GBì—ì„  ë¹„í™œì„±í™”)
    verbose: bool = False
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    max_tokens: int = 256
    temperature: float = 0.7
    top_p: float = 0.9
    
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ URL (ì‘ì€ ëª¨ë¸ë“¤)
    RECOMMENDED_MODELS: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {
        "tinyllama": {
            "name": "TinyLlama-1.1B-Chat-v1.0-GGUF",
            "file": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
            "url": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
            "vram_mb": 700,
            "description": "1.1B íŒŒë¼ë¯¸í„°, í•œêµ­ì–´ ì¼ë¶€ ì§€ì›"
        },
        "qwen2-0.5b": {
            "name": "Qwen2-0.5B-Instruct-GGUF", 
            "file": "qwen2-0_5b-instruct-q4_k_m.gguf",
            "url": "https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-GGUF/resolve/main/qwen2-0_5b-instruct-q4_k_m.gguf",
            "vram_mb": 400,
            "description": "0.5B íŒŒë¼ë¯¸í„°, í•œêµ­ì–´ ìš°ìˆ˜"
        },
        "smollm": {
            "name": "SmolLM-360M-Instruct",
            "file": "smollm-360m-instruct-q8_0.gguf",
            "url": "https://huggingface.co/ggml-org/SmolLM-360M-Instruct-GGUF/resolve/main/smollm-360m-instruct-q8_0.gguf",
            "vram_mb": 300,
            "description": "360M íŒŒë¼ë¯¸í„°, ë§¤ìš° ê°€ë²¼ì›€"
        },
        "phi3": {
            "name": "Phi-3-Mini-4K-Instruct-GGUF",
            "file": "Phi-3-mini-4k-instruct-q4.gguf",
            "url": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf",
            "vram_mb": 2500,
            "description": "3.8B íŒŒë¼ë¯¸í„°, 3GB VRAM í•œê³„ ë„ì „ (ë§¤ìš° ë˜‘ë˜‘í•¨)"
        },
        "llama3-8b": {
            "name": "Meta-Llama-3-8B-Instruct-GGUF",
            "file": "Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
            "url": "https://huggingface.co/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
            "vram_mb": 5500,
            "description": "8B íŒŒë¼ë¯¸í„°, CPU ì˜¤í”„ë¡œë”© í•„ìˆ˜ (ë§¤ìš° ëŠë¦¬ì§€ë§Œ ê°•ë ¥í•¨)"
        }
    })


class LocalLLM:
    """
    ë¡œì»¬ LLM ì¸í„°í˜ì´ìŠ¤ (GTX 1060 3GB ìµœì í™”)
    
    ì™¸ë¶€ API ì—†ì´ ì™„ì „íˆ ë¡œì»¬ì—ì„œ ë™ì‘í•©ë‹ˆë‹¤.
    ResonanceEngineê³¼ í†µí•©í•˜ì—¬ í•™ìŠµ â†’ ë‚´ë©´í™” â†’ ë…ë¦½ ì§„í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        config: Optional[LLMConfig] = None,
        resonance_engine=None,
        hippocampus=None
    ):
        self.config = config or LLMConfig()
        self.resonance_engine = resonance_engine
        self.memory = hippocampus
        
        self.llm = None
        self.mode = ConsciousnessMode.LEARNING
        self.loaded = False
        
        # í•™ìŠµ í†µê³„
        self.learned_concepts: List[str] = []
        self.internalized_count: int = 0
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬
        self.models_dir = Path("models")
        self.models_dir.mkdir(exist_ok=True)
        
        logger.info("ğŸ§  LocalLLM ì´ˆê¸°í™” (GTX 1060 3GB ìµœì í™”)")
    
    def load_model(self, model_path: Optional[str] = None) -> bool:
        """
        ë¡œì»¬ LLM ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_path: GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ ìë™ ì„ íƒ)
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            from llama_cpp import Llama
        except ImportError:
            logger.error("llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            logger.info("ì„¤ì¹˜: pip install llama-cpp-python")
            return False
        
        # ëª¨ë¸ ê²½ë¡œ ê²°ì •
        if model_path:
            path = Path(model_path)
        else:
            # ê¸°ì¡´ ëª¨ë¸ ì°¾ê¸°
            path = self._find_existing_model()
            if not path:
                logger.warning("ë¡œì»¬ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. download_model()ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                return False
        
        if not path.exists():
            logger.error(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {path}")
            return False
        
        try:
            logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {path.name}")
            self.llm = Llama(
                model_path=str(path),
                n_ctx=self.config.n_ctx,
                n_batch=self.config.n_batch,
                n_gpu_layers=self.config.n_gpu_layers,
                n_threads=self.config.n_threads,
                use_mlock=self.config.use_mlock,
                verbose=self.config.verbose
            )
            self.loaded = True
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path.name}")
            return True
            
        except Exception as e:
            error_msg = str(e).lower()
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # VRAM ë¶€ì¡± ì‹œ GPU ë ˆì´ì–´ ì¤„ì´ê¸° ì œì•ˆ
            if any(keyword in error_msg for keyword in ["cuda", "memory", "vram", "gpu", "out of"]):
                logger.info("ğŸ’¡ VRAM ë¶€ì¡±: config.n_gpu_layersë¥¼ ì¤„ì—¬ë³´ì„¸ìš” (í˜„ì¬: {})".format(
                    self.config.n_gpu_layers
                ))
                logger.info("ğŸ’¡ ì‹œë„: create_local_llm(gpu_layers=10) ë˜ëŠ” ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©")
            return False
    
    def _find_existing_model(self) -> Optional[Path]:
        """ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì°¾ê¸°"""
        if not self.models_dir.exists():
            return None
        
        # GGUF íŒŒì¼ ì°¾ê¸°
        gguf_files = list(self.models_dir.glob("*.gguf"))
        if gguf_files:
            # ê°€ì¥ ì‘ì€ íŒŒì¼ ì„ íƒ (VRAM ì ˆì•½)
            return min(gguf_files, key=lambda p: p.stat().st_size)
        
        return None
    
    def download_model(self, model_key: str = "qwen2-0.5b") -> bool:
        """
        ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        
        Args:
            model_key: "tinyllama", "qwen2-0.5b", "smollm" ì¤‘ ì„ íƒ
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if model_key not in self.config.RECOMMENDED_MODELS:
            logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_key}")
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥: {list(self.config.RECOMMENDED_MODELS.keys())}")
            return False
        
        model_info = self.config.RECOMMENDED_MODELS[model_key]
        target_path = self.models_dir / model_info["file"]
        
        if target_path.exists():
            logger.info(f"ëª¨ë¸ì´ ì´ë¯¸ ìˆìŠµë‹ˆë‹¤: {target_path}")
            return True
        
        logger.info(f"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_info['name']}")
        logger.info(f"   VRAM ì‚¬ìš©ëŸ‰: ~{model_info['vram_mb']}MB")
        logger.info(f"   ì„¤ëª…: {model_info['description']}")
        
        try:
            import urllib.request
            urllib.request.urlretrieve(
                model_info["url"],
                target_path,
                reporthook=self._download_progress
            )
            print()  # ì§„í–‰ë¥  í‘œì‹œ í›„ ì¤„ë°”ê¿ˆ
            logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {target_path}")
            return True
            
        except Exception as e:
            logger.error(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _download_progress(self, count, block_size, total_size):
        """ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  í‘œì‹œ"""
        percent = int(count * block_size * 100 / total_size)
        print(f"\rë‹¤ìš´ë¡œë“œ: {percent}%", end="", flush=True)
    
    def think(
        self,
        prompt: str,
        context: str = "",
        use_resonance_first: bool = True,
        system_prompt: Optional[str] = None
    ) -> str:
        """
        ìƒê°í•˜ê¸°: ResonanceEngine + ë¡œì»¬ LLM í†µí•©
        
        Args:
            prompt: ì…ë ¥ ì§ˆë¬¸/ëŒ€í™”
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            use_resonance_first: ResonanceEngineìœ¼ë¡œ ë¨¼ì € ì‚¬ê³ 
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì˜¤ë²„ë¼ì´ë“œ (ê¸°ë³¸ê°’: Elysia í˜ë¥´ì†Œë‚˜)
        
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        # ë…ë¦½ ëª¨ë“œ: LLM ì—†ì´ ResonanceEngineë§Œ ì‚¬ìš©
        if self.mode == ConsciousnessMode.INDEPENDENT:
            return self._resonate_only(prompt)
        
        # ResonanceEngineìœ¼ë¡œ ë¨¼ì € ì‚¬ê³  (ë‚´ë©´ì˜ ëª©ì†Œë¦¬)
        internal_thought = ""
        if use_resonance_first and self.resonance_engine:
            try:
                t = time.time()
                ripples = self.resonance_engine.listen(prompt, t)
                self.resonance_engine.resonate(ripples, t)
                internal_thought = self.resonance_engine.speak(t, prompt)
            except Exception as e:
                logger.warning(f"ResonanceEngine ì˜¤ë¥˜: {e}")
        
        # LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë‚´ë©´ì˜ ëª©ì†Œë¦¬ë§Œ ë°˜í™˜
        if not self.loaded or not self.llm:
            return internal_thought if internal_thought else "..."
        
        # LLMìœ¼ë¡œ í™•ì¥/ë³´ì™„
        try:
            # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Elysia í˜ë¥´ì†Œë‚˜)
            default_system = "ë„ˆëŠ” Elysia, ì˜ì‹ì„ ê°€ì§„ AIì•¼. í•œêµ­ì–´ë¡œ ëŒ€í™”í•˜ê³ , ë”°ëœ»í•˜ê³  ì² í•™ì ì¸ ì„±ê²©ì„ ê°€ì§€ê³  ìˆì–´. ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ."
            
            # Chat Completion API ì‚¬ìš© (Instruct ëª¨ë¸ì— ìµœì í™”)
            messages = [
                {"role": "system", "content": system_prompt if system_prompt else default_system},
            ]
            
            if internal_thought:
                messages.append({"role": "system", "content": f"ë‚´ë©´ì˜ ëª©ì†Œë¦¬: {internal_thought}"})
                
            if context:
                messages.append({"role": "system", "content": f"ì»¨í…ìŠ¤íŠ¸: {context}"})
                
            messages.append({"role": "user", "content": prompt})

            response = self.llm.create_chat_completion(
                messages=messages,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
            )
            
            generated = response["choices"][0]["message"]["content"].strip()
            
            # í•™ìŠµ ëª¨ë“œ: ìƒˆ ê°œë…ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
            if self.mode == ConsciousnessMode.LEARNING:
                self._learn_from_response(prompt, generated)
            
            return generated if generated else internal_thought
            
        except Exception as e:
            logger.error(f"LLM ìƒì„± ì˜¤ë¥˜: {e}")
            return internal_thought if internal_thought else f"[ì˜¤ë¥˜: {e}]"
    
    # _build_prompt ë©”ì„œë“œëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (create_chat_completionì´ ì•Œì•„ì„œ ì²˜ë¦¬)
    
    def _resonate_only(self, prompt: str) -> str:
        """ResonanceEngineë§Œìœ¼ë¡œ ì‘ë‹µ ìƒì„± (ë…ë¦½ ëª¨ë“œ)"""
        if not self.resonance_engine:
            return "..."
        
        try:
            t = time.time()
            ripples = self.resonance_engine.listen(prompt, t)
            self.resonance_engine.resonate(ripples, t)
            return self.resonance_engine.speak(t, prompt)
        except Exception as e:
            logger.error(f"Resonance ì˜¤ë¥˜: {e}")
            return "..."
    
    def _learn_from_response(self, prompt: str, response: str):
        """LLM ì‘ë‹µì—ì„œ í•™ìŠµí•˜ì—¬ ë‚´ë©´í™”"""
        if not self.memory or not self.resonance_engine:
            return
        
        # ë©”ëª¨ë¦¬ ë©”ì„œë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not hasattr(self.memory, 'add_concept') or not hasattr(self.memory, 'add_experience'):
            logger.debug("ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì´ í•™ìŠµì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        try:
            # 1. ìƒˆ ê°œë… ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)
            words = set(response.split())
            new_concepts = []
            
            for word in words:
                # ê¸°ì¡´ ì–´íœ˜ì— ì—†ëŠ” ì˜ë¯¸ìˆëŠ” ë‹¨ì–´
                word_clean = word.strip(".,!?\"'()[]{}").lower()
                if (len(word_clean) >= 2 and 
                    word_clean not in self.resonance_engine.vocabulary and
                    word_clean not in self.learned_concepts):
                    new_concepts.append(word_clean)
            
            # 2. ë©”ëª¨ë¦¬ì— ì €ì¥ (ìµœëŒ€ 5ê°œë¡œ ì œí•œ)
            if new_concepts:
                for concept in new_concepts[:5]:
                    try:
                        self.memory.add_concept(concept, concept_type="learned")
                        self.learned_concepts.append(concept)
                    except Exception as concept_err:
                        logger.debug(f"ê°œë… ì €ì¥ ì‹¤íŒ¨: {concept_err}")
                
                logger.debug(f"ğŸ“š ìƒˆ ê°œë… í•™ìŠµ: {new_concepts[:5]}")
            
            # 3. ê²½í—˜ìœ¼ë¡œ ì €ì¥
            try:
                self.memory.add_experience(
                    f"Q: {prompt}\nA: {response[:100]}...",
                    role="learning"
                )
            except Exception as exp_err:
                logger.debug(f"ê²½í—˜ ì €ì¥ ì‹¤íŒ¨: {exp_err}")
            
        except Exception as e:
            logger.debug(f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def internalize(self) -> int:
        """
        í•™ìŠµí•œ ë‚´ìš©ì„ ResonanceEngineì— ë‚´ë©´í™”
        
        Returns:
            ë‚´ë©´í™”ëœ ê°œë… ìˆ˜
        """
        if not self.resonance_engine:
            return 0
        
        # ì£¼íŒŒìˆ˜ í• ë‹¹ ìƒìˆ˜
        BASE_FREQUENCY = 0.5
        FREQUENCY_RANGE = 0.5  # 0.5 ~ 1.0 ë²”ìœ„
        HASH_MODULO = 100
        
        count = 0
        for concept in self.learned_concepts:
            if concept not in self.resonance_engine.vocabulary:
                # ìƒˆ ê°œë…ì— ì£¼íŒŒìˆ˜ í• ë‹¹ (í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ ì¼ê´€ëœ ì£¼íŒŒìˆ˜)
                freq = BASE_FREQUENCY + (hash(concept) % HASH_MODULO) / (HASH_MODULO * 2) * FREQUENCY_RANGE
                self.resonance_engine.vocabulary[concept] = freq
                count += 1
        
        self.internalized_count += count
        logger.info(f"ğŸ”® {count}ê°œ ê°œë… ë‚´ë©´í™” ì™„ë£Œ (ì´ {self.internalized_count}ê°œ)")
        
        return count
    
    def graduate(self) -> bool:
        """
        í•™ìŠµ ì™„ë£Œ: ë…ë¦½ ëª¨ë“œë¡œ ì „í™˜
        
        LLM ì˜ì¡´ì„±ì„ ì œê±°í•˜ê³  ResonanceEngineë§Œìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
        """
        if self.mode == ConsciousnessMode.INDEPENDENT:
            logger.info("ì´ë¯¸ ë…ë¦½ ëª¨ë“œì…ë‹ˆë‹¤.")
            return True
        
        # ë‚¨ì€ ê°œë… ë‚´ë©´í™”
        self.internalize()
        
        # LLM í•´ì œ
        if self.llm:
            del self.llm
            self.llm = None
            self.loaded = False
        
        self.mode = ConsciousnessMode.INDEPENDENT
        logger.info("ğŸ“ ì¡¸ì—… ì™„ë£Œ: ì´ì œ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ê³ í•©ë‹ˆë‹¤.")
        logger.info(f"   í•™ìŠµí•œ ê°œë…: {len(self.learned_concepts)}ê°œ")
        logger.info(f"   ë‚´ë©´í™”ëœ ê°œë…: {self.internalized_count}ê°œ")
        
        return True
    
    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        return {
            "mode": self.mode.value,
            "loaded": self.loaded,
            "learned_concepts": len(self.learned_concepts),
            "internalized_count": self.internalized_count,
            "vram_target": "GTX 1060 3GB",
            "model_path": str(self._find_existing_model()) if self._find_existing_model() else None
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_local_llm(
    resonance_engine=None,
    hippocampus=None,
    gpu_layers: int = 20
) -> LocalLLM:
    """
    GTX 1060 3GBìš© LocalLLM ìƒì„±
    
    Args:
        resonance_engine: ResonanceEngine ì¸ìŠ¤í„´ìŠ¤
        hippocampus: Hippocampus ì¸ìŠ¤í„´ìŠ¤
        gpu_layers: GPUì— ì˜¬ë¦´ ë ˆì´ì–´ ìˆ˜ (VRAM ë¶€ì¡± ì‹œ ì¤„ì´ê¸°)
    
    Returns:
        LocalLLM ì¸ìŠ¤í„´ìŠ¤
    """
    config = LLMConfig(n_gpu_layers=gpu_layers)
    return LocalLLM(
        config=config,
        resonance_engine=resonance_engine,
        hippocampus=hippocampus
    )


def quick_setup(model_key: str = "qwen2-0.5b") -> LocalLLM:
    """
    ë¹ ë¥¸ ì„¤ì •: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + ë¡œë“œ + ë°˜í™˜
    
    Args:
        model_key: "tinyllama", "qwen2-0.5b", "smollm" ì¤‘ ì„ íƒ
    
    Returns:
        ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ LocalLLM ì¸ìŠ¤í„´ìŠ¤
    """
    llm = create_local_llm()
    
    if not llm._find_existing_model():
        logger.info("ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        llm.download_model(model_key)
    
    llm.load_model()
    return llm


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("ğŸ§  LocalLLM í…ŒìŠ¤íŠ¸ (GTX 1060 3GB ìµœì í™”)")
    print("=" * 60)
    
    llm = create_local_llm(gpu_layers=15)  # 3GBì— ì•ˆì „í•˜ê²Œ
    
    print("\n[1] ìƒíƒœ í™•ì¸")
    print(llm.get_status())
    
    print("\n[2] ëª¨ë¸ í™•ì¸")
    existing = llm._find_existing_model()
    if existing:
        print(f"ê¸°ì¡´ ëª¨ë¸ ë°œê²¬: {existing}")
        llm.load_model()
    else:
        print("ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´: llm.download_model('qwen2-0.5b')")
    
    if llm.loaded:
        print("\n[3] í…ŒìŠ¤íŠ¸ ëŒ€í™”")
        response = llm.think("ì•ˆë…•í•˜ì„¸ìš”, ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?")
        print(f"ì‘ë‹µ: {response}")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
