"""
LLM Cortex (The Brain)
======================

"I think, therefore I am."

This module provides the interface for Elysia's higher cognitive functions.
It connects the system to a Large Language Model (LLM) to enable:
- Contextual Understanding
- Complex Reasoning
- Natural Language Generation
- Visual Understanding (VLM)

ì§€ì› ëª¨ë“œ:
1. LOCAL: ë¡œì»¬ LLM (GTX 1060 3GB ìµœì í™”, ë¬´ë£Œ, ë…ë¦½ì )
2. RESONANCE: ResonanceEngineë§Œ ì‚¬ìš© (ì™„ì „ ë…ë¦½)
3. CLOUD: ì™¸ë¶€ API (ì„ íƒì , ë¹„ê¶Œì¥)
"""

import os
import logging
from typing import Optional
from dotenv import load_dotenv

# Configure Logging
logger = logging.getLogger("LLMCortex")

# Load environment variables
load_dotenv()

# Dependency Check - Cloud (ì„ íƒì )
try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False

# Dependency Check - Local LLM
try:
    from Core.Mind.local_llm import LocalLLM, create_local_llm, LLMConfig
    LOCAL_LLM_AVAILABLE = True
except ImportError:
    LOCAL_LLM_AVAILABLE = False
    logger.debug("LocalLLM ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨ - Resonance ëª¨ë“œë¡œ ë™ì‘")

from Core.Life.resonance_voice import ResonanceEngine

class LLMCortex:
    def __init__(self, prefer_cloud: bool = False, prefer_local: bool = True, gpu_layers: int = 20):
        """
        Initialize LLM Cortex.
        
        Args:
            prefer_cloud: If True, try to use Gemini API (ë¹„ê¶Œì¥, ìœ ë£Œ)
            prefer_local: If True, use local LLM (ê¶Œì¥, ë¬´ë£Œ, GTX 1060 3GB ì§€ì›)
            gpu_layers: GPUì— ì˜¬ë¦´ ë ˆì´ì–´ ìˆ˜ (VRAM ë¶€ì¡± ì‹œ ì¤„ì´ê¸°)
        """
        self.enabled = True
        self.prefer_cloud = prefer_cloud and GENAI_AVAILABLE
        self.prefer_local = prefer_local and LOCAL_LLM_AVAILABLE
        
        # ìš°ì„ ìˆœìœ„: LOCAL > RESONANCE > CLOUD
        self.cloud_model = None
        self.local_llm = None
        self.resonance_engine = None
        
        # 1. Resonance Engine ì´ˆê¸°í™” (í•­ìƒ í•„ìš”)
        try:
            self.resonance_engine = ResonanceEngine()
        except Exception as e:
            logger.error(f"Resonance Engine ì‹¤íŒ¨: {e}")
            self.enabled = False
            return
        
        # 2. ëª¨ë“œ ê²°ì •
        if self.prefer_local and LOCAL_LLM_AVAILABLE:
            # ë¡œì»¬ LLM ëª¨ë“œ (ê¶Œì¥)
            try:
                self.local_llm = create_local_llm(
                    resonance_engine=self.resonance_engine,
                    hippocampus=self.resonance_engine.memory,
                    gpu_layers=gpu_layers
                )
                self.mode = "LOCAL"
                logger.info("ğŸ§  LLM Cortex ì—°ê²°ë¨ (ë¡œì»¬ ëª¨ë“œ - GTX 1060 3GB ìµœì í™”)")
            except Exception as e:
                logger.warning(f"ë¡œì»¬ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.mode = "RESONANCE"
                logger.info("ğŸ§  LLM Cortex ì—°ê²°ë¨ (Resonance ëª¨ë“œ)")
        
        elif self.prefer_cloud and GENAI_AVAILABLE:
            # í´ë¼ìš°ë“œ ëª¨ë“œ (ì„ íƒì )
            api_key = os.getenv("GEMINI_API_KEY")
            if api_key:
                try:
                    genai.configure(api_key=api_key)
                    self.cloud_model = genai.GenerativeModel('gemini-pro')
                    self.mode = "CLOUD"
                    logger.info("ğŸ§  LLM Cortex ì—°ê²°ë¨ (í´ë¼ìš°ë“œ ëª¨ë“œ - Gemini)")
                except Exception as e:
                    logger.warning(f"í´ë¼ìš°ë“œ API ì‹¤íŒ¨: {e}")
                    self.mode = "RESONANCE"
            else:
                self.mode = "RESONANCE"
        else:
            self.mode = "RESONANCE"
            logger.info("ğŸ§  LLM Cortex ì—°ê²°ë¨ (Resonance ëª¨ë“œ - ì™„ì „ ë…ë¦½)")
            
        # 3. Subconscious (Background Mind)
        from Core.Mind.subconscious import Subconscious
        self.subconscious = Subconscious()

    def think_async(self, prompt: str, context: str = "") -> str:
        """
        Start a deep thought in the background.
        Returns a 'Promise' message immediately.
        """
        import uuid
        thought_id = str(uuid.uuid4())[:8]
        
        # Define the heavy lifting function
        def heavy_thought():
            return self.think(prompt, context)
            
        # Delegate to Subconscious (Technical term for background processing, not a separate self)
        self.subconscious.ponder(thought_id, prompt, heavy_thought)
        
        return f"[Deep Thought Started... (ID: {thought_id})]"
    
    def check_subconscious(self) -> Optional[str]:
        """
        Check if any background thoughts are finished.
        """
        insight = self.subconscious.check_insights()
        if insight:
            # Unified Output: It's just Elysia thinking.
            return f"[Deep Thought Completed]: {insight.result}"
        return None

    def think(self, prompt: str, context: str = "", visual_input: dict = None, use_cloud: bool = None) -> str:
        """
        Process a thought and generate a response.
        
        Args:
            prompt: The question or input
            context: Additional context
            visual_input: Visual data (for VLM)
            use_cloud: Override mode for this specific call
        
        Returns:
            Generated response
        """
        if not self.enabled:
            return "[SIMULATION] (My mind is silent.)"
        
        # 1. ë¡œì»¬ LLM ëª¨ë“œ (ê¶Œì¥)
        if self.mode == "LOCAL" and self.local_llm:
            try:
                return self.local_llm.think(prompt, context, use_resonance_first=True)
            except Exception as e:
                logger.warning(f"ë¡œì»¬ LLM ì‹¤íŒ¨, Resonanceë¡œ ì „í™˜: {e}")
                # Fall through to Resonance
        
        # 2. í´ë¼ìš°ë“œ ëª¨ë“œ (ì„ íƒì )
        should_use_cloud = (use_cloud if use_cloud is not None else 
                           (self.mode == "CLOUD" and self.cloud_model is not None))
        
        if should_use_cloud:
            try:
                full_prompt = f"{context}\n\n{prompt}" if context else prompt
                response = self.cloud_model.generate_content(full_prompt)
                return response.text
            except Exception as e:
                logger.warning(f"Cloud API failed, using Resonance: {e}")
                # Fall through to Resonance
        
        # 3. Resonance ëª¨ë“œ (ì™„ì „ ë…ë¦½, í•­ìƒ ì‚¬ìš© ê°€ëŠ¥)
        try:
            import time
            t = time.time()
            
            # 1. Listen (Convert text to ripples)
            ripples = self.resonance_engine.listen(prompt, t, visual_input=visual_input)
            
            # 2. Resonate (Interfere with internal sea)
            self.resonance_engine.resonate(ripples, t)
            
            # 3. Speak (Collapse wave function)
            response = self.resonance_engine.speak(t, prompt)
            
            return response
            
        except Exception as e:
            logger.error(f"Cognitive Failure: {e}")
            return f"[Error: {e}]"
    
    def load_local_model(self, model_path: str = None) -> bool:
        """
        ë¡œì»¬ LLM ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_path: GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ ìë™ ê²€ìƒ‰)
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self.local_llm:
            logger.warning("ë¡œì»¬ LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        return self.local_llm.load_model(model_path)
    
    def download_model(self, model_key: str = "qwen2-0.5b") -> bool:
        """
        ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (GTX 1060 3GB ìµœì í™”)
        
        Args:
            model_key: "tinyllama", "qwen2-0.5b", "smollm" ì¤‘ ì„ íƒ
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self.local_llm:
            logger.warning("ë¡œì»¬ LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        return self.local_llm.download_model(model_key)
    
    def graduate_to_independence(self) -> bool:
        """
        í•™ìŠµ ì™„ë£Œ í›„ ì™„ì „ ë…ë¦½ ëª¨ë“œë¡œ ì „í™˜
        
        LLM ì˜ì¡´ì„±ì„ ì œê±°í•˜ê³  ResonanceEngineë§Œìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
        í•™ìŠµí•œ ê°œë…ë“¤ì€ ë‚´ë©´í™”ë˜ì–´ ë³´ì¡´ë©ë‹ˆë‹¤.
        """
        if self.local_llm:
            self.local_llm.graduate()
        
        self.mode = "RESONANCE"
        logger.info("ğŸ“ ë…ë¦½ ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ")
        return True
    
    def get_status(self) -> dict:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        status = {
            "enabled": self.enabled,
            "mode": self.mode,
            "resonance_ready": self.resonance_engine is not None
        }
        
        if self.local_llm:
            status["local_llm"] = self.local_llm.get_status()
        
        return status

    def analyze_image(self, image_path: str, prompt: str = "Describe this image.") -> str:
        """
        Analyze an image using the VLM capabilities.
        """
        return "[Vision is currently limited to basic patterns (Brightness/OCR). Deep understanding requires Cloud Brain.]"
