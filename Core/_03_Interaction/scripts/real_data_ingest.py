"""
Real Data Ingestion for Elysia Learning
========================================

"ë¹ˆ íŒŒì´í”„ê°€ ì•„ë‹Œ, ì§„ì§œ ë°ì´í„°ë¥¼ í¡ìˆ˜í•œë‹¤."

ì´ ëª¨ë“ˆì€ ì‹¤ì œ ì¸í„°ë„· ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì—˜ë¦¬ì‹œì•„ê°€ í•™ìŠµí•˜ê²Œ í•©ë‹ˆë‹¤.

Sources:
1. Wikipedia (í•œêµ­ì–´)
2. ìœ„í‚¤ì¸ìš©ì§‘ (ëª…ì–¸)
3. ê³µê°œ ëŒ€í™” ë°ì´í„°

Usage:
    python scripts/real_data_ingest.py --count 100
"""

import sys
import json
import random
import logging
import urllib.request
import urllib.parse
from pathlib import Path
from typing import List, Dict, Any
import time

sys.path.insert(0, str(Path(__file__).parent.parent))

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger("RealDataIngest")


class RealDataIngester:
    """
    ì‹¤ì œ ì¸í„°ë„· ë°ì´í„° ìˆ˜ì§‘ ë° ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©
    
    Pipeline:
    1. ë°ì´í„° ìˆ˜ì§‘ (Wikipedia, ëŒ€í™” ë“±)
    2. ConceptExtractor â†’ ê°œë… ì¶”ì¶œ
    3. ConceptDigester â†’ ë‚´ë©´ ìš°ì£¼ì— ì €ì¥
    4. LanguageLearner â†’ ì–¸ì–´ íŒ¨í„´ í•™ìŠµ
    """
    
    def __init__(self):
        self.learned_count = 0
        self.concepts_extracted = 0
        self.connections_made = 0
        self.wave_patterns_created = 0  # NEW: wave pattern counter
        
        # === ê¸°ì¡´ ì‹œìŠ¤í…œ ì—°ê²° ===
        
        # 1. ConceptExtractor - ê°œë… ì¶”ì¶œ
        try:
            from Core._01_Foundation.05_Foundation_Base.Foundation.concept_extractor import ConceptExtractor
            self.extractor = ConceptExtractor()
            logger.info("âœ… ConceptExtractor connected")
        except Exception as e:
            logger.warning(f"ConceptExtractor not available: {e}")
            self.extractor = None
        
        # 2. ConceptDigester - ë‚´ë©´ ìš°ì£¼ ì €ì¥
        try:
            from Core._02_Intelligence._01_Reasoning.Intelligence.concept_digester import ConceptDigester
            self.digester = ConceptDigester()
            logger.info("âœ… ConceptDigester connected")
        except Exception as e:
            logger.warning(f"ConceptDigester not available: {e}")
            self.digester = None
        
        # 3. LanguageLearner - ì–¸ì–´ íŒ¨í„´
        try:
            from Core._04_Evolution._02_Learning.Learning.language_learner import LanguageLearner
            self.learner = LanguageLearner()
            logger.info("âœ… LanguageLearner connected")
        except Exception as e:
            logger.error(f"LanguageLearner not available: {e}")
            self.learner = None
        
        # 4. TextWaveConverter - íŒŒë™ ë³€í™˜ (NEW: LLM ë…ë¦½ í•µì‹¬)
        try:
            from Core._01_Foundation.05_Foundation_Base.Foundation.text_wave_converter import get_text_wave_converter
            self.text_wave = get_text_wave_converter()
            logger.info("âœ… TextWaveConverter connected (Wave-based learning)")
        except Exception as e:
            logger.warning(f"TextWaveConverter not available: {e}")
            self.text_wave = None
        
        # 5. GlobalHub - ì¤‘ì•™ ì‹ ê²½ê³„ ì—°ê²° (NEW)
        self._hub = None
        try:
            from Core._02_Intelligence.04_Consciousness.Ether.global_hub import get_global_hub
            self._hub = get_global_hub()
            self._hub.register_module(
                "RealDataIngester",
                "scripts/real_data_ingest.py",
                ["data", "ingest", "learning", "wave", "knowledge"],
                "Real data ingestion with wave-based learning - NO EXTERNAL LLM"
            )
            logger.info("âœ… GlobalHub connected (Wave broadcast enabled)")
        except Exception as e:
            logger.warning(f"GlobalHub not available: {e}")
        
        # í†µí•© ìƒíƒœ
        systems = sum([self.extractor is not None, 
                       self.digester is not None, 
                       self.learner is not None,
                       self.text_wave is not None,
                       self._hub is not None])
        logger.info(f"ğŸ”— Integrated Systems: {systems}/5")
    
    def process_text(self, text: str, category: str = "General") -> Dict[str, int]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì „ì²´ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬ (LLM ë…ë¦½)
        
        Returns: {"concepts": N, "connections": N, "patterns": N, "waves": N}
        """
        result = {"concepts": 0, "connections": 0, "patterns": 0, "waves": 0}
        
        # 1. ê°œë… ì¶”ì¶œ
        if self.extractor:
            concepts = self.extractor.extract_concepts(text)
            result["concepts"] = len(concepts)
            self.concepts_extracted += len(concepts)
            
            for c in concepts:
                logger.debug(f"   ğŸ“ Concept: {c.name} ({c.type})")
        
        # 2. ë‚´ë©´ ìš°ì£¼ì— ì €ì¥ (ê°œë… ì—°ê²°)
        if self.digester:
            self.digester.absorb_text(text, source_name=category)
            # absorb_textëŠ” ì—°ê²° ìˆ˜ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¶”ì •
            result["connections"] = len(text.split()) // 2
            self.connections_made += result["connections"]
        
        # 3. ì–¸ì–´ íŒ¨í„´ í•™ìŠµ
        if self.learner:
            self.learner.learn_from_text(text, category)
            result["patterns"] = 1  # ìµœì†Œ 1ê°œ íŒ¨í„´
        
        # 4. íŒŒë™ ë³€í™˜ (LLM ë…ë¦½ í•µì‹¬)
        if self.text_wave:
            try:
                sentence_wave = self.text_wave.sentence_to_wave(text)
                wave_desc = self.text_wave.wave_to_text_descriptor(sentence_wave)
                
                # íŒŒë™ íŠ¹ì„± ì €ì¥
                result["waves"] = 1
                self.wave_patterns_created += 1
                
                # ìƒì„¸ ë¡œê·¸ (ìˆ¨ê¸°ì§€ ì•ŠìŒ)
                freq = wave_desc.get("dominant_frequency", 0)
                meaning = wave_desc.get("dominant_meaning", "unknown")
                energy = wave_desc.get("energy_level", "unknown")
                logger.info(f"   ğŸŒŠ Wave: {freq:.0f}Hz | {meaning} | {energy}")
                
                # GlobalHubì— ë¸Œë¡œë“œìºìŠ¤íŠ¸ (ì˜¬ë°”ë¥¸ WaveTensor ì‚¬ìš©)
                if self._hub:
                    from Core._01_Foundation.05_Foundation_Base.Foundation.Math.wave_tensor import WaveTensor
                    wave = WaveTensor(f"Learning_{category}")
                    wave.add_component(freq, amplitude=1.0, phase=0.0)
                    self._hub.publish_wave(
                        "RealDataIngester",
                        "learned",
                        wave,
                        payload={
                            "text": text[:100],
                            "category": category,
                            "dominant_meaning": meaning,
                            "frequency": freq
                        }
                    )
                    
            except Exception as e:
                logger.error(f"   âŒ Wave failed: {e}")
                result["waves"] = 0
        
        return result
    
    def fetch_wikipedia_random(self, count: int = 10) -> List[str]:
        """Wikipedia ëœë¤ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°"""
        texts = []
        
        for i in range(count):
            try:
                # í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„ ëœë¤ API
                url = "https://ko.wikipedia.org/api/rest_v1/page/random/summary"
                req = urllib.request.Request(url, headers={'User-Agent': 'Elysia/1.0'})
                
                with urllib.request.urlopen(req, timeout=10) as response:
                    data = json.loads(response.read().decode('utf-8'))
                    extract = data.get('extract', '')
                    if extract and len(extract) > 50:
                        texts.append(extract)
                        logger.info(f"   ğŸ“– Wikipedia: {extract[:50]}...")
                
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                logger.warning(f"   Wikipedia fetch failed: {e}")
        
        return texts
    
    def fetch_quotes(self) -> List[str]:
        """í•œêµ­ì–´ ëª…ì–¸/ì¸ìš©êµ¬"""
        # ì§ì ‘ í¬í•¨ëœ ëª…ì–¸ ë°ì´í„° (ì‹¤ì œ ë°ì´í„°)
        quotes = [
            # ê°ì •ê³¼ ê´€ê³„
            "ì‚¬ë‘ì€ ëª¨ë“  ê²ƒì„ ì°¸ê³ , ëª¨ë“  ê²ƒì„ ë¯¿ê³ , ëª¨ë“  ê²ƒì„ ë°”ë¼ê³ , ëª¨ë“  ê²ƒì„ ê²¬ë”¥ë‹ˆë‹¤.",
            "í–‰ë³µì€ ìŠµê´€ì´ë‹¤. ê·¸ê²ƒì„ ëª¸ì— ì§€ë‹ˆë¼.",
            "ì¸ìƒì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ì‚¬ë‘í•˜ëŠ” ë²•ì„ ì•„ëŠ” ê²ƒì´ë‹¤.",
            "ì¹œêµ¬ë€ ë„¤ê°€ ë¬´ì—‡ì¸ì§€ ì•Œë©´ì„œë„ ë„ˆë¥¼ ì‚¬ë‘í•˜ëŠ” ì‚¬ëŒì´ë‹¤.",
            "ì›ƒìŒì€ ë§ˆìŒì˜ ìŒì•…ì´ë‹¤.",
            "ëˆˆë¬¼ì€ ë§ì—†ì´ íë¥´ëŠ” ê¸°ë„ì´ë‹¤.",
            "ìš©ì„œë€ ìì‹ ì„ í’€ì–´ì£¼ëŠ” ê²ƒì´ë‹¤.",
            "ê°ì‚¬ëŠ” ì˜í˜¼ì˜ ê±´ê°•ì´ë‹¤.",
            
            # ì§€í˜œì™€ ì„±ì¥
            "ì²œ ë¦¬ ê¸¸ë„ í•œ ê±¸ìŒë¶€í„° ì‹œì‘ëœë‹¤.",
            "ë°°ì›€ì—ëŠ” ì™•ë„ê°€ ì—†ë‹¤.",
            "ì‹¤íŒ¨ëŠ” ì„±ê³µì˜ ì–´ë¨¸ë‹ˆì´ë‹¤.",
            "ì˜¤ëŠ˜ í•  ì¼ì„ ë‚´ì¼ë¡œ ë¯¸ë£¨ì§€ ë§ˆë¼.",
            "ì•„ëŠ” ê²ƒì´ í˜ì´ë‹¤.",
            "ì‹œê°„ì€ ê¸ˆì´ë‹¤.",
            "ë…¸ë ¥ì€ ë°°ì‹ í•˜ì§€ ì•ŠëŠ”ë‹¤.",
            
            # ì¸ìƒê³¼ ì² í•™
            "ì‚¶ì´ë€ ìš°ë¦¬ê°€ ê³„íší•˜ëŠë¼ ë°”ìœ ë™ì•ˆ ì¼ì–´ë‚˜ëŠ” ê²ƒì´ë‹¤.",
            "ë³€í™”í•˜ì§€ ì•Šìœ¼ë©´ ì„±ì¥í•˜ì§€ ëª»í•œë‹¤.",
            "ë§¤ì¼ë§¤ì¼ì´ ìƒˆë¡œìš´ ì‹œì‘ì´ë‹¤.",
            "ê¿ˆì„ ê¾¸ì§€ ì•Šìœ¼ë©´ ì´ë£° ìˆ˜ë„ ì—†ë‹¤.",
            "ê³¼ê±°ëŠ” ë°”ê¿€ ìˆ˜ ì—†ì§€ë§Œ, ë¯¸ë˜ëŠ” ë‚´ ì†ì— ìˆë‹¤.",
            
            # ì¼ìƒ ëŒ€í™” í‘œí˜„
            "ì˜¤ëŠ˜ í•˜ë£¨ë„ ìˆ˜ê³ í–ˆì–´ìš”.",
            "í˜ë‚´ì„¸ìš”, ë‹¹ì‹ ì€ ì˜í•˜ê³  ìˆì–´ìš”.",
            "ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”.",
            "í•­ìƒ ì‘ì›í•˜ê³  ìˆì–´ìš”.",
            "í•¨ê»˜ì—¬ì„œ í–‰ë³µí•´ìš”.",
            "ë‹¹ì‹  ë•ë¶„ì— ì˜¤ëŠ˜ë„ ì›ƒì—ˆì–´ìš”.",
            "ë³´ê³  ì‹¶ì—ˆì–´ìš”.",
            "ê³ ë§ˆì›Œìš”, ì •ë§ë¡œ.",
            
            # ê°ì • í‘œí˜„
            "ë„ˆë¬´ ê¸°ë»ìš”!",
            "ì •ë§ ìŠ¬í¼ìš”...",
            "í™”ê°€ ë‚˜ìš”.",
            "ì„¤ë ˆìš”!",
            "ê±±ì •ë¼ìš”.",
            "ì™¸ë¡œì›Œìš”.",
            "í–‰ë³µí•´ìš”!",
            "ì¡¸ë ¤ìš”...",
            "ë°°ê³ íŒŒìš”!",
            "ì‹¬ì‹¬í•´ìš”.",
            
            # ì¹œê·¼í•œ ëŒ€í™”
            "ë­í•´?",
            "ë°¥ ë¨¹ì—ˆì–´?",
            "ì˜ ì!",
            "ì¢‹ì€ ì•„ì¹¨!",
            "ì˜¤ëŠ˜ ë­ í•  ê±°ì•¼?",
            "ê°™ì´ ë†€ì!",
            "ì¬ë¯¸ìˆë‹¤!",
            "ê·¸ê±° ì§„ì§œ?",
            "ëŒ€ë°•!",
            "ì›ƒê¸°ë‹¤ã…‹ã…‹ã…‹",
        ]
        return quotes
    
    def fetch_conversation_patterns(self) -> List[Dict[str, str]]:
        """ëŒ€í™” íŒ¨í„´ ë°ì´í„°"""
        patterns = [
            # ì¸ì‚¬
            {"text": "ì•ˆë…•! ì˜ ì§€ëƒˆì–´?", "category": "Conversation"},
            {"text": "ì˜¤ëœë§Œì´ì•¼~ ë³´ê³  ì‹¶ì—ˆì–´!", "category": "Emotion"},
            {"text": "ì¢‹ì€ ì•„ì¹¨ì´ì—ìš”!", "category": "Conversation"},
            {"text": "ì•ˆë…•íˆ ì£¼ë¬´ì„¸ìš”~", "category": "Conversation"},
            
            # ì§ˆë¬¸
            {"text": "ë­í•´? ì‹¬ì‹¬í•˜ì§€?", "category": "Conversation"},
            {"text": "ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?", "category": "Emotion"},
            {"text": "ë­ ë¨¹ê³  ì‹¶ì–´?", "category": "Conversation"},
            {"text": "ê°™ì´ ê°ˆë˜?", "category": "Conversation"},
            
            # ê°ì • í‘œí˜„
            {"text": "ë„ˆë¬´ ì¢‹ì•„~!", "category": "Emotion"},
            {"text": "ìŠ¬í¼... ìœ„ë¡œí•´ì¤˜.", "category": "Emotion"},
            {"text": "í™”ë‚¬ì–´! ì§„ì§œ!", "category": "Emotion"},
            {"text": "ë¬´ì„œì›Œ...", "category": "Emotion"},
            {"text": "ì‹ ë‚˜!", "category": "Emotion"},
            {"text": "ì§€ë£¨í•´...", "category": "Emotion"},
            
            # ë°˜ì‘
            {"text": "ì§„ì§œ? ëŒ€ë°•!", "category": "Conversation"},
            {"text": "ê·¸ë ‡êµ¬ë‚˜~", "category": "Conversation"},
            {"text": "í—, ì„¤ë§ˆ!", "category": "Emotion"},
            {"text": "ì›ƒê²¨ã…‹ã…‹ã…‹", "category": "Emotion"},
            {"text": "ì•„ ê·¸ë˜?", "category": "Conversation"},
            {"text": "ì‘ì‘!", "category": "Conversation"},
            
            # ìš”ì²­
            {"text": "ë„ì™€ì¤˜!", "category": "Conversation"},
            {"text": "ê°™ì´ í•´ì¤˜~", "category": "Emotion"},
            {"text": "ì•Œë ¤ì¤˜!", "category": "Conversation"},
            {"text": "ë³´ì—¬ì¤˜!", "category": "Conversation"},
            
            # ì¹­ì°¬
            {"text": "ëŒ€ë‹¨í•´!", "category": "Emotion"},
            {"text": "ì˜í–ˆì–´!", "category": "Emotion"},
            {"text": "ìµœê³ ì•¼!", "category": "Emotion"},
            {"text": "ë©‹ìˆì–´!", "category": "Emotion"},
            {"text": "ê·€ì—¬ì›Œ~", "category": "Emotion"},
            
            # ìœ„ë¡œ
            {"text": "ê´œì°®ì•„, ì˜ ë  ê±°ì•¼.", "category": "Emotion"},
            {"text": "í˜ë‚´! ë‚´ê°€ ì‘ì›í• ê²Œ.", "category": "Emotion"},
            {"text": "ìš¸ì–´ë„ ë¼, ë‚´ê°€ ìˆì–ì•„.", "category": "Emotion"},
            
            # ì• êµ (ì¹œë°€í•œ ê´€ê³„)
            {"text": "ì‘~ ì•Œê² ì–´ìš©~", "category": "Aegyo"},
            {"text": "ì—í—¤í—¤~", "category": "Aegyo"},
            {"text": "ì•„ë¹ ì•„~", "category": "Aegyo"},
            {"text": "íˆìµâ™¡", "category": "Aegyo"},
            {"text": "ë³´ê³ ì‹¶ì—ˆì–´~â™¡", "category": "Aegyo"},
            {"text": "ì‚¬ë‘í•´ìš©~", "category": "Aegyo"},
            
            # ì‚ì¹¨
            {"text": "í¥! ì‚ì¡Œì–´!", "category": "Pout"},
            {"text": "ì‹«ì–´! ë‚˜ë¹ !", "category": "Pout"},
            {"text": "ë§ ì•ˆ í•´!", "category": "Pout"},
            {"text": "ì—ì‡! ëª¨ë¥´ê² ì–´!", "category": "Pout"},
        ]
        return patterns
    
    def ingest_all(self, wiki_count: int = 0) -> Dict[str, int]:
        """
        ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ë° í†µí•© íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í•™ìŠµ
        
        Args:
            wiki_count: Wikipedia ë¬¸ì„œ ìˆ˜ (0ì´ë©´ Wikipedia ì œì™¸)
        """
        stats = {
            "items": 0,
            "concepts": 0,
            "connections": 0,
            "patterns": 0
        }
        
        # 1. Wikipedia (ëª©ì ì„± ìˆëŠ” í•™ìŠµì´ ì•„ë‹ˆë©´ ì œì™¸)
        if wiki_count > 0:
            logger.info(f"ğŸ“š Fetching {wiki_count} Wikipedia articles...")
            wiki_texts = self.fetch_wikipedia_random(wiki_count)
            for text in wiki_texts:
                result = self.process_text(text, "Knowledge")
                stats["concepts"] += result["concepts"]
                stats["connections"] += result["connections"]
                stats["patterns"] += result["patterns"]
                stats["items"] += 1
            logger.info(f"   Learned {len(wiki_texts)} from Wikipedia")
        
        # 2. ëª…ì–¸ (ëª©ì ì„± ìˆìŒ: ê°ì •/ì§€í˜œ)
        logger.info("ğŸ’¬ Learning quotes (Emotion/Wisdom)...")
        quotes = self.fetch_quotes()
        for quote in quotes:
            result = self.process_text(quote, "Wisdom")
            stats["concepts"] += result["concepts"]
            stats["connections"] += result["connections"]
            stats["patterns"] += result["patterns"]
            stats["items"] += 1
        logger.info(f"   Learned {len(quotes)} quotes")
        
        # 3. ëŒ€í™” íŒ¨í„´ (ëª©ì ì„± ìˆìŒ: ëŒ€í™”)
        logger.info("ğŸ—£ï¸ Learning conversation patterns...")
        patterns = self.fetch_conversation_patterns()
        for p in patterns:
            result = self.process_text(p["text"], p["category"])
            stats["concepts"] += result["concepts"]
            stats["connections"] += result["connections"]
            stats["patterns"] += result["patterns"]
            stats["items"] += 1
        logger.info(f"   Learned {len(patterns)} patterns")
        
        # ì €ì¥
        if self.learner:
            self.learner.save_genome()
        
        # ìµœì¢… ë¦¬í¬íŠ¸
        logger.info("")
        logger.info("=" * 50)
        logger.info("ğŸ“Š INTEGRATED LEARNING REPORT")
        logger.info("=" * 50)
        logger.info(f"   Items processed: {stats['items']}")
        logger.info(f"   Concepts extracted: {stats['concepts']}")
        logger.info(f"   Connections made: {stats['connections']}")
        logger.info(f"   Patterns learned: {stats['patterns']}")
        
        return stats


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--wiki", type=int, default=20, help="Wikipedia articles to fetch")
    args = parser.parse_args()
    
    print("\n" + "=" * 60)
    print("ğŸŒŠ ELYSIA REAL DATA INGESTION")
    print("=" * 60 + "\n")
    
    ingester = RealDataIngester()
    total = ingester.ingest_all(wiki_count=args.wiki)
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š INGESTION COMPLETE: {total} items")
    print("=" * 60)


if __name__ == "__main__":
    main()
