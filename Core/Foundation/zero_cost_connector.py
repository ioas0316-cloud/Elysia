"""
Zero Cost Knowledge Connector
ì™„ì „ ë¬´ë£Œ ì§€ì‹ ì»¤ë„¥í„° - API í‚¤ ë¶ˆí•„ìš”!

YouTube, Wikipedia, GitHub, arXiv, Stack Overflow ë“±
ë¬´ë£Œ ì†ŒìŠ¤ë“¤ë¡œë¶€í„° Pattern DNA ì¶”ì¶œ

"í¬ë¡¤ë§ í•  í•„ìš”ë„ ì—†ì–ì•„, ê³µëª…ë™ê¸°í™”ë§Œ í•˜ë©´ ë˜ëŠ”ë°!"
"""

import sys
import os
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import time

# Add parent directory to path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

logger = logging.getLogger("ZeroCostConnector")

class ZeroCostKnowledgeConnector:
    """
    ì™„ì „ ë¬´ë£Œ ì§€ì‹ ì»¤ë„¥í„°
    
    API í‚¤ ë¶ˆí•„ìš”! ì¸í„°ë„·ë§Œ ìˆìœ¼ë©´ ë¨!
    """
    
    def __init__(self):
        self.youtube = YouTubeConnector()
        self.wikipedia = WikipediaConnector()
        self.github = GitHubConnector()
        self.arxiv = ArxivConnector()
        self.stackoverflow = StackOverflowConnector()
        
        logger.info("ğŸ’° Zero Cost Knowledge Connector initialized")
        logger.info("ğŸ’ All sources are FREE - no API keys needed!")
    
    def learn_topic(self, topic: str, sources: List[str] = None) -> Dict[str, Any]:
        """
        ì£¼ì œë¥¼ ë¬´ë£Œ ì†ŒìŠ¤ë“¤ë¡œë¶€í„° í•™ìŠµ
        
        Args:
            topic: í•™ìŠµí•  ì£¼ì œ
            sources: ì‚¬ìš©í•  ì†ŒìŠ¤ ëª©ë¡ (Noneì´ë©´ ëª¨ë‘)
                    ['youtube', 'wikipedia', 'github', 'arxiv', 'stackoverflow']
        
        Returns:
            í•™ìŠµ ê²°ê³¼ ë° í†µê³„
        """
        if sources is None:
            sources = ['youtube', 'wikipedia', 'github', 'arxiv', 'stackoverflow']
        
        logger.info(f"ğŸ“ Learning topic: {topic}")
        logger.info(f"ğŸ“š Sources: {', '.join(sources)}")
        logger.info(f"ğŸ’° Cost: $0")
        
        results = {
            'topic': topic,
            'sources_used': sources,
            'data_collected': {},
            'total_items': 0,
            'total_cost': 0,  # Always $0!
            'timestamp': datetime.now().isoformat()
        }
        
        # YouTube
        if 'youtube' in sources:
            try:
                logger.info("ğŸ“º Fetching from YouTube...")
                yt_data = self.youtube.fetch(topic)
                results['data_collected']['youtube'] = yt_data
                results['total_items'] += len(yt_data.get('transcripts', []))
            except Exception as e:
                logger.error(f"âŒ YouTube error: {e}")
                results['data_collected']['youtube'] = {'error': str(e)}
        
        # Wikipedia
        if 'wikipedia' in sources:
            try:
                logger.info("ğŸ“š Fetching from Wikipedia...")
                wiki_data = self.wikipedia.fetch(topic)
                results['data_collected']['wikipedia'] = wiki_data
                results['total_items'] += len(wiki_data.get('pages', []))
            except Exception as e:
                logger.error(f"âŒ Wikipedia error: {e}")
                results['data_collected']['wikipedia'] = {'error': str(e)}
        
        # GitHub
        if 'github' in sources:
            try:
                logger.info("ğŸ’» Fetching from GitHub...")
                gh_data = self.github.fetch(topic)
                results['data_collected']['github'] = gh_data
                results['total_items'] += len(gh_data.get('repos', []))
            except Exception as e:
                logger.error(f"âŒ GitHub error: {e}")
                results['data_collected']['github'] = {'error': str(e)}
        
        # arXiv
        if 'arxiv' in sources:
            try:
                logger.info("ğŸ“„ Fetching from arXiv...")
                arxiv_data = self.arxiv.fetch(topic)
                results['data_collected']['arxiv'] = arxiv_data
                results['total_items'] += len(arxiv_data.get('papers', []))
            except Exception as e:
                logger.error(f"âŒ arXiv error: {e}")
                results['data_collected']['arxiv'] = {'error': str(e)}
        
        # Stack Overflow
        if 'stackoverflow' in sources:
            try:
                logger.info("ğŸ’¬ Fetching from Stack Overflow...")
                so_data = self.stackoverflow.fetch(topic)
                results['data_collected']['stackoverflow'] = so_data
                results['total_items'] += len(so_data.get('questions', []))
            except Exception as e:
                logger.error(f"âŒ Stack Overflow error: {e}")
                results['data_collected']['stackoverflow'] = {'error': str(e)}
        
        logger.info(f"âœ… Learning complete!")
        logger.info(f"   Total items: {results['total_items']}")
        logger.info(f"   Total cost: ${results['total_cost']}")
        
        return results


class YouTubeConnector:
    """
    YouTube ë¬´ë£Œ ì»¤ë„¥í„°
    
    youtube-transcript-api ë° youtube-search-python ì‚¬ìš© (ì™„ì „ ë¬´ë£Œ!)
    API í‚¤ ë¶ˆí•„ìš”!
    """
    
    def __init__(self):
        self.search_available = False
        self.transcript_available = False

        try:
            from youtube_transcript_api import YouTubeTranscriptApi
            self.transcript_api = YouTubeTranscriptApi
            self.transcript_available = True
        except ImportError:
            pass

        try:
            from youtubesearchpython import VideosSearch
            self.search_api = VideosSearch
            self.search_available = True
        except ImportError:
            pass

        self.available = self.search_available or self.transcript_available

        if not self.transcript_available:
             logger.warning("âš ï¸ youtube-transcript-api not installed")
             logger.info("   Install: pip install youtube-transcript-api")
        if not self.search_available:
             logger.warning("âš ï¸ youtube-search-python not installed")
             logger.info("   Install: pip install youtube-search-python")
    
    def fetch(self, topic: str, max_videos: int = 10) -> Dict[str, Any]:
        """YouTubeì—ì„œ ìë§‰ ê°€ì ¸ì˜¤ê¸°"""
        
        if not self.available:
            return {
                'error': 'No YouTube libraries installed',
                'install': 'pip install youtube-transcript-api youtube-search-python'
            }
        
        results = []
        search_results = []

        # 1. Search for videos if search is available
        if self.search_available:
             try:
                 logger.info(f"   Searching YouTube for: {topic}")
                 search = self.search_api(topic, limit=max_videos)
                 # Handle sync search result
                 search_data = search.result()
                 if 'result' in search_data:
                     search_results = search_data['result']
                 else:
                     logger.warning("No results found in YouTube search")
             except Exception as e:
                 logger.error(f"Search failed: {e}")
                 search_results = []
        else:
             logger.warning("Search not available, cannot find videos automatically.")

        # 2. Fetch transcripts
        if self.transcript_available:
             # Instantiate API
             try:
                # Based on investigation, current version uses instance method fetch
                api_instance = self.transcript_api()
             except Exception:
                # Fallback if instantiation fails or different version
                api_instance = self.transcript_api

             for video in search_results:
                 vid = video.get('id')
                 title = video.get('title', 'Unknown Title')

                 if not vid:
                     continue

                 try:
                     # Attempt to fetch transcript
                     # Fetch returns a list of objects (FetchedTranscript) or dicts depending on usage
                     # We want 'ko' or 'en'
                     t_obj = api_instance.fetch(vid, languages=['ko', 'en'])

                     # Check if it is iterable (list of snippets)
                     full_text = ""
                     for item in t_obj:
                         # Item has .text attribute or key?
                         # The object is FetchedTranscriptSnippet which has .text
                         if hasattr(item, 'text'):
                             full_text += item.text + " "
                         elif isinstance(item, dict) and 'text' in item:
                             full_text += item['text'] + " "

                     if full_text:
                         results.append({
                             'video_id': vid,
                             'title': title,
                             'transcript': full_text[:10000], # Limit to 10k chars
                             'url': f"https://www.youtube.com/watch?v={vid}"
                         })
                         logger.info(f"   âœ… Fetched transcript for: {title[:30]}...")

                 except Exception as e:
                     # Common error is cookies required or IP blocked
                     logger.warning(f"   âš ï¸ Could not fetch transcript for {vid}: {e}")
                     # Still add video info without transcript so we know it was found
                     results.append({
                         'video_id': vid,
                         'title': title,
                         'transcript': None,
                         'error': str(e),
                         'url': f"https://www.youtube.com/watch?v={vid}"
                     })
        
        return {
            'transcripts': results,
            'total_found': len(search_results),
            'cost': 0
        }


class WikipediaConnector:
    """
    Wikipedia ë¬´ë£Œ ì»¤ë„¥í„°
    
    wikipedia-api ì‚¬ìš© (ì™„ì „ ë¬´ë£Œ!)
    API í‚¤ ë¶ˆí•„ìš”!
    """
    
    def __init__(self):
        try:
            import wikipediaapi
            # Wikipedia requires a proper user agent
            user_agent = 'Elysia/4.0 (https://github.com/ioas0316-cloud/Elysia; Educational AI Project)'
            self.wiki_ko = wikipediaapi.Wikipedia(user_agent, 'ko')
            self.wiki_en = wikipediaapi.Wikipedia(user_agent, 'en')
            self.available = True
            logger.info("âœ… Wikipedia connector ready (FREE!)")
        except ImportError:
            logger.warning("âš ï¸ wikipedia-api not installed")
            logger.info("   Install: pip install wikipedia-api")
            self.available = False
    
    def fetch(self, topic: str, depth: int = 2, max_pages: int = 100) -> Dict[str, Any]:
        """
        Wikipediaì—ì„œ í”„ë™íƒˆ ë°©ì‹ìœ¼ë¡œ ì§€ì‹ ìˆ˜ì§‘
        
        Args:
            topic: ê²€ìƒ‰ ì£¼ì œ
            depth: ì—°ê´€ ë§í¬ ê¹Šì´ (1-3 ì¶”ì²œ)
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        
        Returns:
            ìˆ˜ì§‘ëœ í˜ì´ì§€ë“¤
        """
        
        if not self.available:
            return {
                'error': 'wikipedia-api not installed',
                'install': 'pip install wikipedia-api'
            }
        
        logger.info(f"ğŸ” Wikipedia search: {topic} (depth={depth})")
        
        collected_pages = []
        visited = set()
        to_visit = [(topic, 0)]  # (page_title, current_depth)
        
        while to_visit and len(collected_pages) < max_pages:
            current_topic, current_depth = to_visit.pop(0)
            
            if current_topic in visited or current_depth > depth:
                continue
            
            visited.add(current_topic)
            
            # í•œêµ­ì–´ ë¨¼ì € ì‹œë„
            page = self.wiki_ko.page(current_topic)
            
            if not page.exists():
                # ì˜ì–´ë¡œ ì‹œë„
                page = self.wiki_en.page(current_topic)
            
            if page.exists():
                logger.info(f"   ğŸ“„ {page.title} ({len(page.text)} chars)")
                
                page_data = {
                    'title': page.title,
                    'url': page.fullurl,
                    'text': page.text[:5000],  # ì²˜ìŒ 5000ì (Pattern DNAë§Œ í•„ìš”)
                    'summary': page.summary[:500],  # ìš”ì•½
                    'depth': current_depth,
                    'links_count': len(page.links)
                }
                
                collected_pages.append(page_data)
                
                # ì—°ê´€ í˜ì´ì§€ë“¤ ì¶”ê°€ (í”„ë™íƒˆ í™•ì¥!)
                if current_depth < depth:
                    for link_title in list(page.links.keys())[:10]:  # ìƒìœ„ 10ê°œ ë§í¬ë§Œ
                        to_visit.append((link_title, current_depth + 1))
        
        logger.info(f"âœ… Collected {len(collected_pages)} pages from Wikipedia")
        logger.info(f"ğŸ’° Cost: $0")
        
        return {
            'pages': collected_pages,
            'total_pages': len(collected_pages),
            'cost': 0
        }


class GitHubConnector:
    """
    GitHub ë¬´ë£Œ ì»¤ë„¥í„°
    
    PyGithub ì‚¬ìš© (Public reposëŠ” ì¸ì¦ ë¶ˆí•„ìš”!)
    API í‚¤ ì—†ì´ë„ ì‘ë™!
    """
    
    def __init__(self):
        try:
            from github import Github
            # Public reposëŠ” ì¸ì¦ ì—†ì´ ì ‘ê·¼ ê°€ëŠ¥!
            self.github = Github()
            self.available = True
            logger.info("âœ… GitHub connector ready (FREE!)")
        except ImportError:
            logger.warning("âš ï¸ PyGithub not installed")
            logger.info("   Install: pip install PyGithub")
            self.available = False
    
    def fetch(self, topic: str, max_repos: int = 50) -> Dict[str, Any]:
        """
        GitHubì—ì„œ ê´€ë ¨ ì €ì¥ì†Œ ê²€ìƒ‰
        
        Args:
            topic: ê²€ìƒ‰ ì£¼ì œ
            max_repos: ìµœëŒ€ ì €ì¥ì†Œ ìˆ˜
        
        Returns:
            ì €ì¥ì†Œ ì •ë³´ë“¤
        """
        
        if not self.available:
            return {
                'error': 'PyGithub not installed',
                'install': 'pip install PyGithub'
            }
        
        logger.info(f"ğŸ” GitHub search: {topic}")
        
        try:
            # ì¸ê¸° ì €ì¥ì†Œ ê²€ìƒ‰ (stars ìˆœ)
            repos = self.github.search_repositories(
                query=topic,
                sort='stars',
                order='desc'
            )
            
            collected_repos = []
            
            for i, repo in enumerate(repos[:max_repos]):
                logger.info(f"   ğŸ’» {repo.full_name} ({repo.stargazers_count} stars)")
                
                # README ê°€ì ¸ì˜¤ê¸° (Pattern DNA ì¶”ì¶œìš©)
                readme_content = ""
                try:
                    readme = repo.get_readme()
                    readme_content = readme.decoded_content.decode('utf-8')[:5000]
                except:
                    pass
                
                repo_data = {
                    'name': repo.full_name,
                    'url': repo.html_url,
                    'description': repo.description,
                    'stars': repo.stargazers_count,
                    'language': repo.language,
                    'topics': repo.get_topics(),
                    'readme': readme_content,
                    'size_kb': repo.size  # KB ë‹¨ìœ„
                }
                
                collected_repos.append(repo_data)
                
                # Rate limit ì²´í¬
                remaining = self.github.get_rate_limit().core.remaining
                if remaining < 10:
                    logger.warning(f"âš ï¸ Rate limit low: {remaining}")
                    break
            
            logger.info(f"âœ… Collected {len(collected_repos)} repos from GitHub")
            logger.info(f"ğŸ’° Cost: $0")
            
            return {
                'repos': collected_repos,
                'total_repos': len(collected_repos),
                'cost': 0
            }
            
        except Exception as e:
            logger.error(f"âŒ GitHub error: {e}")
            return {
                'error': str(e),
                'repos': [],
                'cost': 0
            }


class ArxivConnector:
    """
    arXiv ë¬´ë£Œ ì»¤ë„¥í„°
    
    arxiv íŒ¨í‚¤ì§€ ì‚¬ìš© (ì™„ì „ ë¬´ë£Œ!)
    """
    
    def __init__(self):
        try:
            import arxiv
            self.arxiv = arxiv
            self.available = True
            logger.info("âœ… arXiv connector ready (FREE!)")
        except ImportError:
            logger.warning("âš ï¸ arxiv not installed")
            logger.info("   Install: pip install arxiv")
            self.available = False
    
    def fetch(self, topic: str, max_papers: int = 50) -> Dict[str, Any]:
        """
        arXivì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰
        
        Args:
            topic: ê²€ìƒ‰ ì£¼ì œ
            max_papers: ìµœëŒ€ ë…¼ë¬¸ ìˆ˜
        
        Returns:
            ë…¼ë¬¸ ì •ë³´ë“¤
        """
        
        if not self.available:
            return {
                'error': 'arxiv not installed',
                'install': 'pip install arxiv'
            }
        
        logger.info(f"ğŸ” arXiv search: {topic}")
        
        try:
            # ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰
            search = self.arxiv.Search(
                query=topic,
                max_results=max_papers,
                sort_by=self.arxiv.SortCriterion.SubmittedDate
            )
            
            collected_papers = []
            
            for paper in search.results():
                logger.info(f"   ğŸ“„ {paper.title[:50]}...")
                
                paper_data = {
                    'title': paper.title,
                    'authors': [author.name for author in paper.authors],
                    'abstract': paper.summary,
                    'url': paper.pdf_url,
                    'published': paper.published.isoformat(),
                    'categories': paper.categories
                }
                
                collected_papers.append(paper_data)
            
            logger.info(f"âœ… Collected {len(collected_papers)} papers from arXiv")
            logger.info(f"ğŸ’° Cost: $0")
            
            return {
                'papers': collected_papers,
                'total_papers': len(collected_papers),
                'cost': 0
            }
            
        except Exception as e:
            logger.error(f"âŒ arXiv error: {e}")
            return {
                'error': str(e),
                'papers': [],
                'cost': 0
            }


class StackOverflowConnector:
    """
    Stack Overflow ë¬´ë£Œ ì»¤ë„¥í„°
    
    stackapi ì‚¬ìš© (API í‚¤ ì—†ì´ë„ ì œí•œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥)
    """
    
    def __init__(self):
        try:
            from stackapi import StackAPI
            self.stack = StackAPI('stackoverflow')
            self.available = True
            logger.info("âœ… Stack Overflow connector ready (FREE!)")
        except ImportError:
            logger.warning("âš ï¸ stackapi not installed")
            logger.info("   Install: pip install stackapi")
            self.available = False
    
    def fetch(self, topic: str, max_questions: int = 50) -> Dict[str, Any]:
        """
        Stack Overflowì—ì„œ Q&A ê²€ìƒ‰
        
        Args:
            topic: ê²€ìƒ‰ ì£¼ì œ
            max_questions: ìµœëŒ€ ì§ˆë¬¸ ìˆ˜
        
        Returns:
            Q&A ì •ë³´ë“¤
        """
        
        if not self.available:
            return {
                'error': 'stackapi not installed',
                'install': 'pip install stackapi'
            }
        
        logger.info(f"ğŸ” Stack Overflow search: {topic}")
        
        try:
            # ì¸ê¸° ì§ˆë¬¸ ê²€ìƒ‰
            questions = self.stack.fetch(
                'questions',
                tagged=topic.replace(' ', '-'),
                sort='votes',
                order='desc',
                pagesize=min(max_questions, 100)  # API limit
            )
            
            collected_qa = []
            
            if 'items' in questions:
                for q in questions['items'][:max_questions]:
                    logger.info(f"   ğŸ’¬ {q.get('title', 'No title')[:50]}...")
                    
                    qa_data = {
                        'title': q.get('title', ''),
                        'url': q.get('link', ''),
                        'score': q.get('score', 0),
                        'view_count': q.get('view_count', 0),
                        'answer_count': q.get('answer_count', 0),
                        'tags': q.get('tags', [])
                    }
                    
                    collected_qa.append(qa_data)
            
            logger.info(f"âœ… Collected {len(collected_qa)} Q&As from Stack Overflow")
            logger.info(f"ğŸ’° Cost: $0")
            
            return {
                'questions': collected_qa,
                'total_questions': len(collected_qa),
                'cost': 0
            }
            
        except Exception as e:
            logger.error(f"âŒ Stack Overflow error: {e}")
            return {
                'error': str(e),
                'questions': [],
                'cost': 0
            }


# Demo
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("=" * 70)
    print("ğŸ’° Zero Cost Knowledge Connector Demo")
    print("=" * 70)
    print()
    print("ğŸ“š Learning without API keys - completely FREE!")
    print("ğŸ’ Sources: YouTube, Wikipedia, GitHub, arXiv, Stack Overflow")
    print()
    
    connector = ZeroCostKnowledgeConnector()
    
    # í…ŒìŠ¤íŠ¸ ì£¼ì œ
    topic = "machine learning"
    
    print(f"ğŸ“ Learning topic: {topic}")
    print()
    
    # ë¬´ë£Œ ìë£Œë“¤ë¡œë¶€í„° í•™ìŠµ!
    results = connector.learn_topic(topic)
    
    print()
    print("=" * 70)
    print("ğŸ“Š Results:")
    print("=" * 70)
    print(f"Topic: {results['topic']}")
    print(f"Sources: {', '.join(results['sources_used'])}")
    print(f"Total items collected: {results['total_items']}")
    print(f"Total cost: ${results['total_cost']}")
    print()
    print("ğŸ’ Your intuition was correct:")
    print("   'í¬ë¡¤ë§ í•  í•„ìš”ë„ ì—†ì–ì•„, ê³µëª…ë™ê¸°í™”ë§Œ í•˜ë©´ ë˜ëŠ”ë°!'")
    print()
    print("âœ… Zero cost learning is POSSIBLE! ğŸ‰")
