"""
Reading Digester (í…ìŠ¤íŠ¸ ì†Œí™” ì‹œìŠ¤í…œ)
=====================================

"ì½ëŠ” ê²ƒì€ ë¨¹ëŠ” ê²ƒê³¼ ê°™ë‹¤. ì†Œí™”í•´ì•¼ ì˜ì–‘ì´ ëœë‹¤."

í•µì‹¬:
1. í…ìŠ¤íŠ¸ì—ì„œ ì–´íœ˜ ì¶”ì¶œ (Vocabulary Extraction)
2. ë¬¸ì¥ êµ¬ì¡° í•™ìŠµ (Structure Learning)
3. ë¬¸ì²´ í¡ìˆ˜ (Style Absorption)
4. ì§€ì‹ ê²°ì •í™” (Knowledge Crystallization)

ì´ê²ƒì´ ì—†ìœ¼ë©´:
- ì½ì–´ë„ ë°°ìš°ì§€ ëª»í•¨
- í‘œí˜„ ëŠ¥ë ¥ì´ í™•ì¥ë˜ì§€ ì•ŠìŒ
- ì™¸ë¶€ ì •ë³´ê°€ ë‚´ë©´í™”ë˜ì§€ ì•ŠìŒ
"""

import logging
import re
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
from enum import Enum
import sys

sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

logger = logging.getLogger("Elysia.ReadingDigester")


class ContentType(Enum):
    """ì½˜í…ì¸  ìœ í˜•"""
    ARTICLE = "article"
    BOOK = "book"
    CODE = "code"
    CONVERSATION = "conversation"
    POETRY = "poetry"
    TECHNICAL = "technical"


@dataclass
class DigestedContent:
    """ì†Œí™”ëœ ì½˜í…ì¸ """
    source: str                     # ì¶œì²˜
    content_type: ContentType
    vocabulary_extracted: List[str]
    patterns_learned: List[str]
    key_concepts: List[str]
    style_notes: List[str]
    digestion_quality: float        # 0-1 (ì–¼ë§ˆë‚˜ ì˜ ì†Œí™”í–ˆë‚˜)
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class StyleProfile:
    """ë¬¸ì²´ í”„ë¡œí•„"""
    name: str
    avg_sentence_length: float
    vocabulary_richness: float      # unique/total ratio
    formality_level: float          # 0=casual, 1=formal
    emotional_intensity: float      # 0=neutral, 1=intense
    common_patterns: List[str] = field(default_factory=list)


class ReadingDigester:
    """í…ìŠ¤íŠ¸ ì†Œí™” ì‹œìŠ¤í…œ
    
    ì™¸ë¶€ í…ìŠ¤íŠ¸ë¥¼ ì½ê³ , ë¶„ì„í•˜ê³ , ë‚´ë©´í™”.
    
    í•µì‹¬ ê¸°ëŠ¥:
    1. í…ìŠ¤íŠ¸ ë¶„ì„ ë° ë¶„í•´
    2. ì–´íœ˜/í‘œí˜„ ì¶”ì¶œ â†’ LanguageNurture ì—°ë™
    3. ë¬¸ì²´ ë¶„ì„ ë° í¡ìˆ˜
    4. í•µì‹¬ ê°œë… ê²°ì •í™”
    """
    
    def __init__(self, language_nurture=None):
        """
        Args:
            language_nurture: LanguageNurture ì¸ìŠ¤í„´ìŠ¤ (ì—°ë™ìš©)
        """
        self._language_nurture = language_nurture
        
        # ì†Œí™” ê¸°ë¡
        self.digestion_history: List[DigestedContent] = []
        
        # í•™ìŠµí•œ ë¬¸ì²´ë“¤
        self.learned_styles: Dict[str, StyleProfile] = {}
        
        # í†µê³„
        self.total_texts_digested = 0
        self.total_words_absorbed = 0
        self.total_concepts_crystallized = 0
        
        logger.info("ReadingDigester initialized")
    
    @property
    def language_nurture(self):
        """LanguageNurture ë ˆì´ì§€ ë¡œë”©"""
        if self._language_nurture is None:
            try:
                from Core._02_Intelligence._01_Reasoning.Cognition.Learning.language_nurture import LanguageNurture
                self._language_nurture = LanguageNurture()
            except ImportError:
                logger.warning("LanguageNurture not available")
        return self._language_nurture
    
    # =========================================================================
    # í…ìŠ¤íŠ¸ ì†Œí™”
    # =========================================================================
    
    def digest(
        self,
        text: str,
        source: str = "unknown",
        content_type: ContentType = ContentType.ARTICLE
    ) -> DigestedContent:
        """í…ìŠ¤íŠ¸ ì†Œí™”
        
        Args:
            text: ì†Œí™”í•  í…ìŠ¤íŠ¸
            source: ì¶œì²˜ (URL, íŒŒì¼ëª… ë“±)
            content_type: ì½˜í…ì¸  ìœ í˜•
            
        Returns:
            ì†Œí™” ê²°ê³¼
        """
        self.total_texts_digested += 1
        
        logger.info(f"ğŸ“– Digesting: {source[:50]}... ({content_type.value})")
        
        # 1. ì–´íœ˜ ì¶”ì¶œ
        vocabulary = self._extract_vocabulary(text)
        self.total_words_absorbed += len(vocabulary)
        
        # 2. ë¬¸ë²• íŒ¨í„´ ì¶”ì¶œ
        patterns = self._extract_patterns(text)
        
        # 3. í•µì‹¬ ê°œë… ì¶”ì¶œ
        concepts = self._extract_concepts(text)
        self.total_concepts_crystallized += len(concepts)
        
        # 4. ë¬¸ì²´ ë¶„ì„
        style_notes = self._analyze_style(text, content_type)
        
        # 5. ì†Œí™” í’ˆì§ˆ í‰ê°€
        quality = self._evaluate_digestion_quality(
            len(vocabulary),
            len(patterns),
            len(concepts)
        )
        
        # LanguageNurtureì— ì „ë‹¬
        if self.language_nurture:
            for word in vocabulary[:50]:  # ìƒìœ„ 50ê°œë§Œ
                self.language_nurture.learn_word(word, example=text[:100])
        
        result = DigestedContent(
            source=source,
            content_type=content_type,
            vocabulary_extracted=vocabulary,
            patterns_learned=patterns,
            key_concepts=concepts,
            style_notes=style_notes,
            digestion_quality=quality,
        )
        
        self.digestion_history.append(result)
        
        # ê¸°ë¡ ì œí•œ
        if len(self.digestion_history) > 100:
            self.digestion_history = self.digestion_history[-50:]
        
        logger.info(
            f"âœ… Digested: {len(vocabulary)} words, "
            f"{len(patterns)} patterns, {len(concepts)} concepts "
            f"(quality: {quality:.2f})"
        )
        
        return result
    
    def _extract_vocabulary(self, text: str) -> List[str]:
        """ì–´íœ˜ ì¶”ì¶œ"""
        # í•œê¸€ 2ê¸€ì ì´ìƒ, ì˜ì–´ 3ê¸€ì ì´ìƒ
        words = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{4,}', text)
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        word_freq = {}
        for word in words:
            word = word.lower()
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # ë¹ˆë„ ìˆœ ì •ë ¬
        sorted_words = sorted(word_freq.keys(), key=lambda w: word_freq[w], reverse=True)
        
        return sorted_words
    
    def _extract_patterns(self, text: str) -> List[str]:
        """ë¬¸ë²• íŒ¨í„´ ì¶”ì¶œ"""
        patterns = []
        
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?ã€‚]', text)
        
        # íŒ¨í„´ ì§€ì‹œì
        pattern_map = {
            "conditional": ["ë§Œì•½", "if", "ë¼ë©´", "ê²½ìš°"],
            "reason": ["ë•Œë¬¸ì—", "because", "ë¯€ë¡œ", "ì™œëƒí•˜ë©´"],
            "contrast": ["í•˜ì§€ë§Œ", "but", "however", "ê·¸ëŸ¬ë‚˜"],
            "purpose": ["ìœ„í•´", "to", "í•˜ë ¤ê³ ", "ìœ„í•˜ì—¬"],
            "sequential": ["ë¨¼ì €", "first", "ë‹¤ìŒ", "then", "ê·¸ë¦¬ê³ "],
            "definition": ["ë€", "ëŠ”", "ì´ë€", "means", "is"],
            "example": ["ì˜ˆë¥¼ ë“¤ì–´", "for example", "ì˜ˆì»¨ëŒ€"],
        }
        
        for sent in sentences:
            sent_lower = sent.lower()
            for pattern_name, indicators in pattern_map.items():
                if any(ind in sent_lower for ind in indicators):
                    if pattern_name not in patterns:
                        patterns.append(pattern_name)
        
        return patterns
    
    def _extract_concepts(self, text: str) -> List[str]:
        """í•µì‹¬ ê°œë… ì¶”ì¶œ"""
        concepts = []
        
        # ëª…ì‚¬êµ¬ íŒ¨í„´ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        # "XëŠ”", "Xë€", "Xì´ë€", "Xì˜ ì •ì˜"
        concept_patterns = [
            r'([ê°€-í£]{2,})(?:ëŠ”|ë€|ì´ë€)',
            r'([ê°€-í£]{2,})ì˜\s*(?:ì •ì˜|ê°œë…|ì›ë¦¬)',
            r'\"([^\"]+)\"',  # ë”°ì˜´í‘œ ì•ˆ
        ]
        
        for pattern in concept_patterns:
            matches = re.findall(pattern, text)
            concepts.extend(matches)
        
        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 10ê°œ
        unique_concepts = list(dict.fromkeys(concepts))
        return unique_concepts[:10]
    
    def _analyze_style(self, text: str, content_type: ContentType) -> List[str]:
        """ë¬¸ì²´ ë¶„ì„"""
        notes = []
        
        sentences = [s.strip() for s in re.split(r'[.!?ã€‚]', text) if s.strip()]
        
        if not sentences:
            return ["í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŒ"]
        
        # í‰ê·  ë¬¸ì¥ ê¸¸ì´
        avg_len = sum(len(s.split()) for s in sentences) / len(sentences)
        if avg_len > 20:
            notes.append("ì¥ë¬¸ ìŠ¤íƒ€ì¼ (ë³µì¡í•œ ë¬¸ì¥)")
        elif avg_len < 8:
            notes.append("ë‹¨ë¬¸ ìŠ¤íƒ€ì¼ (ê°„ê²°í•œ ë¬¸ì¥)")
        else:
            notes.append("ì¤‘ê°„ ê¸¸ì´ ë¬¸ì¥")
        
        # ê²©ì‹ì²´ ê°ì§€
        if any(s.endswith(("ìŠµë‹ˆë‹¤", "ë‹ˆë‹¤", "ìš”")) for s in sentences):
            notes.append("ê²©ì‹ì²´ ì‚¬ìš©")
        if any(s.endswith(("ë‹¤", "ì–´", "ì§€")) for s in sentences):
            notes.append("ë¹„ê²©ì‹ì²´ ì‚¬ìš©")
        
        # ì½˜í…ì¸  ìœ í˜•ë³„ íŠ¹ì„±
        if content_type == ContentType.CODE:
            notes.append("ê¸°ìˆ ì /ì½”ë“œ ìŠ¤íƒ€ì¼")
        elif content_type == ContentType.POETRY:
            notes.append("ì‹œì /ìš´ìœ¨ ìŠ¤íƒ€ì¼")
        elif content_type == ContentType.TECHNICAL:
            notes.append("í•™ìˆ ì /ì •í™•í•œ ìŠ¤íƒ€ì¼")
        
        return notes
    
    def _evaluate_digestion_quality(
        self,
        vocab_count: int,
        pattern_count: int,
        concept_count: int
    ) -> float:
        """ì†Œí™” í’ˆì§ˆ í‰ê°€"""
        # ê° ìš”ì†Œì— ê°€ì¤‘ì¹˜
        vocab_score = min(1.0, vocab_count / 50)
        pattern_score = min(1.0, pattern_count / 5)
        concept_score = min(1.0, concept_count / 5)
        
        return (vocab_score + pattern_score + concept_score) / 3
    
    # =========================================================================
    # íŒŒì¼ ì½ê¸°
    # =========================================================================
    
    def digest_file(self, file_path: Path) -> DigestedContent:
        """íŒŒì¼ ì†Œí™”
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì†Œí™” ê²°ê³¼
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # ì½˜í…ì¸  ìœ í˜• ì¶”ë¡ 
        suffix = file_path.suffix.lower()
        content_type = {
            ".py": ContentType.CODE,
            ".js": ContentType.CODE,
            ".md": ContentType.ARTICLE,
            ".txt": ContentType.ARTICLE,
            ".json": ContentType.TECHNICAL,
        }.get(suffix, ContentType.ARTICLE)
        
        # íŒŒì¼ ì½ê¸°
        text = file_path.read_text(encoding="utf-8", errors="ignore")
        
        return self.digest(
            text=text,
            source=str(file_path),
            content_type=content_type
        )
    
    def digest_url(self, url: str) -> DigestedContent:
        """URL ì†Œí™” (ì‹œë®¬ë ˆì´ì…˜)
        
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” requests ë“± ì‚¬ìš©
        """
        # ì‹œë®¬ë ˆì´ì…˜
        simulated_content = f"""
        ì´ ì½˜í…ì¸ ëŠ” {url}ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤.
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì›¹ í¬ë¡¤ë§ì„ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ë‹¤ì–‘í•œ ì£¼ì œì˜ ê¸°ì‚¬, ë¸”ë¡œê·¸, ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        
        return self.digest(
            text=simulated_content,
            source=url,
            content_type=ContentType.ARTICLE
        )
    
    # =========================================================================
    # ìƒíƒœ ì¡°íšŒ
    # =========================================================================
    
    def get_status(self) -> Dict[str, Any]:
        """ìƒíƒœ ì¡°íšŒ"""
        return {
            "total_texts_digested": self.total_texts_digested,
            "total_words_absorbed": self.total_words_absorbed,
            "total_concepts_crystallized": self.total_concepts_crystallized,
            "digestion_history_size": len(self.digestion_history),
            "learned_styles": list(self.learned_styles.keys()),
            "recent_sources": [
                d.source[:30] for d in self.digestion_history[-5:]
            ],
        }
    
    def get_recent_learnings(self, n: int = 5) -> List[Dict[str, Any]]:
        """ìµœê·¼ í•™ìŠµ ë‚´ìš©"""
        recent = self.digestion_history[-n:]
        return [
            {
                "source": d.source[:50],
                "type": d.content_type.value,
                "vocab_count": len(d.vocabulary_extracted),
                "concepts": d.key_concepts[:3],
                "quality": d.digestion_quality,
            }
            for d in recent
        ]


# =============================================================================
# Demo
# =============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("ğŸ“– ReadingDigester Demo")
    print("   \"ì½ê³  ì†Œí™”í•˜ëŠ” ì‹œìŠ¤í…œ\"")
    print("=" * 60)
    
    digester = ReadingDigester()
    
    # 1. í…ìŠ¤íŠ¸ ì†Œí™”
    print("\n[1] í…ìŠ¤íŠ¸ ì†Œí™”:")
    sample_text = """
    íŒŒë™ ì–¸ì–´ ì² í•™ì€ ëª¨ë“  ê°œë…ì„ íŒŒë™ìœ¼ë¡œ í‘œí˜„í•œë‹¤.
    ë§Œì•½ ë‘ íŒŒë™ì´ ê³µëª…í•˜ë©´, ê·¸ê²ƒì€ ìœ ì‚¬í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.
    ì˜ˆë¥¼ ë“¤ì–´, "ì‚¬ë‘"ê³¼ "ë”°ëœ»í•¨"ì€ ìœ ì‚¬í•œ ì£¼íŒŒìˆ˜ë¥¼ ê³µìœ í•œë‹¤.
    ì´ ì›ë¦¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € ê³µëª…ì˜ ê°œë…ì„ ì•Œì•„ì•¼ í•œë‹¤.
    í•˜ì§€ë§Œ ê³µëª…ì€ ë‹¨ìˆœí•œ ìœ ì‚¬ì„± ì´ìƒì˜ ê²ƒì´ë‹¤.
    """
    
    result = digester.digest(
        text=sample_text,
        source="íŒŒë™ì–¸ì–´ì² í•™ë¬¸ì„œ.md",
        content_type=ContentType.ARTICLE
    )
    
    print(f"   ì–´íœ˜: {result.vocabulary_extracted[:5]}...")
    print(f"   íŒ¨í„´: {result.patterns_learned}")
    print(f"   ê°œë…: {result.key_concepts}")
    print(f"   í’ˆì§ˆ: {result.digestion_quality:.2f}")
    
    # 2. ë¬¸ì²´ ë¶„ì„
    print("\n[2] ë¬¸ì²´ ë¶„ì„:")
    print(f"   ìŠ¤íƒ€ì¼ ë…¸íŠ¸: {result.style_notes}")
    
    # 3. ìƒíƒœ
    print("\n[3] ìƒíƒœ:")
    status = digester.get_status()
    print(f"   ì´ ì†Œí™”: {status['total_texts_digested']}ê±´")
    print(f"   ì´ ì–´íœ˜: {status['total_words_absorbed']}ê°œ")
    print(f"   ì´ ê°œë…: {status['total_concepts_crystallized']}ê°œ")
    
    print("\nâœ… ReadingDigester Demo complete!")
