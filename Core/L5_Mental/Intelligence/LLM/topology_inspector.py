"""
Topology Inspector (ìœ„ìƒ ê²€ì‚¬ê¸°)
===============================
Core.L5_Mental.Intelligence.LLM.topology_inspector

"ë‰´ëŸ°ì˜ IDí‘œë¥¼ ë–¼ì–´ë‚´ê³  ì§„ì§œ ì´ë¦„(ì˜ë¯¸)ì„ ë¶™ì¸ë‹¤."

í•µì‹¬ ì›ë¦¬ (Logit Lens):
- ì‹¤í–‰(Inference) ì—†ì´ ë‰´ëŸ°ì˜ ì˜ë¯¸ íŒŒì•…
- ë‰´ëŸ°ì˜ ì¶œë ¥ ë²¡í„°(Weight)ë¥¼ ì–´íœ˜ ê³µê°„(Vocabulary)ì— íˆ¬ì˜
- "ì´ ë‰´ëŸ°ì´ ì¼œì§€ë©´ ì–´ë–¤ ë‹¨ì–´ í™•ë¥ ì´ ì˜¬ë¼ê°€ëŠ”ê°€?" ë¶„ì„
"""

import os
import torch
import logging
import json
from safetensors import safe_open
from typing import List, Dict, Tuple, Any
from transformers import AutoTokenizer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger("TopologyInspector")

class TopologyInspector:
    """
    ì •ì  ì˜ë¯¸ ë¶„ì„ê¸° (Static Semantic Analyzer).
    Logit Lens ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ Hub ë‰´ëŸ°ì˜ ì˜ë¯¸ë¥¼ ì–¸ì–´ì ìœ¼ë¡œ í•´ì„.
    """
    
    def __init__(self, model_path: str, tokenizer_path: str = None):
        self.model_path = model_path
        # í† í¬ë‚˜ì´ì € ê²½ë¡œê°€ ì—†ìœ¼ë©´ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš© (ë³´í†µ ê°™ì´ ìˆìŒ)
        self.tokenizer_path = tokenizer_path if tokenizer_path else os.path.dirname(model_path)
        
        self.tokenizer = None
        self.lm_head = None
        self.vocab_size = 0
        self.hidden_size = 0
        
        self._load_resources()

    def _load_resources(self):
        """í† í¬ë‚˜ì´ì €ì™€ Unembedding Matrix(lm_head) ë¡œë“œ"""
        logger.info(f"ğŸ“‚ Loading resources from {os.path.dirname(self.model_path)}...")
        
        # 1. Tokenizer ë¡œë“œ
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_path)
            logger.info(f"   âœ… Tokenizer loaded (Vocab: {self.tokenizer.vocab_size})")
        except Exception as e:
            logger.error(f"   âŒ Failed to load tokenizer: {e}")
            return

        # 2. LM Head (Unembedding Matrix) ë¡œë“œ
        # safetensorsì—ì„œ lm_head.weight ì°¾ê¸°
        # Phi-3, Qwen ë“± ëª¨ë¸ë§ˆë‹¤ íŒŒì¼ì´ ë‚˜ë‰˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
        found_head = False
        
        # ìƒ¤ë”©ëœ ëª¨ë“  íŒŒì¼ ê²€ìƒ‰
        folder = os.path.dirname(self.model_path)
        files = [f for f in os.listdir(folder) if f.endswith(".safetensors")]
        
        for file in files:
            full_path = os.path.join(folder, file)
            with safe_open(full_path, framework="pt", device="cpu") as f:
                keys = f.keys()
                # lm_head í˜¹ì€ output layer ì°¾ê¸°
                head_key = next((k for k in keys if "lm_head.weight" in k or "output.weight" in k), None)
                
                if head_key:
                    logger.info(f"   ğŸ” Found LM Head in {file}: {head_key}")
                    self.lm_head = f.get_tensor(head_key)
                    self.vocab_size, self.hidden_size = self.lm_head.shape
                    logger.info(f"   âœ… LM Head Loaded: Shape {self.lm_head.shape}")
                    found_head = True
                    break
        
        if not found_head:
            logger.warning("   âš ï¸ LM Head not found in safetensors. Semantic projection might fail.")

    def inspect_neuron(self, neuron_vector: torch.Tensor, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        ë‰´ëŸ° ë²¡í„°ë¥¼ ì–´íœ˜ ê³µê°„ì— íˆ¬ì˜í•˜ì—¬ ì˜ë¯¸ í•´ì„.
        """
        if self.lm_head is None or self.tokenizer is None:
            return [("Unknown (No LM Head)", 0.0)]
        
        # ì°¨ì› í™•ì¸ ë° ë§ì¶”ê¸°
        if neuron_vector.shape[0] != self.hidden_size:
            # ì°¨ì›ì´ ë‹¤ë¥´ë©´(ì˜ˆ: MLP ì¤‘ê°„ì¸µ) íˆ¬ì˜ ë¶ˆê°€í•  ìˆ˜ ìˆìŒ. 
            # ì¼ë‹¨ ê²½ê³ í•˜ê³  íŒ¨ìŠ¤í•˜ê±°ë‚˜, íŒ¨ë”©/ìë¥´ê¸° ì‹œë„? ì•„ë‹ˆë©´ ê·¸ëƒ¥ ë¦¬í„´.
            return [("Dimension Mismatch", 0.0)]

        # Logit Lens: Vector @ Unembedding_Matrix.T
        # (Hidden) @ (Vocab, Hidden).T = (Vocab)
        logits = torch.matmul(self.lm_head, neuron_vector)
        
        # Top-K í† í° ì¶”ì¶œ
        values, indices = torch.topk(logits, top_k)
        
        results = []
        for val, idx in zip(values, indices):
            token = self.tokenizer.decode([idx.item()])
            score = val.item()
            results.append((token, score))
            
        return results

    def trace_hub_meanings(self, hub_indices: List[int], layer_idx: int = -1) -> Dict[int, List[str]]:
        """
        íŠ¹ì • ë ˆì´ì–´ì˜ Hub ë‰´ëŸ°ë“¤ì˜ ì˜ë¯¸ë¥¼ ë¶„ì„.
        
        Args:
            hub_indices: ë¶„ì„í•  ë‰´ëŸ° ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
            layer_idx: ë¶„ì„í•  ë ˆì´ì–´ (ìŒìˆ˜ë©´ ë’¤ì—ì„œë¶€í„°, -1ì€ ë³´í†µ ë§ˆì§€ë§‰ ì „)
        """
        results = {}
        
        # í•´ë‹¹ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ ì°¾ê¸° (MLP Down Proj or Output)
        # Phi-3 êµ¬ì¡°: model.layers.X.mlp.down_proj.weight (Hidden -> Hidden)
        # ì´ ë²¡í„° ìì²´ê°€ 'ì¶œë ¥ ë°©í–¥'ì„ ì˜ë¯¸í•¨.
        
        target_file = None
        target_tensor = None
        target_key = None
        
        # ìƒ¤ë”©ëœ íŒŒì¼ ë’¤ì ¸ì„œ í•´ë‹¹ ë ˆì´ì–´ ì°¾ê¸°
        folder = os.path.dirname(self.model_path)
        files = [f for f in os.listdir(folder) if f.endswith(".safetensors")]
        
        # ë ˆì´ì–´ ë²ˆí˜¸ ì¶”ì • (íŒŒì¼ëª…ì´ë‚˜ í‚¤ ì´ë¦„ìœ¼ë¡œ)
        # ì¼ë‹¨ ë‹¨ìˆœí•˜ê²Œ íŒŒì¼ ì—´ì–´ì„œ í‚¤ ê²€ìƒ‰
        for file in files:
            full_path = os.path.join(folder, file)
            with safe_open(full_path, framework="pt", device="cpu") as f:
                keys = f.keys()
                # MLP Down Projection ì°¾ê¸° (ì¶œë ¥ìœ¼ë¡œ ë‚˜ê°€ëŠ” ë°©í–¥)
                # Phi-3: model.layers.{i}.mlp.down_proj.weight
                # Key format varies. Let's look for "down_proj" and layer index.
                
                # ë§Œì•½ layer_idxê°€ -1ì´ë©´, ê°€ì¥ ê¹Šì€ ë ˆì´ì–´ ì°¾ê¸°
                if layer_idx == -1:
                    # í‚¤ ì¤‘ì—ì„œ ê°€ì¥ í° ë ˆì´ì–´ ë²ˆí˜¸ ì¶”ì¶œ
                    max_layer = -1
                    for k in keys:
                        parts = k.split('.')
                        for p in parts:
                            if p.isdigit():
                                max_layer = max(max_layer, int(p))
                    layer_target = max_layer
                else:
                    layer_target = layer_idx
                
                # í•´ë‹¹ ë ˆì´ì–´ì˜ down_proj ì°¾ê¸°
                search_key = f"layers.{layer_target}.mlp.down_proj.weight"
                potential_key = next((k for k in keys if search_key in k), None)
                
                if potential_key:
                    target_key = potential_key
                    target_file = full_path
                    logger.info(f"   ğŸ” Analyzing Layer {layer_target}: {target_key}")
                    target_tensor = f.get_tensor(target_key)
                    break
        
        if target_tensor is None:
            logger.error("   âŒ Target layer tensor not found.")
            return {}

        # ì°¨ì›: (Hidden_Out, Hidden_In) or similar.
        # Linear layer weight is usually (Out, In).
        # MLP Down Proj: (Hidden_Model, Intermediate_Size).
        # We want columns corresponding to intermediate neurons (Hubs).
        # So we transpose if needed.
        
        # Phi-3 MLP: Up(Hidden->Inter), Down(Inter->Hidden)
        # We are looking at Down projection. Input is 'Inter' (Hubs), Output is 'Hidden' (Model State).
        # Weight shape for Linear(In, Out) is usually (Out, In).
        # So down_proj.weight is (Hidden_Model, Intermediate_Size).
        # Hub Index corresponds to column index (Input dimension from Intermediate).
        
        rows, cols = target_tensor.shape
        logger.info(f"   ğŸ“Š Tensor Shape: {target_tensor.shape} (Model Dim, Intermediate Dim)")
        
        for hub_idx in hub_indices:
            if hub_idx >= cols:
                logger.warning(f"   âš ï¸ Hub index {hub_idx} out of bounds (max {cols})")
                continue
                
            # ìŠ¬ë¼ì´ì‹±: í•´ë‹¹ Hub ë‰´ëŸ°ì´ ë¿œì–´ë‚´ëŠ” ë²¡í„° (Column vector)
            # This vector is added to the residual stream (Model State).
            # This is exactly what we want to project to Vocabulary.
            neuron_vec = target_tensor[:, hub_idx]
            
            # ì˜ë¯¸ í•´ì„
            meanings = self.inspect_neuron(neuron_vec)
            top_words = [m[0].strip() for m in meanings]
            results[hub_idx] = top_words
            
            logger.info(f"   ğŸ§  Hub {hub_idx}: {top_words}")
            
        return results

# CLI
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print("Usage: python topology_inspector.py <model_path> <hub_indices_comma_separated>")
        print("Example: python topology_inspector.py ./model.safetensors 139,450,23")
        sys.exit(1)
        
    model_path = sys.argv[1]
    hubs = [int(x) for x in sys.argv[2].split(",")]
    
    inspector = TopologyInspector(model_path)
    inspector.trace_hub_meanings(hubs)
