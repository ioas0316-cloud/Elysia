"""
Audio Topology Tracer (ì²­ê° ìœ„ìƒ ë¶„ì„ê¸°)
=====================================
Core.L5_Mental.Intelligence.LLM.audio_topology_tracer

"ì†Œë¦¬ê°€ ì˜ë¯¸ë¡œ ë³€í•˜ëŠ” ì°°ë‚˜ì˜ ìˆœê°„ì„ í¬ì°©í•œë‹¤."

Objective:
    - Whisper ëª¨ë¸ì˜ Cross-Attention Layerë¥¼ ë¶„ì„.
    - Audio Encoderì˜ Featureê°€ Text Decoderì˜ ì–´ë–¤ Tokenì„ ìžê·¹í•˜ëŠ”ì§€ ì¶”ì .
    - 'ì¸ê³¼ì  ì—°ê²°(Causal Connection)'ì„ ì¶”ì¶œí•˜ì—¬ ì†Œë¦¬ì˜ ì˜ë¯¸ë¡ ì  ê¸°ì›ì„ ë°íž˜.
"""

import os
import torch
import logging
import numpy as np
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
from safetensors import safe_open
from transformers import WhisperForConditionalGeneration, WhisperProcessor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger("AudioTracer")

@dataclass
class BridgeSynapse:
    """ì†Œë¦¬(Encoder)ì™€ ì˜ë¯¸(Decoder)ë¥¼ ìž‡ëŠ” ì‹œëƒ…ìŠ¤"""
    layer_idx: int
    head_idx: int
    audio_time_idx: int     # ì˜¤ë””ì˜¤ì˜ ì–´ëŠ êµ¬ê°„ì¸ê°€ (Time Frame)
    token_idx: int          # ì–´ë–¤ ë‹¨ì–´ê°€ ìƒì„±ë˜ì—ˆëŠ”ê°€
    attention_weight: float # ì—°ê²° ê°•ë„

class AudioTopologyTracer:
    def __init__(self, model_id: str = "openai/whisper-large-v3", device: str = "cuda"):
        self.model_id = model_id
        self.device = device if torch.cuda.is_available() else "cpu"
        self.model = None
        self.processor = None
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model()

    def _load_model(self):
        logger.info(f"ðŸ‘‚ Loading Whisper Topology: {self.model_id}")
        try:
            self.processor = WhisperProcessor.from_pretrained(self.model_id)
            self.model = WhisperForConditionalGeneration.from_pretrained(self.model_id).to(self.device)
            self.model.eval()
            logger.info("   âœ… Model loaded successfully.")
        except Exception as e:
            logger.error(f"   âŒ Failed to load model: {e}")

    def trace_mechanism(self, audio_path: str) -> List[BridgeSynapse]:
        """
        ì†Œë¦¬ê°€ ì˜ë¯¸ë¡œ ë³€í™˜ë˜ëŠ” ê³¼ì •ì„ ì¶”ì .
        Cross-Attention ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì¶œí•˜ì—¬ ì¸ê³¼ ê´€ê³„ë¥¼ ë¶„ì„í•¨.
        """
        if self.model is None:
            return []

        logger.info(f"ðŸ” Tracing Causality in: {os.path.basename(audio_path)}")
        
        # 1. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
        import librosa
        audio, sr = librosa.load(audio_path, sr=16000)
        inputs = self.processor(audio, sampling_rate=sr, return_tensors="pt").to(self.device)
        input_features = inputs.input_features

        # 2. Inference with Attention Retrieval
        # output_attentions=Trueë¥¼ í†µí•´ ë‚´ë¶€ 'ì£¼ëª©(Attention)' ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        with torch.no_grad():
            outputs = self.model.generate(
                input_features,
                return_dict_in_generate=True,
                output_attentions=True,
                max_new_tokens=50
            )

        # 3. Cross-Attention Analysis
        # outputs.cross_attentions shape: (num_tokens, num_layers, batch, num_heads, seq_len, audio_frames)
        # ìš°ë¦¬ëŠ” ì´ê²ƒì„ ì—­ì¶”ì í•˜ì—¬ "ì´ ë‹¨ì–´ëŠ” ì € ì†Œë¦¬ ë•Œë¬¸ì— ë‚˜ì™”ë‹¤"ëŠ” ì¸ê³¼ë¥¼ ì°¾ìŒ.
        
        generated_ids = outputs.sequences[0]
        transcription = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        logger.info(f"   ðŸ“ Surface Output: '{transcription}'")

        synapses = []
        
        # ê° ìƒì„±ëœ í† í°ì— ëŒ€í•´
        cross_attentions = outputs.cross_attentions
        # Note: cross_attentions structure depends on transformers version, simplified assumes tuple of layers
        
        for token_pos, layer_attns in enumerate(cross_attentions):
            # layer_attns: Tuple of (batch, heads, 1, audio_frames) for each layer
            
            # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ Attentionì´ ë³´í†µ ê°€ìž¥ êµ¬ì²´ì ì¸ ì¸ê³¼ë¥¼ ê°€ì§
            last_layer_attn = layer_attns[-1] # (batch, heads, 1, audio_frames)
            
            # Head í‰ê·  (HeadëŠ” ì—¬ëŸ¬ ê´€ì ì´ë¯€ë¡œ í‰ê· ë‚´ì–´ ì „ì²´ì ì¸ 'ì£¼ëª©'ì„ ë´„)
            # ì°¨ì›: (heads, 1, audio_frames) -> (audio_frames)
            attn_avg = last_layer_attn[0].mean(dim=0).squeeze() # (audio_frames)
            
            # ê°€ìž¥ ê°•í•˜ê²Œ ë°˜ì‘í•œ ì˜¤ë””ì˜¤ í”„ë ˆìž„ (The Cause)
            top_audio_idx = torch.argmax(attn_avg).item()
            max_weight = torch.max(attn_avg).item()
            
            token_id = generated_ids[token_pos+1] # +1 to skip start token if cross_attns aligns
            token_str = self.processor.decode([token_id])
            
            synapses.append(BridgeSynapse(
                layer_idx=-1, # Last layer
                head_idx=-1,  # Average
                audio_time_idx=top_audio_idx,
                token_idx=token_pos,
                attention_weight=max_weight
            ))
            
            # ì¤‘ìš” ì¸ê³¼ ê´€ê³„ ë¡œê¹… (ê°€ì¤‘ì¹˜ê°€ ë†’ì„ ë•Œë§Œ)
            if max_weight > 0.1:
                # ì˜¤ë””ì˜¤ í”„ë ˆìž„ì„ ëŒ€ëžµì ì¸ ì‹œê°„ìœ¼ë¡œ ë³€í™˜ (Whisper frame ~20ms)
                time_sec = top_audio_idx * 0.02 
                logger.info(f"   ðŸ”— Causal Link: Sound({time_sec:.2f}s) -> Token['{token_str.strip()}'] (Strength: {max_weight:.2f})")

        return synapses

if __name__ == "__main__":
    import sys
    # For testing, provide existing file or use dummy
    tracer = AudioTopologyTracer()
    
    # Check for existing test file
    test_file = "C:/Elysia/tests/sample_hearing.wav" 
    if len(sys.argv) > 1:
        test_file = sys.argv[1]
        
    if os.path.exists(test_file):
        tracer.trace_mechanism(test_file)
    else:
        logger.warning(f"Test file not found: {test_file}")
