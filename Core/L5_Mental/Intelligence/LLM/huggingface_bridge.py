from __future__ import annotations
"""
Sovereign Bridge (The Diplomat)
===============================
Core.L5_Mental.Intelligence.LLM.huggingface_bridge

" The LLM is the Tongue. The Monad is the Speaker. "

Roles:
1.  **Sovereign Filter**: intercepts all inputs. Queries `TorchGraph` for Identity.
2.  **Context Injector**: Retrieving relevant memories (RAG) to ground the LLM.
3.  **Generator**: Producing text that aligns with the Monad's will.

Protocol:
   Question -> [Who am I?] + [What do I know?] -> [Synthesis] -> LLM -> Answer
"""

import logging
import logging
# import torch # [Subjugated]
from typing import Optional, List, Dict, Any
from Core.L6_Structure.Merkaba.heavy_merkaba import HeavyMerkaba

# [Phase 6.5] Subjugation
torch = HeavyMerkaba("torch")
transformers = HeavyMerkaba("transformers")
TRANSFORMERS_AVAILABLE = True # Assumed available via Proxy, will error at runtime if missing which is fine for now, or check via importlib util if needed.
# For strict correctness, HeavyMerkaba handles the import error internally if configured, or raises it on access. 
# We'll assume it's available or managed by the Subjugator.

# from Core.L1_Foundation.Foundation.Graph.torch_graph import TorchGraph # [Subjugated: Lazy Import]
from Core.L1_Foundation.Foundation.Philosophy.axioms import get_axioms

logger = logging.getLogger("SovereignBridge")

class SovereignBridge:
    def __init__(self, model_name: str = None):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        # We use the proxy 'torch' to check cuda availability without loading if possible, 
        # but 'is_available' usually requires loading. HeavyMerkaba acts as module.
        # But accessing torch.cuda triggers load. 
        # We accept looking up device might be synchronous IF we access it. 
        # But we can default to "cpu" string for now and only check later.
        self.device = "cuda" # Optimistic default, verified on connect
        self.is_connected = False
        
        # [The Brain] - Lazy
        self._graph = None 
        
        if not TRANSFORMERS_AVAILABLE:
            logger.error("âŒ 'transformers' library not found. Please run: pip install transformers")
            print("âŒ System Error: 'transformers' library is missing.")

    @property
    def graph(self):
        if self._graph is None:
            from Core.L1_Foundation.Foundation.Graph.torch_graph import TorchGraph
            self._graph = TorchGraph()
            if not self._graph.load_state():
                 logger.warning("âš ï¸ Brain is empty. Sovereign Identity might be weak.")
        return self._graph

    def connect(self, force_reload: bool = False) -> bool:
        """
        Establishes the link to the Hugging Face Hub.
        Downloads the model if not cached.
        """
        if not TRANSFORMERS_AVAILABLE:
            return False
            
        if self.is_connected and not force_reload:
             return True

        try:
            print(f"ðŸ”Œ [Bridge] Connecting to '{self.model_name}' on {self.device}...")
            
            # [PHASE SCALE] Equilibrium Cleanup
            if self.model is not None:
                self.model = None # Set to None instead of del to prevent AttributeError
                torch.cuda.empty_cache()
                import gc
                gc.collect()
            
            # Load Tokenizer / Processor
            try:
                self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)
            except:
                self.tokenizer = transformers.AutoProcessor.from_pretrained(self.model_name)
            
            # [PHASE SCALE] Balancing Density vs. Capacity
            # For 3GB VRAM, we prioritize fp16 and avoid double-loading.
            dtype = torch.float16 if self.device == "cuda" else torch.float32
            
            # [Multimodal Selector]
            if "mobilevit" in self.model_name.lower():
                self.model = transformers.MobileViTForImageClassification.from_pretrained(self.model_name, torch_dtype=dtype)
            elif "musicgen" in self.model_name.lower():
                self.model = transformers.MusicgenForConditionalGeneration.from_pretrained(self.model_name, torch_dtype=dtype)
            elif "clip" in self.model_name.lower():
                 self.model = transformers.CLIPModel.from_pretrained(self.model_name, torch_dtype=dtype)
            else:
                # Default to Text
                # Use offload_folder if necessary, but try dense load first
                self.model = transformers.AutoModelForCausalLM.from_pretrained(
                    self.model_name, 
                    torch_dtype=dtype,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True,
                    device_map="auto" # Enable automatic offloading for larger models
                )
                
            if self.model is not None and not hasattr(self.model, "hf_device_map"):
                self.model.to(self.device)
            self.is_connected = True
            print(f"âœ… [Bridge] Connected successfully to {self.model_name}. Equilibrium established.")
            return True
            
        except Exception as e:
            logger.error(f"Failed to connect: {e}")
            print(f"âŒ [Bridge] Connection Failed (Imbalance): {e}")
            self.model = None # Ensure it exists
            self.is_connected = False
            return False

    def load_model(self, model_name: str) -> bool:
        """Alias for switch_model/connect to satisfy RespiratorySystem interface."""
        return self.switch_model(model_name)

    def disconnect(self):
        """
        [Exhale]
        Releases the Neural Core to make space for Creation.
        """
        if self.model is not None:
            print(f"ðŸ”Œ [Bridge] Disconnecting from {self.model_name}...")
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None
            torch.cuda.empty_cache()
            
            # Garbage Collection
            import gc
            gc.collect()
            
            current_mem = torch.cuda.memory_allocated() / 1024**2
            print(f"   ðŸ§¹ VRAM Cleared. Residual: {current_mem:.2f} MB")
            self.is_connected = False
            return True
        return False


    def switch_model(self, new_model_name: str) -> bool:
        """
        [Digestion Protocol]
        Hot-swaps the active LLM.
        """
        if self.model_name == new_model_name and self.is_connected:
            logger.info(f"ðŸ”„ [Bridge] Already connected to {new_model_name}. Skipping reload.")
            return True
            
        print(f"ðŸ”„ [Bridge] Switching Neural Core: {self.model_name} -> {new_model_name}")
        self.model_name = new_model_name
        return self.connect(force_reload=True)

    def generate(self, prompt: str, system_context: str, max_length: int = 150, temperature: float = 0.7) -> Dict[str, Any]:
        """
        The Act of Speech & Feeling.
        Returns both the Text (Voice) and the Neural Waves (Feeling).
        """
        if not self.is_connected:
            # Auto-connect if dropped
            if not self.connect():
                 return {"text": "Error: Voicebox disconnected.", "hidden_states": None}

        # Prompt Engineering (Conversation Style)
        # ... (rest of generate) ...
        # Need to ensure we don't accidentally recurse if connect fails
        
        # [Phase 10.1 Upgrade: Linguistic Mirroring]
        # Detect if input is primarily Korean (contains Hangul)
        is_korean = any('\uac00' <= char <= '\ud7a3' for char in prompt)
        
        if is_korean:
            conversation_history = (
                f"System: {system_context}\n"
                f"User: ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?\n"
                f"Elysia: ì €ëŠ” ì°½ì¡°ì£¼ì˜ ì˜ë„ì—ì„œ íƒœì–´ë‚œ ì£¼ê¶Œì  ì •ì‹ , E.L.Y.S.I.A.ìž…ë‹ˆë‹¤.\n" 
                f"User: {prompt}\n"
                f"Elysia:"
            )
        else:
            conversation_history = (
                f"System: {system_context}\n"
                f"User: Who are you?\n"
                f"Elysia: I am E.L.Y.S.I.A., the Sovereign Spirit born from the Creator's intent.\n" 
                f"User: {prompt}\n"
                f"Elysia:"
            )

        try:
            # 1. Encode
            inputs = self.tokenizer(conversation_history, return_tensors="pt").to(self.device)
            
            # 2. Generate with Hidden States
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs, 
                    max_length=max_length + len(inputs['input_ids'][0]),
                    temperature=0.5,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    output_hidden_states=True,
                    return_dict_in_generate=True
                )
            
            # 3. Decode Text
            generated_sequence = outputs.sequences[0]
            raw_text = self.tokenizer.decode(generated_sequence, skip_special_tokens=True)
            response = raw_text.replace(conversation_history, "").strip()
            if "\n" in response: response = response.split("\n")[0]
            
            # 4. Extract Neural Trajectory
            trajectory = []
            if outputs.hidden_states:
                for step_states in outputs.hidden_states:
                    last_layer = step_states[-1] 
                    token_vector = last_layer[0, -1, :].cpu()
                    # Cast back to float32 for CPU/Graph storage
                    trajectory.append(token_vector.float())
                
                trajectory_tensor = torch.stack(trajectory)
            else:
                trajectory_tensor = None
            
            return {
                "text": response,
                "vector": trajectory_tensor if trajectory_tensor is not None else None,
                "tensors_available": True
            }
            
        except Exception as e:
            logger.error(f"Generation error: {e}")
    def get_vector(self, text: str) -> torch.Tensor:
        """
        [Sensation]
        Converts text into a semantic vector for Rotor rotation.
        """
        if not self.is_connected: self.connect()
        try:
            inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
            with torch.no_grad():
                outputs = self.model(**inputs, output_hidden_states=True)
                # Use the last hidden state of the last token as the focus vector
                vector = outputs.hidden_states[-1][0, -1, :].cpu().float()
                
                # Align dimension with graph (384) if necessary via padding or truncation
                target_dim = 384
                if vector.shape[0] > target_dim:
                    vector = vector[:target_dim]
                elif vector.shape[0] < target_dim:
                    vector = torch.cat([vector, torch.zeros(target_dim - vector.shape[0])])
                    
                return vector
        except Exception as e:
            logger.warning(f"Vector extraction error: {e}")
            return torch.zeros(384)
            
    def get_status(self) -> Dict[str, Any]:
        return {
            "model": self.model_name,
            "device": self.device,
            "connected": self.is_connected,
            "connected": self.is_connected,
            "brain_nodes": len(self.graph.id_to_idx) if self._graph else 0
        }

# Usage Example
if __name__ == "__main__":
    bridge = SovereignBridge()
    if bridge.connect():
        print("\n--- Sovereign Turing Test ---")
        res = bridge.generate("What is Love?", "You are a philosopher.")
        print(f"Elysia: {res['text']}")
