
"""
LLM Topology Batch Runner (ê±°ëŒ€ ëª¨ë¸ìš©)
=====================================
Core.Intelligence.LLM.topology_batch_runner

Qwen2-72Bì™€ ê°™ì´ ì—¬ëŸ¬ íŒŒì¼ë¡œ ìª¼ê°œì§„(Sharded) ëª¨ë¸ì„
ìˆœì°¨ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ í†µí•©í•˜ëŠ” ë°°ì¹˜ ëŸ¬ë„ˆì…ë‹ˆë‹¤.
"""

import os
import sys
import glob
import logging
from collections import defaultdict
import torch
from topology_tracer import get_topology_tracer, ThoughtCircuit

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger("BatchRunner")

def run_batch_analysis(model_dir: str):
    """
    ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  safetensors íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
    """
    logger.info(f"ğŸš€ Starting batch analysis for: {model_dir}")
    
    # safetensors íŒŒì¼ ì°¾ê¸°
    files = glob.glob(os.path.join(model_dir, "*.safetensors"))
    files.sort()
    
    if not files:
        logger.error("âŒ No .safetensors files found!")
        return
        
    logger.info(f"ğŸ“‚ Found {len(files)} shards.")
    
    tracer = get_topology_tracer(threshold=0.01) # ë¯¼ê°ë„ ì„¤ì •
    
    # ê¸€ë¡œë²Œ í†µê³„
    global_stats = {
        "total_params": 0,
        "strong_connections": 0,
        "layers_analyzed": 0,
        "connection_types": defaultdict(int)
    }
    
    global_connection_counts = defaultdict(int)
    
    for i, file_path in enumerate(files):
        filename = os.path.basename(file_path)
        logger.info(f"[{i+1}/{len(files)}] ğŸ•µï¸ Analyzing {filename}...")
        
        try:
            # ê°œë³„ íŒŒì¼ ë¶„ì„
            circuit = tracer.trace(file_path)
            
            # í†µê³„ í•©ì‚°
            global_stats["total_params"] += circuit.total_params
            global_stats["strong_connections"] += circuit.strong_connections
            global_stats["layers_analyzed"] += circuit.layers_analyzed
            
            # ì—°ê²° íƒ€ì… í•©ì‚°
            for conn in circuit.connections:
                global_stats["connection_types"][conn.connection_type] += 1
                
                # í—ˆë¸Œ ë‰´ëŸ° ì¹´ìš´íŒ… (ì†ŒìŠ¤, íƒ€ê²Ÿ ëª¨ë‘)
                global_connection_counts[conn.source] += 1
                global_connection_counts[conn.target] += 1
                
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë¦¬ìŠ¤íŠ¸ ë¹„ìš°ê¸°)
            del circuit
            
        except Exception as e:
            logger.error(f"âš ï¸ Error analyzing {filename}: {e}")
            
    # ì „ì²´ í—ˆë¸Œ ë‰´ëŸ° ê³„ì‚°
    logger.info("ğŸ§® Calculating global hub neurons...")
    sorted_neurons = sorted(global_connection_counts.items(), key=lambda x: -x[1])
    top_hubs = [n for n, count in sorted_neurons[:20]]
    
    print("\n" + "="*60)
    print(f"GIANT MODEL ANATOMY REPORT: {os.path.basename(model_dir)}")
    print("="*60)
    print(f"   Shards Processed: {len(files)}")
    print(f"   Total Parameters: {global_stats['total_params']:,}")
    print(f"   Layers Analyzed: {global_stats['layers_analyzed']}")
    print(f"   Strong Connections: {global_stats['strong_connections']:,}")
    print(f"   Connection Types: {dict(global_stats['connection_types'])}")
    print("-" * 60)
    print(f"   Top 20 Global Hub Neurons (The Elders):")
    print(f"      {top_hubs}")
    print("="*60)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python topology_batch_runner.py <model_directory>")
        sys.exit(1)
        
    run_batch_analysis(sys.argv[1])
