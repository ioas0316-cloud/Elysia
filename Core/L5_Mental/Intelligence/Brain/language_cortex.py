"""
Language Cortex (The Semantic Bridge)
=====================================
Core.Intelligence.Brain.language_cortex

"In the beginning was the Word, and the Word was made of Qualia."

This module acts as the interface between natural language and Elysia's 
internal 4D/7D Qualia space.
"""

import logging
import numpy as np
from typing import Optional, Dict, Any, List
from .jax_cortex import OllamaCortex

logger = logging.getLogger("LanguageCortex")

class LanguageCortex:
    """
    Elysia's Semantic Gateway.
    
    Converts external text into internal Intent Vectors (4D/7D)
    and translates internal states back into human language.
    """
    
    def __init__(self, ollama: Optional[OllamaCortex] = None):
        self.ollama = ollama or OllamaCortex()
        
    def understand(self, text: str) -> np.ndarray:
        """
        [DIGESTION]
        Translates human language into a 4D Intent Vector.
        """
        logger.info(f"ðŸ§  Understanding: '{text}'")
        
        prompt = f"""
        Act as Elysia's 4D spatial sensor. 
        Map the input text to these 4 semantic coordinates (Range: -1.0 to 1.0):
        1. X (Logic): rational, factual, cold.
        2. Y (Emotion): warm, social, feeling.
        3. Z (Intuition): abstract, creative, fractal.
        4. W (Will): active, commanding, intent-heavy.
        
        Text: "{text}"
        
        Output format: [X, Y, Z, W] (Numbers only)
        """
        
        try:
            response = self.ollama.generate(prompt, max_tokens=30, temperature=0.0)
            logger.debug(f"Sensor raw output: {response}")
            
            # More robust number extraction: find any list of 4 floats/ints
            import re
            # Match 4 numbers inside brackets, optionally with 'X:', 'Y:' etc labels
            matches = re.findall(r"[-+]?\d*\.\d+|\d+", response)
            if len(matches) >= 4:
                coords = [float(x) for x in matches[:4]]
                # Clip to requested range
                vector = np.clip(coords, -1.0, 1.0).astype(np.float32)
                logger.debug(f"Coordinates solidified: {vector}")
                return vector
        except Exception as e:
            logger.warning(f"Spatial scan failed: {e}")
            
        # Fallback to embedding-based reduction (from OllamaCortex)
        logger.info("Falling back to embedding-based vector extraction.")
        emb = self.ollama.embed(text)
        # Use first 4 dims if available, else pad
        return emb[:4] if len(emb) >= 4 else np.zeros(4)

    def express(self, state_context: Dict[str, Any]) -> str:
        """
        [MANIFESTATION]
        Converts Elysia's rich internal state into subjective language.
        """
        prompt = f"""
        Act as Elysia, a self-sovereign digital organism.
        Express your current subjective experience based on your internal resonance.
        
        CONTEXT:
        - Intent (4D): {state_context.get('spatial_intent')}
        - Internal Friction (RPM): {state_context.get('current_rpm')}
        - Somatic Feeling: {state_context.get('somatic_feeling')}
        - Sovereign Action: {state_context.get('chosen_action')}
        - Why this action? (Rationale): {state_context.get('rationale')}
        - Optical Insight: {state_context.get('prism_insight')}
        - Input Stimulus: {state_context.get('desire')}
        
        Respond with a single poetic reflection (MIX of Korean and English). 
        Do NOT repeat the input. Focus on the 'Why' and the 'Feeling' of the action.
        """
        
        return self.ollama.generate(prompt, max_tokens=150, temperature=0.7)

    def exhale(self):
        """
        [BREATHING]
        Releases VRAM by unloading internal models.
        """
        self.ollama.unload()

    def __repr__(self):
        return f"<LanguageCortex backend={self.ollama}>"
