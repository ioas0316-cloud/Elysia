"""
Wikipedia Dump Parser (ìœ„í‚¤í”¼ë””ì•„ ë¤í”„ íŒŒì„œ)
=============================================

"ì§„ì§œ ì§€ì‹ì˜ ëŒ€ëŸ‰ í¡ìˆ˜ - í¬ë¡¤ë§ ì—†ì´ ë¡œì»¬ì—ì„œ"

Wikipedia XML ë¤í”„ íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±í•˜ì—¬
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€ëŸ‰ í¡ìˆ˜

ë‹¤ìš´ë¡œë“œ: https://dumps.wikimedia.org/kowiki/latest/
íŒŒì¼ëª…: kowiki-latest-pages-articles.xml.bz2

[NEW 2025-12-16] ì§„ì§œ ë°ì´í„° íŒŒì„œ
"""

import os
import sys
import bz2
import re
import logging
from pathlib import Path
from typing import Generator, Dict, Any, Optional
import xml.etree.ElementTree as ET
from html import unescape

sys.path.insert(0, "c:\\Elysia")

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger("WikipediaDumpParser")


class WikipediaDumpParser:
    """
    Wikipedia XML ë¤í”„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì„œ
    
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì : ì „ì²´ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì§€ ì•ŠìŒ
    ìŠ¤íŠ¸ë¦¬ë°: ë¬¸ì„œë¥¼ í•˜ë‚˜ì”© yield
    """
    
    def __init__(self, dump_path: str):
        self.dump_path = Path(dump_path)
        self.namespace = "{http://www.mediawiki.org/xml/export-0.11/}"
        
        if not self.dump_path.exists():
            raise FileNotFoundError(f"Dump file not found: {dump_path}")
        
        logger.info(f"ğŸ“š Wikipedia dump parser initialized: {dump_path}")
        
        # í†µê³„
        self.total_parsed = 0
        self.skipped_redirects = 0
        self.skipped_special = 0
    
    def _clean_wikitext(self, text: str) -> str:
        """
        ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë§ˆí¬ì—… ì œê±°
        
        ì™„ë²½í•˜ì§€ ì•Šì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ë‚´ìš© ì¶”ì¶œ
        """
        if not text:
            return ""
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = unescape(text)
        
        # ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²´í¬
        if text.strip().lower().startswith("#redirect") or text.strip().startswith("#ë„˜ê²¨ì£¼ê¸°"):
            return ""
        
        # ì œê±°í•  íŒ¨í„´ë“¤
        patterns = [
            (r'\{\{[^}]*\}\}', ''),           # í…œí”Œë¦¿ {{ }}
            (r'\[\[íŒŒì¼:[^\]]*\]\]', ''),      # íŒŒì¼ ë§í¬
            (r'\[\[File:[^\]]*\]\]', ''),      # File links
            (r'\[\[Category:[^\]]*\]\]', ''),  # ì¹´í…Œê³ ë¦¬
            (r'\[\[ë¶„ë¥˜:[^\]]*\]\]', ''),      # í•œê¸€ ì¹´í…Œê³ ë¦¬
            (r'\[\[([^\]|]*)\|([^\]]*)\]\]', r'\2'),  # [[ë§í¬|í…ìŠ¤íŠ¸]] â†’ í…ìŠ¤íŠ¸
            (r'\[\[([^\]]*)\]\]', r'\1'),     # [[ë§í¬]] â†’ ë§í¬
            (r"'''([^']*?)'''", r'\1'),       # ë³¼ë“œ
            (r"''([^']*?)''", r'\1'),         # ì´íƒ¤ë¦­
            (r'<ref[^>]*>.*?</ref>', ''),     # ì°¸ì¡°
            (r'<ref[^/>]*/>', ''),            # ë‹¨ì¼ ì°¸ì¡°
            (r'<[^>]+>', ''),                 # HTML íƒœê·¸
            (r'\{\|[^}]*\|\}', ''),           # í…Œì´ë¸”
            (r'^\*+\s*', '', re.MULTILINE),   # ë¦¬ìŠ¤íŠ¸ ë§ˆì»¤
            (r'^#+\s*', '', re.MULTILINE),    # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
            (r'^=+\s*([^=]+)\s*=+', r'\1', re.MULTILINE),  # í—¤ë”
            (r'\n{3,}', '\n\n'),              # ê³¼ë‹¤ ì¤„ë°”ê¿ˆ
        ]
        
        for pattern, replacement, *flags in patterns:
            flag = flags[0] if flags else 0
            text = re.sub(pattern, replacement, text, flags=flag)
        
        # ê³µë°± ì •ë¦¬
        text = ' '.join(text.split())
        
        return text.strip()
    
    def _is_valid_article(self, title: str) -> bool:
        """ìœ íš¨í•œ ë¬¸ì„œì¸ì§€ í™•ì¸ (íŠ¹ìˆ˜ í˜ì´ì§€ ì œì™¸)"""
        invalid_prefixes = [
            "ìœ„í‚¤ë°±ê³¼:", "Wikipedia:", "í‹€:", "Template:",
            "ë¶„ë¥˜:", "Category:", "íŒŒì¼:", "File:",
            "ë„ì›€ë§:", "Help:", "ì‚¬ìš©ì:", "User:",
            "í† ë¡ :", "Talk:", "ëª¨ë“ˆ:", "Module:"
        ]
        
        for prefix in invalid_prefixes:
            if title.startswith(prefix):
                return False
        
        return True
    
    def stream_articles(self, max_articles: int = None, min_length: int = 100) -> Generator[Dict[str, str], None, None]:
        """
        ë¬¸ì„œ ìŠ¤íŠ¸ë¦¬ë°
        
        max_articles: ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (None=ì „ì²´)
        min_length: ìµœì†Œ ë³¸ë¬¸ ê¸¸ì´
        
        Yields: {"title": str, "content": str}
        """
        logger.info(f"ğŸ”„ Starting to stream articles (max: {max_articles or 'unlimited'})...")
        
        # bz2 ì••ì¶• ë˜ëŠ” ì¼ë°˜ XML
        if str(self.dump_path).endswith('.bz2'):
            file_handle = bz2.open(self.dump_path, 'rt', encoding='utf-8')
        else:
            file_handle = open(self.dump_path, 'r', encoding='utf-8')
        
        try:
            # ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±
            context = ET.iterparse(file_handle, events=('end',))
            
            for event, elem in context:
                # <page> íƒœê·¸ ì™„ë£Œ ì‹œ
                if elem.tag == f"{self.namespace}page" or elem.tag == "page":
                    title_elem = elem.find(f"{self.namespace}title") or elem.find("title")
                    text_elem = elem.find(f".//{self.namespace}text") or elem.find(".//text")
                    
                    if title_elem is not None and text_elem is not None:
                        title = title_elem.text or ""
                        raw_text = text_elem.text or ""
                        
                        # ìœ íš¨ì„± ê²€ì‚¬
                        if not self._is_valid_article(title):
                            self.skipped_special += 1
                            elem.clear()
                            continue
                        
                        # ìœ„í‚¤í…ìŠ¤íŠ¸ ì •ì œ
                        content = self._clean_wikitext(raw_text)
                        
                        # ë¦¬ë‹¤ì´ë ‰íŠ¸ ìŠ¤í‚µ
                        if not content:
                            self.skipped_redirects += 1
                            elem.clear()
                            continue
                        
                        # ìµœì†Œ ê¸¸ì´ ì²´í¬
                        if len(content) < min_length:
                            elem.clear()
                            continue
                        
                        self.total_parsed += 1
                        
                        # ì§„í–‰ ë¡œê·¸
                        if self.total_parsed % 1000 == 0:
                            logger.info(f"   ğŸ“„ Parsed {self.total_parsed} articles...")
                        
                        yield {
                            "title": title,
                            "content": content[:2000]  # ìµœëŒ€ 2000ì
                        }
                        
                        # ìµœëŒ€ ë¬¸ì„œ ìˆ˜ ì²´í¬
                        if max_articles and self.total_parsed >= max_articles:
                            break
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    elem.clear()
                    
        finally:
            file_handle.close()
        
        logger.info(f"âœ… Parsing complete: {self.total_parsed} articles")
        logger.info(f"   Skipped: {self.skipped_redirects} redirects, {self.skipped_special} special pages")
    
    def absorb_to_universe(self, max_articles: int = 1000, batch_size: int = 100) -> Dict[str, int]:
        """
        Wikipedia ë¤í”„ë¥¼ InternalUniverseì— ì§ì ‘ í¡ìˆ˜
        """
        from Core.Autonomy.local_dataset_absorber import get_local_absorber
        
        absorber = get_local_absorber(batch_size=batch_size)
        
        results = {"total": 0, "absorbed": 0, "isolated": 0, "failed": 0}
        batch = []
        
        for article in self.stream_articles(max_articles=max_articles):
            batch.append(article)
            results["total"] += 1
            
            if len(batch) >= batch_size:
                # ë°°ì¹˜ í¡ìˆ˜
                if absorber.universe:
                    batch_result = absorber.universe.absorb_batch(batch)
                    results["absorbed"] += batch_result.get("absorbed", 0)
                    results["isolated"] += batch_result.get("isolated", 0)
                batch = []
        
        # ë‚¨ì€ ë°°ì¹˜
        if batch and absorber.universe:
            batch_result = absorber.universe.absorb_batch(batch)
            results["absorbed"] += batch_result.get("absorbed", 0)
            results["isolated"] += batch_result.get("isolated", 0)
        
        logger.info(f"ğŸ‰ Wikipedia absorption complete!")
        logger.info(f"   Total: {results['total']}, Absorbed: {results['absorbed']}, Isolated: {results['isolated']}")
        
        return results


# CLI
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Wikipedia Dump Parser")
    parser.add_argument("dump_path", help="Path to Wikipedia XML dump (can be .bz2)")
    parser.add_argument("--max", type=int, default=1000, help="Max articles to process")
    parser.add_argument("--batch", type=int, default=100, help="Batch size")
    parser.add_argument("--preview", action="store_true", help="Preview first 5 articles only")
    
    args = parser.parse_args()
    
    try:
        wiki_parser = WikipediaDumpParser(args.dump_path)
        
        if args.preview:
            print("\n" + "="*60)
            print("ğŸ“– PREVIEW MODE - First 5 articles")
            print("="*60)
            
            for i, article in enumerate(wiki_parser.stream_articles(max_articles=5)):
                print(f"\n--- {article['title']} ---")
                print(article['content'][:300] + "...")
                
        else:
            print("\n" + "="*60)
            print("ğŸ§  ABSORBING TO INTERNAL UNIVERSE")
            print("="*60)
            
            results = wiki_parser.absorb_to_universe(
                max_articles=args.max,
                batch_size=args.batch
            )
            
            print(f"\nâœ… Done! Absorbed {results['absorbed']} articles from Wikipedia")
            
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        print("\nğŸ’¡ Download Wikipedia dump from:")
        print("   Korean: https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2")
        print("   English: https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2")
