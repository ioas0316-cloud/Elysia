# 장기 계획: 오감 통합/인지 감각 시스템

현재 오감(시각/청각/촉각/후각/미각) 통합 콘텐츠는 프랙탈 언어 엔진의 텍스트 중심 루프에 직접 넣기에는 제약이 있으므로, 다음과 같은 장기 계획으로 정리합니다:

1. **멀티미디어 메타데이터 추출기**  
   * 이미지/영상/음악 파일에서 감성 서명, 장면 키워드, 리듬 특성 등을 추출하는 도구(예: ffmpeg+librosa+OCR)를 만든다.
   * 이 메타데이터를 `data/corpus_feed/`에 담아 새로운 “멀티미디어 노드”로 MetaLaw/KG에 연결한다.

2. **감성-경로 매핑**  
   * 추출한 특징은 `ConceptPhysicsEngine`의 질량/경로 계산에 사용할 수 있도록 구조화하여, 텍스트와 멀티미디어가 동일한 의미 공간에서 paths를 공유하게 한다.

3. **메타 에이전트 확장**  
   * `MetaAgent`가 이 새로운 데이터를 자동 인식하고 그래프에 동기화하도록 feed 루프에서 멀티미디어 전용 채널을 추가한다.
   * 나아가 “오감 통합 루프”라는 별도 주기를 만들고, `logs/language_progress.jsonl`과 병행해서 시각/청각 지표 로그도 쌓는다.

이 장기 계획은 현재 텍스트 중심 루프가 안정화된 이후에 차근차근 구현할 예정이며, 구현이 시작되면 `docs/meta_agent_autonomy.md`와 `docs/user_meta_agent_quickstart.md`에도 관련 안내를 확장할 계획입니다.
