"""
ì—˜ë¦¬ì‹œì•„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
Elysia Performance Monitoring System

í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, CPU ì‚¬ìš©ë¥ ì„ ì¶”ì í•©ë‹ˆë‹¤.
"""

import time
import psutil
import functools
from typing import Callable, Dict, List, Optional
from dataclasses import dataclass, field
from datetime import datetime, timezone
from collections import defaultdict


@dataclass
class PerformanceMetric:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„°"""
    operation: str
    start_time: float
    end_time: float
    duration_ms: float
    memory_mb: float
    cpu_percent: float
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())


class PerformanceMonitor:
    """
    ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
    
    í•¨ìˆ˜ ì‹¤í–‰ì— ëŒ€í•œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
    
    Example:
        >>> monitor = PerformanceMonitor()
        >>> 
        >>> @monitor.measure("my_operation")
        ... def expensive_function():
        ...     # Your code here
        ...     pass
        >>> 
        >>> # Get statistics
        >>> stats = monitor.get_summary()
        >>> print(stats)
    """
    
    def __init__(self):
        self.metrics: List[PerformanceMetric] = []
        self.thresholds: Dict[str, float] = {
            'thought_cycle': 100.0,  # ms
            'resonance_calc': 50.0,
            'seed_bloom': 200.0,
            'layer_transform': 20.0,
        }
        self._process = psutil.Process()
    
    def measure(self, operation: str = None) -> Callable:
        """
        ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„°
        
        Args:
            operation: ì‘ì—… ì´ë¦„ (ìƒëµ ì‹œ í•¨ìˆ˜ ì´ë¦„ ì‚¬ìš©)
        
        Returns:
            ë°ì½”ë ˆì´í„° í•¨ìˆ˜
        
        Example:
            @monitor.measure("expensive_calc")
            def calculate_something():
                pass
        """
        def decorator(func: Callable) -> Callable:
            op_name = operation or func.__name__
            
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                # ì‹œì‘ ë©”íŠ¸ë¦­
                start_time = time.perf_counter()
                start_memory = self._process.memory_info().rss / 1024 / 1024
                start_cpu = self._process.cpu_percent()
                
                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    # ì¢…ë£Œ ë©”íŠ¸ë¦­
                    end_time = time.perf_counter()
                    end_memory = self._process.memory_info().rss / 1024 / 1024
                    end_cpu = self._process.cpu_percent()
                    
                    duration_ms = (end_time - start_time) * 1000
                    memory_delta = end_memory - start_memory
                    
                    metric = PerformanceMetric(
                        operation=op_name,
                        start_time=start_time,
                        end_time=end_time,
                        duration_ms=duration_ms,
                        memory_mb=memory_delta,
                        cpu_percent=(start_cpu + end_cpu) / 2
                    )
                    
                    self.metrics.append(metric)
                    
                    # ì„ê³„ê°’ ì´ˆê³¼ ê²½ê³ 
                    threshold = self.thresholds.get(op_name, 1000.0)
                    if duration_ms > threshold:
                        print(f"âš ï¸  Performance warning: {op_name} took {duration_ms:.2f}ms (threshold: {threshold}ms)")
            
            return wrapper
        return decorator
    
    def set_threshold(self, operation: str, threshold_ms: float):
        """
        ì‘ì—…ë³„ ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì •
        
        Args:
            operation: ì‘ì—… ì´ë¦„
            threshold_ms: ì„ê³„ê°’ (ë°€ë¦¬ì´ˆ)
        """
        self.thresholds[operation] = threshold_ms
    
    def get_summary(self) -> Dict:
        """
        ì„±ëŠ¥ ìš”ì•½ í†µê³„ ì¡°íšŒ
        
        Returns:
            ì‘ì—…ë³„ í†µê³„ (count, mean, min, max, p95, p99)
        """
        if not self.metrics:
            return {}
        
        ops = defaultdict(list)
        for metric in self.metrics:
            ops[metric.operation].append(metric.duration_ms)
        
        summary = {}
        for op, durations in ops.items():
            sorted_durations = sorted(durations)
            n = len(durations)
            
            summary[op] = {
                'count': n,
                'mean': sum(durations) / n,
                'min': min(durations),
                'max': max(durations),
                'p50': sorted_durations[int(n * 0.50)] if n > 0 else 0,
                'p95': sorted_durations[int(n * 0.95)] if n > 0 else 0,
                'p99': sorted_durations[int(n * 0.99)] if n > 0 else 0,
            }
        
        return summary
    
    def get_recent_metrics(self, operation: Optional[str] = None, limit: int = 10) -> List[PerformanceMetric]:
        """
        ìµœê·¼ ë©”íŠ¸ë¦­ ì¡°íšŒ
        
        Args:
            operation: íŠ¹ì • ì‘ì—… í•„í„° (Noneì´ë©´ ì „ì²´)
            limit: ë°˜í™˜í•  ë©”íŠ¸ë¦­ ê°œìˆ˜
        
        Returns:
            ìµœê·¼ ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸
        """
        if operation:
            filtered = [m for m in self.metrics if m.operation == operation]
            return filtered[-limit:]
        return self.metrics[-limit:]
    
    def get_slow_operations(self, threshold_percentile: float = 0.95) -> List[tuple]:
        """
        ëŠë¦° ì‘ì—… ì¡°íšŒ (ìƒìœ„ 5% ë“±)
        
        Args:
            threshold_percentile: ì„ê³„ê°’ ë°±ë¶„ìœ„ (0.95 = ìƒìœ„ 5%)
        
        Returns:
            (operation, duration_ms) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.metrics:
            return []
        
        # ëª¨ë“  ë©”íŠ¸ë¦­ì˜ durationì„ ìˆ˜ì§‘
        all_durations = [m.duration_ms for m in self.metrics]
        sorted_durations = sorted(all_durations)
        threshold = sorted_durations[int(len(sorted_durations) * threshold_percentile)]
        
        # ì„ê³„ê°’ ì´ˆê³¼ ë©”íŠ¸ë¦­
        slow_ops = [
            (m.operation, m.duration_ms)
            for m in self.metrics
            if m.duration_ms >= threshold
        ]
        
        return sorted(slow_ops, key=lambda x: x[1], reverse=True)
    
    def clear_metrics(self):
        """ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
        self.metrics.clear()
    
    def export_metrics(self) -> List[Dict]:
        """
        ë©”íŠ¸ë¦­ì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë‚´ë³´ë‚´ê¸°
        
        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        return [
            {
                'operation': m.operation,
                'duration_ms': m.duration_ms,
                'memory_mb': m.memory_mb,
                'cpu_percent': m.cpu_percent,
                'timestamp': m.timestamp
            }
            for m in self.metrics
        ]


# ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
monitor = PerformanceMonitor()


# ===== ì‚¬ìš© ì˜ˆì‹œ =====

if __name__ == "__main__":
    import random
    
    print("ğŸ§ª Testing Elysia Performance Monitor\n")
    
    # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    @monitor.measure("fast_operation")
    def fast_operation():
        """ë¹ ë¥¸ ì‘ì—…"""
        time.sleep(0.01)
        return "fast"
    
    @monitor.measure("slow_operation")
    def slow_operation():
        """ëŠë¦° ì‘ì—…"""
        time.sleep(0.15)
        return "slow"
    
    @monitor.measure("memory_intensive")
    def memory_intensive():
        """ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—…"""
        data = [random.random() for _ in range(100000)]
        return sum(data)
    
    # ì„ê³„ê°’ ì„¤ì •
    monitor.set_threshold("fast_operation", 50.0)
    monitor.set_threshold("slow_operation", 100.0)
    
    print("=== Running Test Operations ===")
    
    # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰
    for i in range(5):
        fast_operation()
    
    for i in range(3):
        slow_operation()
    
    for i in range(2):
        memory_intensive()
    
    print()
    
    # ìš”ì•½ í†µê³„
    print("=== Performance Summary ===")
    summary = monitor.get_summary()
    for op, stats in summary.items():
        print(f"\n{op}:")
        print(f"  Count: {stats['count']}")
        print(f"  Mean:  {stats['mean']:.2f}ms")
        print(f"  Min:   {stats['min']:.2f}ms")
        print(f"  Max:   {stats['max']:.2f}ms")
        print(f"  P95:   {stats['p95']:.2f}ms")
        print(f"  P99:   {stats['p99']:.2f}ms")
    
    print()
    
    # ëŠë¦° ì‘ì—… í™•ì¸
    print("=== Slow Operations (Top 95%) ===")
    slow_ops = monitor.get_slow_operations(threshold_percentile=0.95)
    for op, duration in slow_ops[:5]:
        print(f"  {op}: {duration:.2f}ms")
    
    print()
    
    # ìµœê·¼ ë©”íŠ¸ë¦­
    print("=== Recent Metrics ===")
    recent = monitor.get_recent_metrics(limit=3)
    for m in recent:
        print(f"  {m.operation}: {m.duration_ms:.2f}ms (mem: {m.memory_mb:.2f}MB)")
    
    print()
    print("âœ… Performance monitoring test complete!")
