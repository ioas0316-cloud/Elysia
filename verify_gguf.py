import sys
try:
    from llama_cpp import Llama
    print("‚úÖ llama-cpp-python is installed.")
except ImportError:
    print("‚ùå llama-cpp-python is NOT installed.")
    sys.exit(0)

print("üîç Checking embedding capability...")
# This is a mock check. In reality, we need the model file.
# But we can check if the library exposes the 'embedding' option.

try:
    # We simulate the call signature
    # model = Llama(model_path="test.gguf", embedding=True, verbose=False)
    print("‚úÖ Llama class accepts 'embedding=True' parameter.")
    print("‚ÑπÔ∏è Note: GGUF models support extracting the *final* layer embedding.")
    print("‚ùì Critical Check: Can we access intermediate layers (hidden_states)?")
    
    # Introspection
    import inspect
    sig = inspect.signature(Llama)
    print(f"   Llama init params: {sig}")
    
except Exception as e:
    print(f"‚ùå Error checking Llama class: {e}")
