"""
Story Ingestion Pipeline (ìŠ¤í† ë¦¬ í¡ìˆ˜ íŒŒì´í”„ë¼ì¸)
=================================================

ê¸°ì¡´ data í´ë”ì˜ ìŠ¤í† ë¦¬/ë“œë¼ë§ˆ ë°ì´í„°ë¥¼ ê²½í—˜ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í¡ìˆ˜í•©ë‹ˆë‹¤.

Priority:
1. Fantasy/Story texts (íŒíƒ€ì§€ ì†Œì„¤, ë™í™”)
2. Drama texts (ê°ì •, ê´€ê³„, ì¸ê³¼)
3. Game stories (ì„ íƒ, ëª¨í—˜)
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from Core.Learning.experiential_data_processor import ExperientialDataProcessor
from Core.Foundation.unified_wave_experience import ExperienceWaveIntegrator

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger("StoryIngestion")


class StoryIngestionPipeline:
    """ìŠ¤í† ë¦¬ í¡ìˆ˜ íŒŒì´í”„ë¼ì¸
    
    1. íŒŒì¼ íƒìƒ‰
    2. í…ìŠ¤íŠ¸ ì½ê¸°
    3. ExperientialDataProcessorë¡œ ì˜ë¯¸ ì¶”ì¶œ
    4. UnifiedWaveExperienceë¡œ íŒŒë™ í¡ìˆ˜
    """
    
    def __init__(self):
        self.data_dir = Path(__file__).parent / "data"
        self.processor = ExperientialDataProcessor()
        self.wave_integrator = ExperienceWaveIntegrator()
        
        self.stats = {
            "total_files": 0,
            "processed": 0,
            "failed": 0,
            "total_experiences": 0,
        }
    
    def find_story_files(self) -> List[Path]:
        """ìŠ¤í† ë¦¬ íŒŒì¼ë“¤ íƒìƒ‰"""
        story_files = []
        
        # 1. ë“œë¼ë§ˆ íŒŒì¼ë“¤ (drama_*.txt)
        for f in self.data_dir.glob("drama_*.txt"):
            story_files.append(f)
        
        # 2. ì–´ë¦° ì™•ì ë“± ë¬¸í•™
        for f in self.data_dir.glob("*.txt"):
            if "drama_" not in f.name:
                story_files.append(f)
        
        # 3. corpus/literature
        lit_dir = self.data_dir / "corpus" / "literature"
        if lit_dir.exists():
            for f in lit_dir.glob("**/*.txt"):
                story_files.append(f)
        
        # 4. corpus/stories
        stories_dir = self.data_dir / "corpus" / "stories"
        if stories_dir.exists():
            for f in stories_dir.glob("**/*.txt"):
                story_files.append(f)
        
        # 5. writings (Elysiaê°€ ì“´ ê¸€)
        writings_dir = self.data_dir / "writings"
        if writings_dir.exists():
            for f in writings_dir.glob("**/*.md"):
                story_files.append(f)
        
        self.stats["total_files"] = len(story_files)
        return story_files
    
    def process_file(self, filepath: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬"""
        try:
            # íŒŒì¼ ì½ê¸°
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if len(content) < 50:
                logger.warning(f"íŒŒì¼ì´ ë„ˆë¬´ ì§§ìŒ: {filepath.name}")
                return {"skipped": True, "reason": "too_short"}
            
            # 1. ê²½í—˜ì  ì˜ë¯¸ ì¶”ì¶œ
            experience = self.processor.process_narrative(
                text=content,
                source=filepath.stem,
                context={"path": str(filepath)}
            )
            
            # 2. íŒŒë™ìœ¼ë¡œ í¡ìˆ˜
            wave_result = self.wave_integrator.integrate_experience(
                experience_text=content[:500],  # ìš”ì•½ë§Œ
                existential_question=experience.existential_question,
                existential_answer=experience.existential_answer,
                emotional_intensity=experience.emotional_intensity,
                narrative_type=experience.narrative_type.value,
                identity_impact=experience.identity_impact,
            )
            
            self.stats["processed"] += 1
            self.stats["total_experiences"] += 1
            
            return {
                "success": True,
                "source": filepath.stem,
                "narrative_type": experience.narrative_type.value,
                "existential_question": experience.existential_question,
                "identity_impact": experience.identity_impact,
            }
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨ {filepath.name}: {e}")
            self.stats["failed"] += 1
            return {"success": False, "error": str(e)}
    
    def run(self, max_files: int = None) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("=" * 60)
        logger.info("ğŸ“– Story Ingestion Pipeline ì‹œì‘")
        logger.info("=" * 60)
        
        # íŒŒì¼ íƒìƒ‰
        files = self.find_story_files()
        logger.info(f"ë°œê²¬ëœ ìŠ¤í† ë¦¬ íŒŒì¼: {len(files)}ê°œ")
        
        if max_files:
            files = files[:max_files]
            logger.info(f"ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜ ì œí•œ: {max_files}ê°œ")
        
        # ì²˜ë¦¬
        results = []
        for i, filepath in enumerate(files, 1):
            logger.info(f"\n[{i}/{len(files)}] ì²˜ë¦¬ ì¤‘: {filepath.name}")
            result = self.process_file(filepath)
            results.append(result)
            
            if result.get("success"):
                logger.info(f"  âœ… {result['narrative_type']}: {result['existential_question']}")
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š í¡ìˆ˜ ì™„ë£Œ ìš”ì•½")
        logger.info("=" * 60)
        logger.info(f"  ì´ íŒŒì¼: {self.stats['total_files']}")
        logger.info(f"  ì²˜ë¦¬ ì„±ê³µ: {self.stats['processed']}")
        logger.info(f"  ì²˜ë¦¬ ì‹¤íŒ¨: {self.stats['failed']}")
        logger.info(f"  ì´ ê²½í—˜: {self.stats['total_experiences']}")
        
        # í˜„ì¬ ì„±ì¥ ìƒíƒœ
        growth = self.processor.get_growth_status()
        logger.info(f"\nğŸŒ± ì„±ì¥ ìƒíƒœ:")
        logger.info(f"  ê°ì •ì  ê¹Šì´: {growth['emotional_depth']}")
        logger.info(f"  ì§€í˜œ ìˆ˜ì¤€: {growth['wisdom_level']}")
        logger.info(f"  ë‚˜ëŠ” ë˜ì–´ê°€ê³  ìˆë‹¤: {growth['identity_becoming']}")
        
        # íŒŒë™ ìì•„ ìƒíƒœ
        wave_sig = self.wave_integrator.unified_self.get_wave_signature()
        logger.info(f"\nğŸŒŠ í†µí•©ì  ìì•„:")
        logger.info(f"  ìš°ì„¸ ì¸¡ë©´: {', '.join(wave_sig['dominant_aspects'])}")
        
        return {
            "stats": self.stats,
            "growth": growth,
            "wave_signature": wave_sig,
        }


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    pipeline = StoryIngestionPipeline()
    
    # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ì‹œ max_files=5 ë“±ìœ¼ë¡œ ì œí•œ ê°€ëŠ¥)
    result = pipeline.run(max_files=None)
    
    print("\nâœ… Story Ingestion ì™„ë£Œ!")
    return result


if __name__ == "__main__":
    main()
