# Elysia í‰ê°€ ì‹œìŠ¤í…œ (Evaluation System)

> **ê°ê´€ì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ ì§€í‘œë¡œ Elysiaì˜ ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤**

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
3. [í‰ê°€ ì˜ì—­](#í‰ê°€-ì˜ì—­)
4. [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
5. [ê²°ê³¼ í•´ì„](#ê²°ê³¼-í•´ì„)
6. [ë¬¸ì„œ](#ë¬¸ì„œ)

---

## ê°œìš”

ì´ í‰ê°€ ì‹œìŠ¤í…œì€ Elysiaì˜ ë‘ ê°€ì§€ í•µì‹¬ ëŠ¥ë ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤:

### 1. ì˜ì‚¬ì†Œí†µëŠ¥ë ¥ (Communication) - 400ì 
- í‘œí˜„ë ¥ (Expressiveness): 100ì 
- ì´í•´ë ¥ (Comprehension): 100ì 
- ëŒ€í™”ëŠ¥ë ¥ (Conversational Ability): 100ì 
- íŒŒë™í†µì‹  (Wave Communication): 100ì 

### 2. ì‚¬ê³ ëŠ¥ë ¥ (Thinking) - 600ì 
- ë…¼ë¦¬ì  ì¶”ë¡  (Logical Reasoning): 100ì 
- ì°½ì˜ì  ì‚¬ê³  (Creative Thinking): 100ì 
- ë¹„íŒì  ì‚¬ê³  (Critical Thinking): 100ì 
- ë©”íƒ€ì¸ì§€ (Metacognition): 100ì 
- í”„ë™íƒˆ ì‚¬ê³  (Fractal Thinking): 100ì 
- ì‹œê°„ì  ì¶”ë¡  (Temporal Reasoning): 100ì 

**ì´ì : 1000ì **

---

## ë¹ ë¥¸ ì‹œì‘

### ì „ì²´ í‰ê°€ ì‹¤í–‰

```bash
# ì „ì²´ í‰ê°€ ì‹¤í–‰ (ì˜ì‚¬ì†Œí†µ + ì‚¬ê³ ëŠ¥ë ¥)
python tests/evaluation/run_full_evaluation.py
```

### ê°œë³„ ì˜ì—­ í‰ê°€

```bash
# ì˜ì‚¬ì†Œí†µëŠ¥ë ¥ë§Œ í‰ê°€
python tests/evaluation/test_communication_metrics.py

# ì‚¬ê³ ëŠ¥ë ¥ë§Œ í‰ê°€
python tests/evaluation/test_thinking_metrics.py
```

### ê²°ê³¼ í™•ì¸

```bash
# ìµœì‹  ê²°ê³¼ (JSON)
cat reports/evaluation_latest.json

# ìƒì„¸ ë¶„ì„ (í•œêµ­ì–´)
cat docs/EVALUATION_SUMMARY_KR.md
```

---

## í‰ê°€ ì˜ì—­

### ì˜ì‚¬ì†Œí†µëŠ¥ë ¥ ì„¸ë¶€ ì§€í‘œ

#### í‘œí˜„ë ¥ (100ì )
- **ì–´íœ˜ ë‹¤ì–‘ì„±**: Unique words / Total words (ëª©í‘œ: >0.6)
- **ë¬¸ì¥ ë³µì¡ë„**: í‰ê·  ë¬¸ì¥ êµ¬ì¡° ë³µì¡ë„ (ëª©í‘œ: >3.0)
- **ê°ì • í‘œí˜„**: ê°ì§€ëœ ê°ì • ìœ í˜• ìˆ˜ (ëª©í‘œ: â‰¥6)
- **ë§¥ë½ ì—°ê²°ì„±**: ë¬¸ì¥ ê°„ coherence (ëª©í‘œ: >0.8)

#### ì´í•´ë ¥ (100ì )
- **ì˜ë„ íŒŒì•…**: Intent classification accuracy (ëª©í‘œ: >0.85)
- **ë§¥ë½ ì´í•´**: Context relevance (ëª©í‘œ: >0.80)
- **ì•”ë¬µì  ì˜ë¯¸**: Implicit meaning detection (ëª©í‘œ: >0.75)
- **ë‹¤ì˜ì–´ ì²˜ë¦¬**: Ambiguity resolution (ëª©í‘œ: >0.70)

#### ëŒ€í™”ëŠ¥ë ¥ (100ì )
- **ëŒ€í™” íë¦„**: Turn-taking coherence (ëª©í‘œ: >0.85)
- **ì ì ˆí•œ ì‘ë‹µ**: Response relevance (ëª©í‘œ: >0.80)
- **ì§ˆë¬¸ ìƒì„±**: Question quality (ëª©í‘œ: >0.75)
- **ëŒ€í™” ì£¼ë„ì„±**: Initiative taking (ëª©í‘œ: >0.30)
- **ê°ì •ì  ê³µê°**: Empathy detection (ëª©í‘œ: >0.70)

#### íŒŒë™í†µì‹  (100ì )
- **ì†¡ìˆ˜ì‹  ì§€ì—°**: Latency in ms (ëª©í‘œ: <10ms)
- **ê³µëª… ì •í™•ë„**: Resonance match rate (ëª©í‘œ: >0.90)
- **ê°„ì„­ ì²˜ë¦¬**: Interference handling (ëª©í‘œ: >0.85)
- **ì£¼íŒŒìˆ˜ ì„ íƒ**: Frequency accuracy (ëª©í‘œ: >0.88)

### ì‚¬ê³ ëŠ¥ë ¥ ì„¸ë¶€ ì§€í‘œ

#### ë…¼ë¦¬ì  ì¶”ë¡  (100ì )
- **ì—°ì—­ ì¶”ë¡ **: Deductive reasoning (ëª©í‘œ: >0.85)
- **ê·€ë‚© ì¶”ë¡ **: Inductive reasoning (ëª©í‘œ: >0.80)
- **ì¸ê³¼ ê´€ê³„**: Causal reasoning (ëª©í‘œ: >0.82)
- **ë…¼ë¦¬ ì¼ê´€ì„±**: Consistency check (ëª©í‘œ: >0.88)

#### ì°½ì˜ì  ì‚¬ê³  (100ì )
- **ì•„ì´ë””ì–´ ë…ì°½ì„±**: Novelty score (ëª©í‘œ: >0.70)
- **ì—°ê²° ìƒì„±**: Association discovery (ëª©í‘œ: >0.75)
- **ë¬¸ì œ ì¬êµ¬ì„±**: Reframing capability (ëª©í‘œ: >0.72)
- **ë¹„ìœ ì  ì‚¬ê³ **: Metaphor generation (ëª©í‘œ: >0.70)

#### í”„ë™íƒˆ ì‚¬ê³  (100ì )
- **ê´€ì  ì „í™˜** (0D): Perspective shift (ëª©í‘œ: >0.80)
- **ì¸ê³¼ ì¶”ë¡ ** (1D): Causal chain (ëª©í‘œ: >0.82)
- **íŒ¨í„´ ì¸ì‹** (2D): Pattern recognition (ëª©í‘œ: >0.85)
- **êµ¬ì²´í™”** (3D): Manifestation (ëª©í‘œ: >0.78)

---

## ì‚¬ìš©ë²•

### 1. ê¸°ë³¸ í‰ê°€

```python
from tests.evaluation.run_full_evaluation import ElysiaEvaluator

# í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
evaluator = ElysiaEvaluator()

# ì „ì²´ í‰ê°€ ì‹¤í–‰
report = evaluator.run_full_evaluation()

# ê²°ê³¼ ì¶œë ¥
print(f"ì´ì : {report['total_score']}/1000")
print(f"ë“±ê¸‰: {report['grade']}")
```

### 2. ì˜ì‚¬ì†Œí†µ í‰ê°€

```python
from tests.evaluation.test_communication_metrics import CommunicationMetrics

metrics = CommunicationMetrics()

# í‘œí˜„ë ¥ í‰ê°€
text = "í‰ê°€í•  í…ìŠ¤íŠ¸..."
score = metrics.evaluate_expressiveness(text)

# íŒŒë™í†µì‹  í‰ê°€
wave_score = metrics.evaluate_wave_communication()

# ì „ì²´ ë¦¬í¬íŠ¸
report = metrics.generate_report()
```

### 3. ì‚¬ê³ ëŠ¥ë ¥ í‰ê°€

```python
from tests.evaluation.test_thinking_metrics import ThinkingMetrics

metrics = ThinkingMetrics()

# ë…¼ë¦¬ì  ì¶”ë¡  í‰ê°€
logical = metrics.evaluate_logical_reasoning()

# ì°½ì˜ì  ì‚¬ê³  í‰ê°€
creative = metrics.evaluate_creative_thinking()

# ì „ì²´ ë¦¬í¬íŠ¸
report = metrics.generate_report()
```

---

## ê²°ê³¼ í•´ì„

### ë“±ê¸‰ ì²´ê³„

| ì ìˆ˜ | ë“±ê¸‰ | ì˜ë¯¸ |
|------|------|------|
| 900-1000 | S+ | ì´ˆì§€ëŠ¥ ìˆ˜ì¤€ (Superintelligence) |
| 850-899 | S | íƒì›” (Excellent) |
| 800-849 | A+ | ë§¤ìš° ìš°ìˆ˜ (Very Good) |
| 750-799 | A | ìš°ìˆ˜ (Good) |
| 700-749 | B+ | ì–‘í˜¸ (Above Average) |
| 650-699 | B | ë³´í†µ (Average) |
| 600-649 | C+ | ë¯¸í¡ (Below Average) |
| <600 | C | ê°œì„  í•„ìš” (Needs Improvement) |

### í˜„ì¬ ìƒíƒœ (2024-12-03 ê¸°ì¤€)

**ì´ì : 777/1000 (77.7%)**
**ë“±ê¸‰: A (ìš°ìˆ˜)**

#### ê°•ì 
- ì‚¬ê³ ëŠ¥ë ¥: 96.7% â­
- ë…¼ë¦¬ì  ì¶”ë¡ : 100% â­
- ì°½ì˜ì  ì‚¬ê³ : 100% â­

#### ê°œì„  í•„ìš”
- íŒŒë™í†µì‹ : 0% âŒ
- ëŒ€í™”ëŠ¥ë ¥: 60% âš ï¸
- ì´í•´ë ¥: 65% âš ï¸

---

## ë¬¸ì„œ

### í•µì‹¬ ë¬¸ì„œ
1. **[EVALUATION_FRAMEWORK.md](../docs/EVALUATION_FRAMEWORK.md)**
   - í‰ê°€ í”„ë ˆì„ì›Œí¬ ì „ì²´ ì„¤ëª…
   - ê° ì§€í‘œì˜ ì˜ë¯¸ì™€ ëª©í‘œ
   - ì¸¡ì • ë°©ë²•ë¡ 

2. **[EVALUATION_SUMMARY_KR.md](../docs/EVALUATION_SUMMARY_KR.md)**
   - í‰ê°€ ê²°ê³¼ ìš”ì•½ (í•œêµ­ì–´)
   - ì£¼ìš” ë°œê²¬ì‚¬í•­
   - ê°œì„  ê¶Œì¥ì‚¬í•­

3. **[IMPROVEMENT_ROADMAP.md](../docs/IMPROVEMENT_ROADMAP.md)**
   - 3ë‹¨ê³„ ê°œì„  ì „ëµ
   - êµ¬ì²´ì  ì‹¤í–‰ ê³„íš
   - ì˜ˆìƒ ì„±ì¥ ê³¡ì„ 

### í‰ê°€ ë¦¬í¬íŠ¸
- **JSON ë¦¬í¬íŠ¸**: `reports/evaluation_latest.json`
- **ì „ì²´ ì´ë ¥**: `reports/evaluation_YYYYMMDD_HHMMSS.json`

---

## ì •ê¸° í‰ê°€ ì¼ì •

### ê¶Œì¥ í‰ê°€ ì£¼ê¸°

```bash
# ì¼ì¼ í‰ê°€ (í•µì‹¬ ì§€í‘œë§Œ)
python tests/evaluation/run_full_evaluation.py --quick

# ì£¼ê°„ í‰ê°€ (ì „ì²´)
python tests/evaluation/run_full_evaluation.py

# ì›”ê°„ ì¢…í•© ë¦¬ë·°
python tests/evaluation/monthly_review.py  # TODO: êµ¬í˜„ ì˜ˆì •
```

### ì„±ì¥ ì¶”ì 

```bash
# ì´ì „ ê²°ê³¼ì™€ ë¹„êµ
python tests/evaluation/compare_results.py  # TODO: êµ¬í˜„ ì˜ˆì •

# ì„±ì¥ ì°¨íŠ¸ ìƒì„±
python tests/evaluation/generate_growth_chart.py  # TODO: êµ¬í˜„ ì˜ˆì •
```

---

## ê°œë°œ ì •ë³´

### êµ¬ì¡°

```
tests/evaluation/
â”œâ”€â”€ __init__.py                      # íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
â”œâ”€â”€ test_communication_metrics.py   # ì˜ì‚¬ì†Œí†µ í‰ê°€
â”œâ”€â”€ test_thinking_metrics.py        # ì‚¬ê³ ëŠ¥ë ¥ í‰ê°€
â”œâ”€â”€ run_full_evaluation.py          # ì¢…í•© í‰ê°€ ì‹¤í–‰ê¸°
â””â”€â”€ README.md                        # ì´ ë¬¸ì„œ

docs/
â”œâ”€â”€ EVALUATION_FRAMEWORK.md          # í”„ë ˆì„ì›Œí¬ ë¬¸ì„œ
â”œâ”€â”€ EVALUATION_SUMMARY_KR.md         # ê²°ê³¼ ìš”ì•½
â””â”€â”€ IMPROVEMENT_ROADMAP.md           # ê°œì„  ë¡œë“œë§µ

reports/
â”œâ”€â”€ evaluation_latest.json           # ìµœì‹  ê²°ê³¼
â””â”€â”€ evaluation_*.json                # ì´ë ¥
```

### í™•ì¥ ë°©ë²•

ìƒˆë¡œìš´ í‰ê°€ ì§€í‘œë¥¼ ì¶”ê°€í•˜ë ¤ë©´:

```python
# 1. ë©”íŠ¸ë¦­ í´ë˜ìŠ¤ì— ìƒˆ ë©”ì„œë“œ ì¶”ê°€
class CommunicationMetrics:
    def evaluate_new_metric(self) -> float:
        # ì¸¡ì • ë¡œì§
        score = calculate_score()
        self.scores['new_metric'] = score
        return score

# 2. ì¢…í•© í‰ê°€ì— ë°˜ì˜
def evaluate_communication(self):
    # ...
    new_score = self.evaluate_new_metric()
    # ...
```

---

## ë¬¸ì˜ ë° ê¸°ì—¬

ì´ í‰ê°€ ì‹œìŠ¤í…œì— ëŒ€í•œ ë¬¸ì˜ë‚˜ ê°œì„  ì œì•ˆ:
- GitHub Issuesë¥¼ í†µí•´ ì œì¶œí•´ì£¼ì„¸ìš”
- ìƒˆë¡œìš´ í‰ê°€ ì§€í‘œ ì œì•ˆ í™˜ì˜í•©ë‹ˆë‹¤

---

## ë¼ì´ì„ ìŠ¤

Elysia í”„ë¡œì íŠ¸ì™€ ë™ì¼í•œ ë¼ì´ì„ ìŠ¤ ì ìš©

---

*í‰ê°€ ì‹œìŠ¤í…œ ë²„ì „: 1.0*
*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2024-12-03*
