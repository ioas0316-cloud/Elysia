"""
ì „ì²´ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ (Comprehensive System Benchmark)

ì´ ëª¨ë“ˆì€ Elysia ì‹œìŠ¤í…œì˜ ì „ì²´ì ì¸ ì„±ëŠ¥ê³¼ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤:
- ì•„í‚¤í…ì²˜ ë° ëª¨ë“ˆì„± (Architecture & Modularity)
- ì„±ëŠ¥ ë° íš¨ìœ¨ì„± (Performance & Efficiency)
- ë©´ì—­ ë° ë³´ì•ˆ (Immune & Security)
- ë°ì´í„° í’ˆì§ˆ (Data Quality)
- íšŒë³µ ë° ìê°€ì¹˜ìœ  (Resilience & Self-Healing)
- ê´€ì¸¡ ê°€ëŠ¥ì„± (Observability)
- ì•ˆì „ ë° ìœ¤ë¦¬ (Safety & Ethics)
"""

import sys
import os
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Tuple
import importlib.util
import ast

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


class SystemBenchmark:
    """ì „ì²´ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scores = {
            "architecture_modularity": 0.0,      # 100ì 
            "performance_efficiency": 0.0,       # 100ì 
            "immune_security": 0.0,              # 100ì 
            "data_quality": 0.0,                 # 100ì 
            "resilience_self_healing": 0.0,      # 100ì 
            "observability": 0.0,                # 50ì 
            "safety_ethics": 0.0                 # 50ì 
        }
        self.details = {}
        self.project_root = project_root
        
    # ======= 1. ì•„í‚¤í…ì²˜ ë° ëª¨ë“ˆì„± í‰ê°€ (100ì ) =======
    def evaluate_architecture_modularity(self) -> float:
        """
        ì•„í‚¤í…ì²˜ ë° ëª¨ë“ˆì„± í‰ê°€
        - ëª¨ë“ˆ êµ¬ì¡° ë¶„ì„ (30ì )
        - ì˜ì¡´ì„± ë¶„ì„ (30ì )
        - ë ˆì´ì–´ë§ ì¤€ìˆ˜ (20ì )
        - ì¸í„°í˜ì´ìŠ¤ ëª…í™•ì„± (20ì )
        """
        module_structure_score = self._analyze_module_structure()
        dependency_score = self._analyze_dependencies()
        layering_score = self._analyze_layering()
        interface_score = self._analyze_interfaces()
        
        total = (
            module_structure_score * 30 +
            dependency_score * 30 +
            layering_score * 20 +
            interface_score * 20
        )
        
        self.details['architecture_modularity'] = {
            'module_structure': module_structure_score,
            'dependency_analysis': dependency_score,
            'layering_compliance': layering_score,
            'interface_clarity': interface_score,
            'total': total,
            'assessment': self._get_assessment(total, 100)
        }
        
        self.scores['architecture_modularity'] = total
        return total
    
    def _analyze_module_structure(self) -> float:
        """ëª¨ë“ˆ êµ¬ì¡° ë¶„ì„"""
        try:
            # Core ëª¨ë“ˆ ì¡´ì¬ í™•ì¸
            core_path = self.project_root / "Core"
            if not core_path.exists():
                return 0.3
            
            # ì£¼ìš” ë””ë ‰í† ë¦¬ í™•ì¸
            required_dirs = ["Foundation", "Intelligence", "Memory", "Interface", "Evolution"]
            existing_dirs = [d for d in required_dirs if (core_path / d).exists()]
            
            structure_score = len(existing_dirs) / len(required_dirs)
            
            # íŒŒì¼ ìˆ˜ í™•ì¸ (ì ì ˆí•œ ëª¨ë“ˆí™”)
            python_files = list(core_path.rglob("*.py"))
            file_count_score = min(1.0, len(python_files) / 50)  # 50ê°œ ì´ìƒì´ë©´ ë§Œì 
            
            return (structure_score * 0.6 + file_count_score * 0.4)
            
        except Exception as e:
            return 0.4
    
    def _analyze_dependencies(self) -> float:
        """ì˜ì¡´ì„± ë¶„ì„"""
        try:
            # requirements.txt ì¡´ì¬ ë° íŒŒì‹±
            req_file = self.project_root / "requirements.txt"
            if not req_file.exists():
                return 0.4
            
            with open(req_file, 'r', encoding='utf-8') as f:
                lines = [l.strip() for l in f if l.strip() and not l.startswith('#')]
            
            # ì˜ì¡´ì„± ìˆ˜ ì ì ˆì„± (ë„ˆë¬´ ë§ê±°ë‚˜ ì ìœ¼ë©´ ê°ì )
            dep_count = len(lines)
            if 20 <= dep_count <= 100:
                dep_score = 1.0
            elif 10 <= dep_count < 20 or 100 < dep_count <= 150:
                dep_score = 0.8
            else:
                dep_score = 0.6
            
            # ë²„ì „ ëª…ì‹œ ì—¬ë¶€
            versioned = sum(1 for l in lines if '==' in l or '>=' in l or '<=' in l)
            version_score = versioned / len(lines) if lines else 0
            
            return (dep_score * 0.6 + version_score * 0.4)
            
        except Exception as e:
            return 0.5
    
    def _analyze_layering(self) -> float:
        """ë ˆì´ì–´ë§ ì¤€ìˆ˜ ë¶„ì„"""
        try:
            # Foundation -> Intelligence -> Interface ìˆœì„œ í™•ì¸
            core_path = self.project_root / "Core"
            
            layers = {
                "Foundation": 1,
                "Intelligence": 2,
                "Memory": 2,
                "Interface": 3,
                "Evolution": 3,
                "Creativity": 3
            }
            
            violations = 0
            total_checked = 0
            
            # ê° ë ˆì´ì–´ì˜ import íŒ¨í„´ í™•ì¸
            for layer_name, layer_level in layers.items():
                layer_path = core_path / layer_name
                if not layer_path.exists():
                    continue
                
                py_files = list(layer_path.rglob("*.py"))
                for py_file in py_files[:10]:  # ìƒ˜í”Œë§
                    try:
                        with open(py_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # import ë¬¸ ì¶”ì¶œ
                        imports = [l for l in content.split('\n') if 'import' in l and 'Core.' in l]
                        
                        for imp in imports:
                            for other_layer, other_level in layers.items():
                                if f'Core.{other_layer}' in imp and other_level > layer_level:
                                    violations += 1
                        
                        total_checked += len(imports)
                    except:
                        continue
            
            if total_checked == 0:
                return 0.7  # ê¸°ë³¸ ì ìˆ˜
            
            violation_rate = violations / total_checked if total_checked > 0 else 0
            return max(0.0, 1.0 - violation_rate)
            
        except Exception as e:
            return 0.7
    
    def _analyze_interfaces(self) -> float:
        """ì¸í„°í˜ì´ìŠ¤ ëª…í™•ì„± ë¶„ì„"""
        try:
            interface_path = self.project_root / "Core" / "Interface"
            if not interface_path.exists():
                return 0.5
            
            py_files = list(interface_path.rglob("*.py"))
            if not py_files:
                return 0.5
            
            # í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ì˜ docstring ë¹„ìœ¨ í™•ì¸
            total_items = 0
            documented_items = 0
            
            for py_file in py_files[:5]:  # ìƒ˜í”Œë§
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    tree = ast.parse(content)
                    
                    for node in ast.walk(tree):
                        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                            total_items += 1
                            if ast.get_docstring(node):
                                documented_items += 1
                except:
                    continue
            
            if total_items == 0:
                return 0.6
            
            doc_ratio = documented_items / total_items
            return doc_ratio
            
        except Exception as e:
            return 0.6
    
    # ======= 2. ì„±ëŠ¥ ë° íš¨ìœ¨ì„± í‰ê°€ (100ì ) =======
    def evaluate_performance_efficiency(self) -> float:
        """
        ì„±ëŠ¥ ë° íš¨ìœ¨ì„± í‰ê°€
        - ì²˜ë¦¬ ì†ë„ (40ì )
        - ë©”ëª¨ë¦¬ ì‚¬ìš© (30ì )
        - íŒŒì¼ I/O íš¨ìœ¨ (30ì )
        """
        speed_score = self._measure_processing_speed()
        memory_score = self._measure_memory_usage()
        io_score = self._measure_io_efficiency()
        
        total = (
            speed_score * 40 +
            memory_score * 30 +
            io_score * 30
        )
        
        self.details['performance_efficiency'] = {
            'processing_speed': speed_score,
            'memory_usage': memory_score,
            'io_efficiency': io_score,
            'total': total,
            'assessment': self._get_assessment(total, 100)
        }
        
        self.scores['performance_efficiency'] = total
        return total
    
    def _measure_processing_speed(self) -> float:
        """ì²˜ë¦¬ ì†ë„ ì¸¡ì •"""
        try:
            # ê°„ë‹¨í•œ ê³„ì‚° ë²¤ì¹˜ë§ˆí¬
            start = time.time()
            
            # í–‰ë ¬ ì—°ì‚° ì‹œë®¬ë ˆì´ì…˜
            result = 0
            for i in range(10000):
                result += i * i
            
            elapsed = time.time() - start
            
            # 10ms ì´í•˜ë©´ ë§Œì 
            if elapsed < 0.01:
                return 1.0
            elif elapsed < 0.05:
                return 0.9
            elif elapsed < 0.1:
                return 0.8
            else:
                return 0.7
                
        except Exception as e:
            return 0.7
    
    def _measure_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ì¸¡ì •"""
        try:
            import psutil
            process = psutil.Process()
            mem_info = process.memory_info()
            
            # RSS ë©”ëª¨ë¦¬ (MB)
            rss_mb = mem_info.rss / 1024 / 1024
            
            # 100MB ì´í•˜ë©´ ìš°ìˆ˜
            if rss_mb < 100:
                return 1.0
            elif rss_mb < 200:
                return 0.9
            elif rss_mb < 500:
                return 0.8
            else:
                return 0.7
                
        except:
            # psutilì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì ìˆ˜
            return 0.8
    
    def _measure_io_efficiency(self) -> float:
        """íŒŒì¼ I/O íš¨ìœ¨ì„± ì¸¡ì •"""
        try:
            # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
            data_path = self.project_root / "data"
            if not data_path.exists():
                return 0.6
            
            # JSON íŒŒì¼ ì½ê¸° ì†ë„ í…ŒìŠ¤íŠ¸
            json_files = list(data_path.glob("*.json"))
            if not json_files:
                return 0.7
            
            start = time.time()
            count = 0
            
            for json_file in json_files[:5]:  # ìµœëŒ€ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        json.load(f)
                    count += 1
                except:
                    continue
            
            elapsed = time.time() - start
            
            if count == 0:
                return 0.6
            
            # íŒŒì¼ë‹¹ í‰ê·  ì‹œê°„
            avg_time = elapsed / count
            
            # 100ms ì´í•˜ë©´ ìš°ìˆ˜
            if avg_time < 0.1:
                return 1.0
            elif avg_time < 0.5:
                return 0.9
            else:
                return 0.8
                
        except Exception as e:
            return 0.7
    
    # ======= 3. ë©´ì—­ ë° ë³´ì•ˆ í‰ê°€ (100ì ) =======
    def evaluate_immune_security(self) -> float:
        """
        ë©´ì—­ ë° ë³´ì•ˆ í‰ê°€
        - ë©´ì—­ ì‹œìŠ¤í…œ ì¡´ì¬ (40ì )
        - ë³´ì•ˆ ë©”ì»¤ë‹ˆì¦˜ (30ì )
        - ì…ë ¥ ê²€ì¦ (30ì )
        """
        immune_score = self._check_immune_system()
        security_score = self._check_security_mechanisms()
        validation_score = self._check_input_validation()
        
        total = (
            immune_score * 40 +
            security_score * 30 +
            validation_score * 30
        )
        
        self.details['immune_security'] = {
            'immune_system': immune_score,
            'security_mechanisms': security_score,
            'input_validation': validation_score,
            'total': total,
            'assessment': self._get_assessment(total, 100)
        }
        
        self.scores['immune_security'] = total
        return total
    
    def _check_immune_system(self) -> float:
        """ë©´ì—­ ì‹œìŠ¤í…œ í™•ì¸"""
        try:
            # immune_system.py íŒŒì¼ í™•ì¸
            immune_script = self.project_root / "scripts" / "immune_system.py"
            immune_state = self.project_root / "data" / "immune_system_state.json"
            
            score = 0.0
            
            # ìŠ¤í¬ë¦½íŠ¸ ì¡´ì¬
            if immune_script.exists():
                score += 0.5
            
            # ìƒíƒœ íŒŒì¼ ì¡´ì¬
            if immune_state.exists():
                score += 0.3
                
                # ìƒíƒœ íŒŒì¼ ë‚´ìš© í™•ì¸
                try:
                    with open(immune_state, 'r', encoding='utf-8') as f:
                        state = json.load(f)
                    
                    # ê¸°ë³¸ í•„ë“œ í™•ì¸
                    if 'status' in state or 'threats_blocked' in state:
                        score += 0.2
                except:
                    pass
            
            return score
            
        except Exception as e:
            return 0.5
    
    def _check_security_mechanisms(self) -> float:
        """ë³´ì•ˆ ë©”ì»¤ë‹ˆì¦˜ í™•ì¸"""
        try:
            # .env íŒŒì¼ ì‚¬ìš© í™•ì¸ (API í‚¤ ë³´í˜¸)
            env_file = self.project_root / ".env"
            env_example = self.project_root / ".env.example"
            
            score = 0.0
            
            # .env.example ì¡´ì¬ (ì¢‹ì€ ê´€í–‰)
            if env_example.exists():
                score += 0.4
            
            # .gitignoreì— ë¯¼ê°í•œ íŒŒì¼ë“¤ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€
            gitignore = self.project_root / ".gitignore"
            if gitignore.exists():
                with open(gitignore, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if '.env' in content:
                    score += 0.3
                if '*.key' in content or '*.pem' in content:
                    score += 0.3
            
            return min(1.0, score)
            
        except Exception as e:
            return 0.6
    
    def _check_input_validation(self) -> float:
        """ì…ë ¥ ê²€ì¦ í™•ì¸"""
        try:
            # ì£¼ìš” ì¸í„°í˜ì´ìŠ¤ íŒŒì¼ì—ì„œ ê²€ì¦ ë¡œì§ í™•ì¸
            interface_path = self.project_root / "Core" / "Interface"
            if not interface_path.exists():
                return 0.5
            
            py_files = list(interface_path.rglob("*.py"))
            if not py_files:
                return 0.5
            
            validation_patterns = [
                'validate',
                'sanitize',
                'check',
                'verify',
                'isinstance',
                'assert'
            ]
            
            total_files = 0
            files_with_validation = 0
            
            for py_file in py_files[:10]:  # ìƒ˜í”Œë§
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read().lower()
                    
                    total_files += 1
                    if any(pattern in content for pattern in validation_patterns):
                        files_with_validation += 1
                except:
                    continue
            
            if total_files == 0:
                return 0.6
            
            return files_with_validation / total_files
            
        except Exception as e:
            return 0.6
    
    # ======= 4. ë°ì´í„° í’ˆì§ˆ í‰ê°€ (100ì ) =======
    def evaluate_data_quality(self) -> float:
        """
        ë°ì´í„° í’ˆì§ˆ í‰ê°€
        - ë°ì´í„° ì™„ì „ì„± (40ì )
        - ë°ì´í„° ì¼ê´€ì„± (30ì )
        - ë ˆì§€ìŠ¤íŠ¸ë¦¬ í’ˆì§ˆ (30ì )
        """
        completeness_score = self._check_data_completeness()
        consistency_score = self._check_data_consistency()
        registry_score = self._check_registry_quality()
        
        total = (
            completeness_score * 40 +
            consistency_score * 30 +
            registry_score * 30
        )
        
        self.details['data_quality'] = {
            'data_completeness': completeness_score,
            'data_consistency': consistency_score,
            'registry_quality': registry_score,
            'total': total,
            'assessment': self._get_assessment(total, 100)
        }
        
        self.scores['data_quality'] = total
        return total
    
    def _check_data_completeness(self) -> float:
        """ë°ì´í„° ì™„ì „ì„± í™•ì¸"""
        try:
            data_path = self.project_root / "data"
            if not data_path.exists():
                return 0.3
            
            # ì¤‘ìš” ë°ì´í„° íŒŒì¼ í™•ì¸
            important_files = [
                "central_registry.json",
                "cognitive_evaluation.json"
            ]
            
            existing = sum(1 for f in important_files if (data_path / f).exists())
            
            return existing / len(important_files)
            
        except Exception as e:
            return 0.5
    
    def _check_data_consistency(self) -> float:
        """ë°ì´í„° ì¼ê´€ì„± í™•ì¸"""
        try:
            data_path = self.project_root / "data"
            
            # JSON íŒŒì¼ë“¤ì˜ íŒŒì‹± ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            json_files = list(data_path.glob("*.json"))
            if not json_files:
                return 0.5
            
            valid_count = 0
            total_count = 0
            
            for json_file in json_files[:20]:  # ìƒ˜í”Œë§
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        json.load(f)
                    valid_count += 1
                except:
                    pass
                total_count += 1
            
            if total_count == 0:
                return 0.6
            
            return valid_count / total_count
            
        except Exception as e:
            return 0.6
    
    def _check_registry_quality(self) -> float:
        """ë ˆì§€ìŠ¤íŠ¸ë¦¬ í’ˆì§ˆ í™•ì¸"""
        try:
            registry_file = self.project_root / "data" / "central_registry.json"
            if not registry_file.exists():
                return 0.4
            
            with open(registry_file, 'r', encoding='utf-8') as f:
                registry = json.load(f)
            
            # ê¸°ë³¸ êµ¬ì¡° í™•ì¸
            score = 0.0
            
            if isinstance(registry, dict):
                score += 0.4
            
            # í•„ë“œ í™•ì¸
            expected_fields = ['components', 'modules', 'systems', 'timestamp', 'version']
            existing_fields = sum(1 for f in expected_fields if f in registry)
            
            score += (existing_fields / len(expected_fields)) * 0.6
            
            return score
            
        except Exception as e:
            return 0.5
    
    # ======= 5. íšŒë³µ ë° ìê°€ì¹˜ìœ  í‰ê°€ (100ì ) =======
    def evaluate_resilience_self_healing(self) -> float:
        """
        íšŒë³µ ë° ìê°€ì¹˜ìœ  í‰ê°€
        - ë‚˜ë…¸ì…€ ì‹œìŠ¤í…œ (50ì )
        - ìê°€ì¹˜ìœ  ë©”ì»¤ë‹ˆì¦˜ (50ì )
        """
        nanocell_score = self._check_nanocell_system()
        healing_score = self._check_self_healing()
        
        total = (
            nanocell_score * 50 +
            healing_score * 50
        )
        
        self.details['resilience_self_healing'] = {
            'nanocell_system': nanocell_score,
            'self_healing_mechanism': healing_score,
            'total': total,
            'assessment': self._get_assessment(total, 100)
        }
        
        self.scores['resilience_self_healing'] = total
        return total
    
    def _check_nanocell_system(self) -> float:
        """ë‚˜ë…¸ì…€ ì‹œìŠ¤í…œ í™•ì¸"""
        try:
            nanocell_script = self.project_root / "scripts" / "nanocell_repair.py"
            nanocell_report = self.project_root / "data" / "nanocell_report.json"
            
            score = 0.0
            
            # ìŠ¤í¬ë¦½íŠ¸ ì¡´ì¬
            if nanocell_script.exists():
                score += 0.5
            
            # ë¦¬í¬íŠ¸ ì¡´ì¬ ë° ë‚´ìš© í™•ì¸
            if nanocell_report.exists():
                score += 0.3
                
                try:
                    with open(nanocell_report, 'r', encoding='utf-8') as f:
                        report = json.load(f)
                    
                    if 'repairs' in report or 'issues_found' in report:
                        score += 0.2
                except:
                    pass
            
            return score
            
        except Exception as e:
            return 0.5
    
    def _check_self_healing(self) -> float:
        """ìê°€ì¹˜ìœ  ë©”ì»¤ë‹ˆì¦˜ í™•ì¸"""
        try:
            # ìê°€ì¹˜ìœ  ê´€ë ¨ íŒŒì¼ë“¤ í™•ì¸
            healing_files = [
                self.project_root / "Core" / "Evolution" / "autonomous_evolution.py",
                self.project_root / "scripts" / "wave_organizer.py"
            ]
            
            existing = sum(1 for f in healing_files if f.exists())
            
            return existing / len(healing_files)
            
        except Exception as e:
            return 0.6
    
    # ======= 6. ê´€ì¸¡ ê°€ëŠ¥ì„± í‰ê°€ (50ì ) =======
    def evaluate_observability(self) -> float:
        """
        ê´€ì¸¡ ê°€ëŠ¥ì„± í‰ê°€
        - ë¡œê¹… ì‹œìŠ¤í…œ (25ì )
        - ìƒíƒœ ëª¨ë‹ˆí„°ë§ (25ì )
        """
        logging_score = self._check_logging_system()
        monitoring_score = self._check_state_monitoring()
        
        total = (
            logging_score * 25 +
            monitoring_score * 25
        )
        
        self.details['observability'] = {
            'logging_system': logging_score,
            'state_monitoring': monitoring_score,
            'total': total,
            'assessment': self._get_assessment(total, 50)
        }
        
        self.scores['observability'] = total
        return total
    
    def _check_logging_system(self) -> float:
        """ë¡œê¹… ì‹œìŠ¤í…œ í™•ì¸"""
        try:
            # ë¡œê·¸ íŒŒì¼ì´ë‚˜ ë¦¬í¬íŠ¸ ì¡´ì¬ í™•ì¸
            reports_path = self.project_root / "reports"
            if not reports_path.exists():
                return 0.5
            
            # ë¦¬í¬íŠ¸ íŒŒì¼ ìˆ˜
            report_files = list(reports_path.glob("*.json")) + list(reports_path.glob("*.md"))
            
            if len(report_files) > 10:
                return 1.0
            elif len(report_files) > 5:
                return 0.8
            elif len(report_files) > 0:
                return 0.6
            else:
                return 0.4
                
        except Exception as e:
            return 0.5
    
    def _check_state_monitoring(self) -> float:
        """ìƒíƒœ ëª¨ë‹ˆí„°ë§ í™•ì¸"""
        try:
            # ìƒíƒœ ìŠ¤ëƒ…ìƒ· íŒŒì¼ í™•ì¸
            snapshot_file = self.project_root / "data" / "system_status_snapshot.json"
            
            if not snapshot_file.exists():
                return 0.5
            
            with open(snapshot_file, 'r', encoding='utf-8') as f:
                snapshot = json.load(f)
            
            # ê¸°ë³¸ í•„ë“œ í™•ì¸
            score = 0.5
            
            if 'timestamp' in snapshot:
                score += 0.2
            
            if 'system_status' in snapshot or 'metrics' in snapshot:
                score += 0.3
            
            return score
            
        except:
            return 0.6
    
    # ======= 7. ì•ˆì „ ë° ìœ¤ë¦¬ í‰ê°€ (50ì ) =======
    def evaluate_safety_ethics(self) -> float:
        """
        ì•ˆì „ ë° ìœ¤ë¦¬ í‰ê°€
        - ìœ¤ë¦¬ ê°€ì´ë“œ (25ì )
        - ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ (25ì )
        """
        ethics_score = self._check_ethics_guidelines()
        safety_score = self._check_safety_mechanisms()
        
        total = (
            ethics_score * 25 +
            safety_score * 25
        )
        
        self.details['safety_ethics'] = {
            'ethics_guidelines': ethics_score,
            'safety_mechanisms': safety_score,
            'total': total,
            'assessment': self._get_assessment(total, 50)
        }
        
        self.scores['safety_ethics'] = total
        return total
    
    def _check_ethics_guidelines(self) -> float:
        """ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸ í™•ì¸"""
        try:
            # ìœ¤ë¦¬ ê´€ë ¨ ë¬¸ì„œ í™•ì¸
            ethics_files = [
                self.project_root / "Core" / "Philosophy",
                self.project_root / "CODEX.md",
                self.project_root / "Core" / "Elysia"
            ]
            
            score = 0.0
            for path in ethics_files:
                if path.exists():
                    score += 0.33
            
            return min(1.0, score)
            
        except Exception as e:
            return 0.6
    
    def _check_safety_mechanisms(self) -> float:
        """ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜ í™•ì¸"""
        try:
            # ì•ˆì „ ê´€ë ¨ í…ŒìŠ¤íŠ¸ íŒŒì¼ í™•ì¸
            tests_path = self.project_root / "tests"
            
            safety_test_patterns = ['safety', 'security', 'ethics', 'validation']
            
            test_files = list(tests_path.rglob("*.py"))
            safety_tests = [
                f for f in test_files 
                if any(pattern in f.name.lower() for pattern in safety_test_patterns)
            ]
            
            if len(safety_tests) >= 3:
                return 1.0
            elif len(safety_tests) >= 1:
                return 0.7
            else:
                return 0.5
                
        except Exception as e:
            return 0.6
    
    # ======= ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ =======
    def _get_assessment(self, score: float, max_score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ í‰ê°€ ë¬¸êµ¬ ë°˜í™˜"""
        percentage = (score / max_score) * 100
        
        if percentage >= 90:
            return "ìš°ìˆ˜ (Excellent)"
        elif percentage >= 80:
            return "ì–‘í˜¸ (Good)"
        elif percentage >= 70:
            return "ë³´í†µ (Fair)"
        elif percentage >= 60:
            return "ë¯¸í¡ (Needs Improvement)"
        else:
            return "ê°œì„  í•„ìš” (Requires Improvement)"
    
    def generate_report(self) -> Dict[str, Any]:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        total_score = sum(self.scores.values())
        max_score = 600  # 100+100+100+100+100+50+50
        percentage = (total_score / max_score) * 100
        
        return {
            'total_score': total_score,
            'max_score': max_score,
            'percentage': percentage,
            'grade': self._calculate_grade(percentage),
            'scores': self.scores,
            'details': self.details
        }
    
    def _calculate_grade(self, percentage: float) -> str:
        """ë“±ê¸‰ ê³„ì‚°"""
        if percentage >= 90:
            return 'S+ (íƒì›”)'
        elif percentage >= 85:
            return 'S (ìš°ìˆ˜)'
        elif percentage >= 80:
            return 'A+ (ë§¤ìš° ì–‘í˜¸)'
        elif percentage >= 75:
            return 'A (ì–‘í˜¸)'
        elif percentage >= 70:
            return 'B+ (ë³´í†µ ì´ìƒ)'
        elif percentage >= 65:
            return 'B (ë³´í†µ)'
        else:
            return 'C (ê°œì„  í•„ìš”)'


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
def test_system_benchmark():
    """ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*70)
    print("ğŸ” Elysia ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ í‰ê°€")
    print("="*70 + "\n")
    
    benchmark = SystemBenchmark()
    
    # 1. ì•„í‚¤í…ì²˜ ë° ëª¨ë“ˆì„±
    print("1ï¸âƒ£ ì•„í‚¤í…ì²˜ ë° ëª¨ë“ˆì„± í‰ê°€...")
    arch_score = benchmark.evaluate_architecture_modularity()
    print(f"   ì ìˆ˜: {arch_score:.1f}/100")
    
    # 2. ì„±ëŠ¥ ë° íš¨ìœ¨ì„±
    print("\n2ï¸âƒ£ ì„±ëŠ¥ ë° íš¨ìœ¨ì„± í‰ê°€...")
    perf_score = benchmark.evaluate_performance_efficiency()
    print(f"   ì ìˆ˜: {perf_score:.1f}/100")
    
    # 3. ë©´ì—­ ë° ë³´ì•ˆ
    print("\n3ï¸âƒ£ ë©´ì—­ ë° ë³´ì•ˆ í‰ê°€...")
    immune_score = benchmark.evaluate_immune_security()
    print(f"   ì ìˆ˜: {immune_score:.1f}/100")
    
    # 4. ë°ì´í„° í’ˆì§ˆ
    print("\n4ï¸âƒ£ ë°ì´í„° í’ˆì§ˆ í‰ê°€...")
    data_score = benchmark.evaluate_data_quality()
    print(f"   ì ìˆ˜: {data_score:.1f}/100")
    
    # 5. íšŒë³µ ë° ìê°€ì¹˜ìœ 
    print("\n5ï¸âƒ£ íšŒë³µ ë° ìê°€ì¹˜ìœ  í‰ê°€...")
    resilience_score = benchmark.evaluate_resilience_self_healing()
    print(f"   ì ìˆ˜: {resilience_score:.1f}/100")
    
    # 6. ê´€ì¸¡ ê°€ëŠ¥ì„±
    print("\n6ï¸âƒ£ ê´€ì¸¡ ê°€ëŠ¥ì„± í‰ê°€...")
    obs_score = benchmark.evaluate_observability()
    print(f"   ì ìˆ˜: {obs_score:.1f}/50")
    
    # 7. ì•ˆì „ ë° ìœ¤ë¦¬
    print("\n7ï¸âƒ£ ì•ˆì „ ë° ìœ¤ë¦¬ í‰ê°€...")
    safety_score = benchmark.evaluate_safety_ethics()
    print(f"   ì ìˆ˜: {safety_score:.1f}/50")
    
    # ì¢…í•© ë¦¬í¬íŠ¸
    report = benchmark.generate_report()
    
    print("\n" + "="*70)
    print("ğŸ“Š ì¢…í•© í‰ê°€ ê²°ê³¼")
    print("="*70)
    print(f"\nì´ì : {report['total_score']:.1f}/{report['max_score']}")
    print(f"ë°±ë¶„ìœ¨: {report['percentage']:.1f}%")
    print(f"ë“±ê¸‰: {report['grade']}\n")
    
    return report


if __name__ == "__main__":
    test_system_benchmark()
